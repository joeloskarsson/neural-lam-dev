{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74e156a",
   "metadata": {},
   "source": [
    "##  Sparse Model Verification\n",
    "\n",
    "This script verifies output from a ML-based foundation model versus a\n",
    "traditional NWP system for the atmospheric system. The defaults set at the top of\n",
    "this script are tailored to the Alps-Clariden HPC system at CSCS.\n",
    "- The NWP-model is called COSMO-E and is initialised with the ensemble mean of the analysis. Only surface level data is available in the archive at MeteoSwiss.\n",
    "- The ML-model is called Neural-LAM and is initialised with the deterministic analysis.\n",
    "- The Ground Truth are surface level observations from MeteoSwiss.\n",
    "\n",
    "For more info about the COSMO model see:\n",
    "- https://www.cosmo-model.org/content/model/cosmo/coreDocumentation/cosmo_io_guide_6.00.pdf\n",
    "- https://www.research-collection.ethz.ch/handle/20.500.11850/720460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import kurtosis, skew, wasserstein_distance\n",
    "from scores.categorical import ThresholdEventOperator as TEO\n",
    "from scores.continuous import (\n",
    "    mae,\n",
    "    mean_error,\n",
    "    mse,\n",
    "    rmse,\n",
    ")\n",
    "from scores.continuous.correlation import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257a51b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "### DEFAULTS ###\n",
    "# This config will be applied to the data before any plotting. The data will be\n",
    "# sliced and indexed according to the values in this config.The whole analysis and\n",
    "# plotting will be done on the reduced data.\n",
    "\n",
    "# IF YOUR DATA HAS DIFFERENT DIMENSIONS OR NAMES, PLEASE ADJUST THE CELLS BELOW\n",
    "# MAKE SURE THE XARRAY DATASETS LOOK OKAY BEFORE RUNNING CHAPTER 1-4\n",
    "\n",
    "# This path should point to the NWP forecast data in zarr format\n",
    "PATH_NWP = \"/capstor/store/cscs/swissai/a01/sadamov/cosmo_e_forecast.zarr\"\n",
    "# This path should point to the ML forecast data in zarr format (e.g. produced by neural-lam in `eval` mode)\n",
    "PATH_ML = \"/iopsstor/scratch/cscs/sadamov/pyprojects_data/neural-lam/eval_results/preds_7_19_margin_interior_lr_0001_ar_12.zarr\"\n",
    "# This path should point to the observations data in zarr format\n",
    "PATH_OBS = \"/capstor/store/cscs/swissai/a01/sadamov/cosmo_observations.zarr\"\n",
    "\n",
    "# All variables should map to the same values.\n",
    "# For obs please also theck the pre-processing in the cell below\n",
    "# Often some data wrangling is required to match the variables\n",
    "# OBS and ML variables should match and always be present\n",
    "VARIABLES_ML = {\n",
    "    \"T_2M\": \"temperature_2m\",\n",
    "    \"U_10M\": \"wind_u_10m\",\n",
    "    \"V_10M\": \"wind_v_10m\",\n",
    "    \"PS\": \"surface_pressure\",\n",
    "    \"TOT_PREC\": \"precipitation\",\n",
    "}\n",
    "# The function calls are flexible to allow for missing nwp data\n",
    "VARIABLES_NWP = {\n",
    "    \"temperature_2m\": \"temperature_2m\",\n",
    "    \"wind_u_10m\": \"wind_u_10m\",\n",
    "    \"wind_v_10m\": \"wind_v_10m\",\n",
    "    \"surface_pressure\": \"surface_pressure\",\n",
    "    \"precipitation_1hr\": \"precipitation\",\n",
    "}\n",
    "VARIABLES_OBS = [\n",
    "    \"air_temperature\",\n",
    "    \"wind_speed\",\n",
    "    \"wind_direction\",\n",
    "    \"air_pressure\",\n",
    "    \"precipitation\",\n",
    "]\n",
    "\n",
    "# Add units dictionary after the imports\n",
    "# units from zarr archives are not reliable and should rather be defined here\n",
    "VARIABLE_UNITS = {\n",
    "    # Surface and near-surface variables\n",
    "    \"temperature_2m\": \"K\",\n",
    "    \"wind_u_10m\": \"m/s\",\n",
    "    \"wind_v_10m\": \"m/s\",\n",
    "    \"surface_pressure\": \"Pa\",\n",
    "    \"precipitation\": \"mm/h\",\n",
    "}\n",
    "\n",
    "# elapsed forecast duration in steps for the forecast - [0] refers to the first forecast step at t+1\n",
    "# this should be a list of integers\n",
    "ELAPSED_FORECAST_DURATION = list(range(0, 120, 1))\n",
    "# Select specific start_times for the forecast. This is the start and end of\n",
    "# a slice in xarray. The start_time is included, the end_time is excluded.\n",
    "# This should be a list of two strings in the format \"YYYY-MM-DDTHH:MM:SS\"\n",
    "# Should be handy to evaluate certain dates, e.g. for a case study of a storm\n",
    "START_TIMES = [\"2019-10-31T00:00:00\", \"2020-10-23T13:00:00\"]  # Full year\n",
    "# START_TIMES = [\"2020-02-08T00:00:00\", \"2020-02-15T00:00:00\"]  # Ciara/Sabine\n",
    "\n",
    "# Select specific plot times for the forecast (will be used to create maps for all variables)\n",
    "# This only affect chapter one with the plotting of the maps\n",
    "# Map creation takes a lot of time so this is limited to a single time step\n",
    "# Simply rerun these cells and chapter one for more time steps\n",
    "PLOT_TIME = \"2020-02-10T12:00:00\"\n",
    "\n",
    "# Define Thresholds for the ETS metric (Equitable Threat Score)\n",
    "# These are calculated for wind and precipitation if available\n",
    "# The score creates contingency tables for different thresholds\n",
    "# The ETS is calculated for each threshold and the results are plotted\n",
    "# The default thresholds are [0.1, 1, 5] for precipitation and [2.5, 5, 10] for wind\n",
    "THRESHOLDS_PRECIPITATION = [0.1, 1, 5]  # mm/h\n",
    "THRESHOLDS_WIND = [2.5, 5, 10]  # m/s\n",
    "\n",
    "# Define the metrics to compute for the verification\n",
    "# Some additional verifications will always be computed if the repsective vars\n",
    "# are available in the data\n",
    "METRICS = [\n",
    "    \"MAE\",\n",
    "    \"RMSE\",\n",
    "    \"MSE\",\n",
    "    \"ME\",\n",
    "    \"STDEV_ERR\",\n",
    "    \"RelativeMAE\",\n",
    "    \"RelativeRMSE\",\n",
    "    \"PearsonR\",\n",
    "    \"Wasserstein\",\n",
    "]\n",
    "\n",
    "# Map projection settings for plotting of the ml data\n",
    "PROJECTION = ccrs.RotatedPole(\n",
    "    pole_longitude=190,\n",
    "    pole_latitude=43,\n",
    "    central_rotated_longitude=10,\n",
    ")\n",
    "\n",
    "# For some chapters a random seed is required to reproduce the results\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# The DPI used in all plots in the notebook, export to pdf will always be 300 DPI\n",
    "DPI = 100\n",
    "\n",
    "# Subsample the data for faster plotting, 10 refers to every 10th element\n",
    "# This is used to create the histograms in chapter 2 (along space and time)\n",
    "# and in chapter 3 for the energy spectra (along time)\n",
    "# There is a trade-off between speed and accuracy, that each user has to find\n",
    "SUBSAMPLE_HISTOGRAM = None\n",
    "\n",
    "# Takes a long time, but if you see NaN in your output, you can set this to True\n",
    "# This will check if there are any missing values in the data further below\n",
    "CHECK_MISSING = True\n",
    "# In this script missing data is allowed as observations often have missing values\n",
    "# All time steps with missing values will be omitted from the verification\n",
    "# - Scores/Xarray masked arrays are created and false values are omitted by default\n",
    "# - For Scipy metrics, we need to convert to numpy arrays and change nan-policy to 'omit'.\n",
    "# - Fore the wasserstein metric and the wind vector without internal nan-handling policy,\n",
    "#  we need to remove the missing values for obs, ml, nwp before calculating the metric\n",
    "\n",
    "# Font sizes for consistent plotting (different fig-sizes wil require different font sizes)\n",
    "FONT_SIZES = {\n",
    "    \"axes\": 13,  # Axis labels and titles\n",
    "    \"ticks\": 13,  # Tick labels\n",
    "    \"legend\": 13,  # Legend text\n",
    "    \"cbar\": 13,  # Colorbar labels\n",
    "    \"suptitle\": 15,  # Figure suptitle\n",
    "    \"title\": 13,  # Axes titles\n",
    "    \"stats\": 13,  # Statistics text in plots\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create directories for plots and tables\n",
    "Path(\"plots\").mkdir(exist_ok=True)\n",
    "Path(\"tables\").mkdir(exist_ok=True)\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "COLORS = {\n",
    "    \"gt\": \"#000000\",  # Black\n",
    "    \"ml\": \"#E69F00\",  # Orange\n",
    "    \"nwp\": \"#56B4E9\",  # Light blue\n",
    "    \"error\": \"#CC79A7\",  # Pink\n",
    "}\n",
    "\n",
    "# Line styles and markers for accessibility\n",
    "LINE_STYLES = {\n",
    "    \"gt\": (\"solid\", \"o\"),\n",
    "    \"ml\": (\"dashed\", \"s\"),\n",
    "    \"nwp\": (\"dotted\", \"^\"),\n",
    "}\n",
    "\n",
    "# Set global font sizes\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": FONT_SIZES[\"axes\"],\n",
    "    \"axes.titlesize\": FONT_SIZES[\"axes\"],\n",
    "    \"axes.labelsize\": FONT_SIZES[\"axes\"],\n",
    "    \"xtick.labelsize\": FONT_SIZES[\"ticks\"],\n",
    "    \"ytick.labelsize\": FONT_SIZES[\"ticks\"],\n",
    "    \"legend.fontsize\": FONT_SIZES[\"legend\"],\n",
    "    \"figure.titlesize\": FONT_SIZES[\"suptitle\"],\n",
    "})\n",
    "\n",
    "# Colorblind-friendly colormap for 2D plots\n",
    "COLORMAP = \"viridis\"\n",
    "\n",
    "\n",
    "def save_plot(fig, name, time=None, remove_title=True, dpi=300):\n",
    "    \"\"\"Helper function to save plots consistently\n",
    "\n",
    "    Args:\n",
    "        fig: matplotlib figure object\n",
    "        name (str): base name for the plot file\n",
    "        time (datetime, optional): timestamp to append to filename\n",
    "        remove_title (bool): remove suptitle/title hierarchically if True\n",
    "        dpi (int): resolution for the saved figure, defaults to 300\n",
    "    \"\"\"\n",
    "    if time is not None:\n",
    "        name = f\"{name}_{time.dt.strftime('%Y%m%d_%H').values}\"\n",
    "\n",
    "    # Sanitize filename by replacing problematic characters\n",
    "    safe_name = name.replace(\"/\", \"_per_\")\n",
    "\n",
    "    # Normalize the path and ensure plots directory exists\n",
    "    plot_dir = Path(\"plots\")\n",
    "    plot_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Remove titles if requested\n",
    "    if remove_title:\n",
    "        if hasattr(fig, \"texts\") and fig.texts:  # Check for suptitle\n",
    "            fig.suptitle(\"\")\n",
    "        ax = fig.gca()\n",
    "        if ax.get_title():\n",
    "            ax.set_title(\"\")\n",
    "\n",
    "    pdf_path = plot_dir / f\"observations_{safe_name}.pdf\"\n",
    "    fig.savefig(pdf_path, bbox_inches=\"tight\", dpi=dpi)\n",
    "\n",
    "\n",
    "def export_table(df, name, caption=\"\"):\n",
    "    \"\"\"Helper function to export tables consistently\"\"\"\n",
    "    # Export to LaTeX with caption\n",
    "    latex_str = df.to_latex(\n",
    "        float_format=\"%.4f\", caption=caption, label=f\"tab:{name}\"\n",
    "    )\n",
    "    with open(f\"tables/observations_{name}.tex\", \"w\") as f:\n",
    "        f.write(latex_str)\n",
    "\n",
    "    # Export to CSV\n",
    "    df.to_csv(f\"tables/observations_{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ab4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nwp = xr.open_zarr(PATH_NWP)\n",
    "ds_nwp = ds_nwp.sel(time=slice(*START_TIMES))\n",
    "ds_nwp = ds_nwp[VARIABLES_NWP.keys()].rename(VARIABLES_NWP)\n",
    "ds_nwp = ds_nwp.rename_dims({\n",
    "    \"lead_time\": \"elapsed_forecast_duration\",\n",
    "    \"time\": \"start_time\",\n",
    "})\n",
    "ds_nwp = ds_nwp.rename_vars({\n",
    "    \"lead_time\": \"elapsed_forecast_duration\",\n",
    "    \"time\": \"start_time\",\n",
    "    \"lon\": \"longitude\",\n",
    "    \"lat\": \"latitude\",\n",
    "})\n",
    "forecast_times = (\n",
    "    ds_nwp.start_time.values[:, None] + ds_nwp.elapsed_forecast_duration.values\n",
    ")\n",
    "ds_nwp = ds_nwp.assign_coords(\n",
    "    forecast_time=(\n",
    "        (\"start_time\", \"elapsed_forecast_duration\"),\n",
    "        forecast_times,\n",
    "    )\n",
    ")\n",
    "\n",
    "# # Calculate hourly values by taking differences along elapsed_forecast_duration\n",
    "ds_nwp[\"precipitation\"] = ds_nwp.precipitation.diff(\n",
    "    dim=\"elapsed_forecast_duration\"\n",
    ")\n",
    "# The NWP data starts at elapsed forecast duration 0 = start_time\n",
    "ds_nwp = ds_nwp.drop_isel(elapsed_forecast_duration=0).isel(\n",
    "    elapsed_forecast_duration=ELAPSED_FORECAST_DURATION\n",
    ")\n",
    "\n",
    "ds_nwp = ds_nwp.transpose(\"start_time\", \"elapsed_forecast_duration\", \"x\", \"y\")\n",
    "ds_nwp = ds_nwp[\n",
    "    [\n",
    "        \"start_time\",\n",
    "        \"elapsed_forecast_duration\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_NWP.values(),\n",
    "    ]\n",
    "]\n",
    "\n",
    "ds_nwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b64d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ds_ml = xr.open_zarr(PATH_ML)\n",
    "ds_ml = ds_ml.sel(state_feature=list(VARIABLES_ML.keys()))\n",
    "ds_ml = ds_ml.sel(start_time=slice(*START_TIMES))\n",
    "for feature in ds_ml.state_feature.values:\n",
    "    ds_ml[VARIABLES_ML[feature]] = ds_ml[\"state\"].sel(state_feature=feature)\n",
    "forecast_times = (\n",
    "    ds_ml.start_time.values[:, None] + ds_ml.elapsed_forecast_duration.values\n",
    ")\n",
    "ds_ml = ds_ml.assign_coords(\n",
    "    forecast_time=(\n",
    "        (\"start_time\", \"elapsed_forecast_duration\"),\n",
    "        forecast_times,\n",
    "    )\n",
    ")\n",
    "ds_ml = ds_ml.drop_vars([\"state\", \"state_feature\", \"time\"])\n",
    "ds_ml = ds_ml.transpose(\"start_time\", \"elapsed_forecast_duration\", \"x\", \"y\")\n",
    "ds_ml = ds_ml[\n",
    "    [\n",
    "        \"start_time\",\n",
    "        \"elapsed_forecast_duration\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_ML.values(),\n",
    "    ]\n",
    "]\n",
    "ds_ml = ds_ml.assign_coords({\n",
    "    \"latitude\": ds_nwp.latitude,\n",
    "    \"longitude\": ds_nwp.longitude,\n",
    "})\n",
    "\n",
    "ds_ml = ds_ml.isel(elapsed_forecast_duration=ELAPSED_FORECAST_DURATION)\n",
    "\n",
    "ds_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_VAR_MAPPING = {\n",
    "    \"air_temperature\": \"temperature_2m\",\n",
    "    \"air_pressure\": \"surface_pressure\",\n",
    "    \"precipitation\": \"precipitation\",\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_wind_components(\n",
    "    ds, speed_var=\"wind_speed\", dir_var=\"wind_direction\"\n",
    "):\n",
    "    \"\"\"Calculate u and v wind components from speed and direction.\"\"\"\n",
    "    ds = ds.copy()\n",
    "    ds[\"wind_u_10m\"] = ds[speed_var] * np.cos(np.radians(90 - ds[dir_var]))\n",
    "    ds[\"wind_v_10m\"] = ds[speed_var] * np.sin(np.radians(90 - ds[dir_var]))\n",
    "    ds = ds.drop_vars([speed_var, dir_var])\n",
    "    return ds\n",
    "\n",
    "\n",
    "ds_obs = xr.open_zarr(PATH_OBS)\n",
    "ds_obs = ds_obs[VARIABLES_OBS].rename_vars(OBS_VAR_MAPPING)\n",
    "ds_obs = ds_obs.sel(time=np.unique(ds_ml.forecast_time.values.flatten()))\n",
    "ds_obs = ds_obs.where(ds_obs != 32767, np.nan)\n",
    "ds_obs = calculate_wind_components(ds_obs)\n",
    "ds_obs[\"temperature_2m\"] += 273.15  # Convert to Kelvin\n",
    "ds_obs[\"surface_pressure\"] *= 100  # Convert to Pa\n",
    "ds_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea4254",
   "metadata": {},
   "source": [
    "Check for missing data in any of the variables. If you have missing data and don't treat it properly, the corresponding time steps will be removed from the evaluation. This can lead to a bias in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_data(ds_obs):\n",
    "    \"\"\"\n",
    "    Create a 2D table of missing data percentages by variable and month.\n",
    "\n",
    "    Args:\n",
    "        ds_obs (xarray.Dataset): Observation dataset\n",
    "    \"\"\"\n",
    "    # Convert time to pandas datetime for month extraction\n",
    "    times = pd.DatetimeIndex(ds_obs.time.values)\n",
    "\n",
    "    # Calculate total stations with missing data\n",
    "    stations_with_missing = len([\n",
    "        station\n",
    "        for station in ds_obs.station\n",
    "        if np.isnan(ds_obs.sel(station=station)).any()\n",
    "    ])\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    missing_by_month = {}\n",
    "\n",
    "    # Calculate percentages for each variable and month\n",
    "    for var in ds_obs.data_vars:\n",
    "        missing_by_month[var] = {}\n",
    "        for month in range(1, 13):\n",
    "            month_mask = times.month == month\n",
    "            if not any(month_mask):\n",
    "                continue\n",
    "\n",
    "            total_elements = ds_obs.sizes[\"station\"] * month_mask.sum()\n",
    "            n_missing = (\n",
    "                np.isnan(ds_obs[var].sel(time=ds_obs.time[month_mask]))\n",
    "                .sum()\n",
    "                .values\n",
    "            )\n",
    "            missing_by_month[var][month] = (n_missing / total_elements) * 100\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(missing_by_month)\n",
    "\n",
    "    # Format percentages to 2 decimal places\n",
    "    df = df.round(2)\n",
    "\n",
    "    # Print summary and table\n",
    "    print(\n",
    "        f\"\\nMissing Data Analysis: {stations_with_missing} out of {ds_obs.sizes['station']} stations affected\"\n",
    "    )\n",
    "    print(\"Percentage of Missing Values by Variable and Month:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Call the function\n",
    "if CHECK_MISSING:\n",
    "    analyze_missing_data(ds_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ds_obs.sizes[\"time\"] == len(\n",
    "    np.unique(ds_ml.forecast_time.values.flatten())\n",
    "), (\n",
    "    f\"Number of time steps do not match: {ds_obs.sizes['time']} != {len(np.unique(ds_ml.forecast_time.values.flatten()))}\"\n",
    ")\n",
    "assert ds_ml.sizes[\"start_time\"] == ds_nwp.sizes[\"start_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ba6ed",
   "metadata": {},
   "source": [
    "### 1. Maps\n",
    "\n",
    "**Random Time Selection:** A random time step is selected to avoid bias in the comparison, ensuring that the assessment is representative of typical model performance.\n",
    "\n",
    "**Consistent Color Scales:** By setting the same minimum and maximum values across all datasets for each variable, we ensure that color differences in the plots reflect true discrepancies, not artifacts of scaling.\n",
    "\n",
    "**Spatial Patterns:** The plots reveal how the ML model and NWP model represent geographical features like weather fronts, high and low-pressure systems, and temperature gradients. Visual comparisons can immediately highlight areas where the models perform well or poorly, guiding further investigation.\n",
    "\n",
    "**Edge Effects:** Near the boundaries, artifacts may occur as the model does not calculate a loss in the boundary region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the map extent based on the ML data\n",
    "extent = [\n",
    "    ds_ml.longitude.min().values + 6.2,\n",
    "    ds_ml.longitude.max().values - 6.7,\n",
    "    ds_ml.latitude.min().values + 3.4,\n",
    "    ds_ml.latitude.max().values - 2.4,\n",
    "]\n",
    "if PLOT_TIME is None:\n",
    "    plot_time = None\n",
    "else:\n",
    "    plot_time = ds_ml.sel(start_time=PLOT_TIME).start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c3b31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def add_map_features(axes):\n",
    "    n_rows, _ = axes.shape\n",
    "    for i, ax_row in enumerate(axes):\n",
    "        for j, ax in enumerate(ax_row):\n",
    "            ax.coastlines(resolution=\"50m\")\n",
    "            ax.add_feature(cfeature.BORDERS, linestyle=\"-\", alpha=0.7)\n",
    "            gl = ax.gridlines(\n",
    "                draw_labels=True, dms=True, x_inline=False, y_inline=False\n",
    "            )\n",
    "\n",
    "            # Turn off all labels by default\n",
    "            gl.top_labels = False\n",
    "            gl.bottom_labels = False\n",
    "            gl.left_labels = False\n",
    "            gl.right_labels = False\n",
    "\n",
    "            # Enable left labels only for leftmost column\n",
    "            if j == 0:\n",
    "                gl.left_labels = True\n",
    "\n",
    "            # Enable bottom labels only for last row\n",
    "            if i == n_rows - 1:\n",
    "                gl.bottom_labels = True\n",
    "\n",
    "\n",
    "def plot_comparison_maps(ds_obs, ds_nwp, ds_ml, plot_time=None, variables=None):\n",
    "    \"\"\"\n",
    "    Plot comparison between observations, NWP and ML data for each variable and forecast step.\n",
    "\n",
    "    Args:\n",
    "        ds_obs (xarray.Dataset): Observations dataset\n",
    "        ds_nwp (xarray.Dataset): NWP forecast data\n",
    "        ds_ml (xarray.Dataset): ML forecast data\n",
    "        plot_time (str): Time for plot title\n",
    "        variables (list): List of variables to plot. If None, plots all common variables\n",
    "    \"\"\"\n",
    "    # Convert plot_time to pandas datetime if it's a string\n",
    "    if isinstance(plot_time, str):\n",
    "        plot_time = pd.to_datetime(plot_time)\n",
    "\n",
    "    # Get common variables if not specified\n",
    "    if variables is None:\n",
    "        variables = list(set(ds_nwp.data_vars).intersection(ds_ml.data_vars))\n",
    "\n",
    "    # Get number of forecast steps\n",
    "    n_steps = len(ds_ml.elapsed_forecast_duration)\n",
    "\n",
    "    for var in variables:\n",
    "        # Create figure with n_steps rows and 3 columns\n",
    "        fig = plt.figure(\n",
    "            figsize=(15, 3 * n_steps),\n",
    "            dpi=100,  # Increased height multiplier\n",
    "        )\n",
    "        axes = np.array([\n",
    "            [\n",
    "                plt.subplot(n_steps, 3, i * 3 + j + 1, projection=PROJECTION)\n",
    "                for j in range(3)\n",
    "            ]\n",
    "            for i in range(n_steps)\n",
    "        ])\n",
    "\n",
    "        # Set extent for all subplots\n",
    "        for ax_row in axes:\n",
    "            for ax in ax_row:\n",
    "                ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "        # Initialize arrays for global min/max\n",
    "        arrays_for_minmax = []\n",
    "\n",
    "        # Collect data for min/max calculation across all timesteps\n",
    "        for step_idx, step in enumerate(ds_ml.elapsed_forecast_duration):\n",
    "            forecast_time = ds_ml.forecast_time.sel(\n",
    "                start_time=plot_time, elapsed_forecast_duration=step\n",
    "            )\n",
    "\n",
    "            obs_data = ds_obs.sel(time=forecast_time)[var]\n",
    "            nwp_data = ds_nwp.sel(\n",
    "                start_time=plot_time, elapsed_forecast_duration=step\n",
    "            )[var]\n",
    "            ml_data = ds_ml.sel(\n",
    "                start_time=plot_time, elapsed_forecast_duration=step\n",
    "            )[var]\n",
    "\n",
    "            arrays_for_minmax.extend([\n",
    "                obs_data.values,\n",
    "                nwp_data.values,\n",
    "                ml_data.values,\n",
    "            ])\n",
    "\n",
    "        # Calculate global min/max\n",
    "        vmin = min(np.nanmin(arr) for arr in arrays_for_minmax)\n",
    "        vmax = max(np.nanmax(arr) for arr in arrays_for_minmax)\n",
    "\n",
    "        # Plot for each forecast step\n",
    "        for step_idx, step in enumerate(ds_ml.elapsed_forecast_duration):\n",
    "            forecast_time = ds_ml.forecast_time.sel(\n",
    "                start_time=plot_time, elapsed_forecast_duration=step\n",
    "            )\n",
    "            forecast_hours = int(step.values / 1e9 / 3600)  # Convert to hours\n",
    "\n",
    "            # Plot observations\n",
    "            obs_data = ds_obs.sel(time=forecast_time)[var]\n",
    "            axes[step_idx, 0].scatter(\n",
    "                ds_obs.longitude,\n",
    "                ds_obs.latitude,\n",
    "                c=obs_data,\n",
    "                cmap=\"viridis\",\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                transform=ccrs.PlateCarree(),\n",
    "            )\n",
    "\n",
    "            # Plot NWP data\n",
    "            nwp_data = ds_nwp.sel(\n",
    "                start_time=plot_time, elapsed_forecast_duration=step\n",
    "            )[var]\n",
    "            axes[step_idx, 1].pcolormesh(\n",
    "                ds_nwp.longitude,\n",
    "                ds_nwp.latitude,\n",
    "                nwp_data,\n",
    "                cmap=\"viridis\",\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                transform=ccrs.PlateCarree(),\n",
    "                shading=\"auto\",\n",
    "                rasterized=True,\n",
    "            )\n",
    "\n",
    "            # Plot ML data\n",
    "            ml_data = ds_ml.sel(\n",
    "                start_time=plot_time, elapsed_forecast_duration=step\n",
    "            )[var]\n",
    "            im2 = axes[step_idx, 2].pcolormesh(\n",
    "                ds_ml.longitude,\n",
    "                ds_ml.latitude,\n",
    "                ml_data,\n",
    "                cmap=\"viridis\",\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "                transform=ccrs.PlateCarree(),\n",
    "                shading=\"auto\",\n",
    "                rasterized=True,\n",
    "            )\n",
    "\n",
    "            # Add titles for first row\n",
    "            if step_idx == 0:\n",
    "                axes[step_idx, 0].set_title(f\"Observations\\n+{forecast_hours}h\")\n",
    "                axes[step_idx, 1].set_title(f\"NWP\\n+{forecast_hours}h\")\n",
    "                axes[step_idx, 2].set_title(f\"ML\\n+{forecast_hours}h\")\n",
    "            else:\n",
    "                axes[step_idx, 0].set_title(f\"+{forecast_hours}h\")\n",
    "                axes[step_idx, 1].set_title(f\"+{forecast_hours}h\")\n",
    "                axes[step_idx, 2].set_title(f\"+{forecast_hours}h\")\n",
    "\n",
    "        # Add map features\n",
    "        add_map_features(axes)\n",
    "\n",
    "        # Adjust subplot spacing\n",
    "        plt.subplots_adjust(\n",
    "            top=0.9,\n",
    "            bottom=0.05,\n",
    "            hspace=0.15,\n",
    "            wspace=0.03,\n",
    "        )\n",
    "\n",
    "        # Add colorbar with adjusted position\n",
    "        cbar_ax = fig.add_axes([0.15, 0.0, 0.7, 0.02])\n",
    "        plt.colorbar(\n",
    "            im2,\n",
    "            cax=cbar_ax,\n",
    "            orientation=\"horizontal\",\n",
    "            label=f\"[{VARIABLE_UNITS[var]}]\",\n",
    "        )\n",
    "\n",
    "        # Set overall title with adjusted position\n",
    "        plt.suptitle(\n",
    "            f\"{var} Comparison at {str(plot_time.dt.date.values)} - {str(plot_time.dt.hour.values)} UTC\",\n",
    "            y=0.95,\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "        save_plot(fig, f\"comparison_{var}_multi_step\", time=plot_time)\n",
    "        plt.close()\n",
    "\n",
    "# Call the function\n",
    "plot_comparison_maps(ds_obs, ds_nwp, ds_ml, plot_time=plot_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181ba84",
   "metadata": {},
   "source": [
    "Interpolation from the gridded model data to the station locations is done using the nearest neighbor method. Since we are only looking at surface level data, this is a reasonable approach. However, if you are working with data at different levels, you may want to consider a more sophisticated interpolation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17daa0c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def interpolate_to_obs(\n",
    "    ds_model_1, ds_model_2, ds_obs, vars_plot, neighbors=None\n",
    "):\n",
    "    \"\"\"Optimized function to interpolate model datasets to observation points using xarray/dask.\"\"\"\n",
    "\n",
    "    print(\"Starting optimized interpolation...\")\n",
    "\n",
    "    # Extract observation coordinates\n",
    "    points_obs = np.column_stack([\n",
    "        ds_obs.latitude.values,\n",
    "        ds_obs.longitude.values,\n",
    "    ])\n",
    "\n",
    "    # Extract model coordinates from 2D lat/lon arrays\n",
    "    model_lats = ds_model_1.latitude\n",
    "    model_lons = ds_model_1.longitude\n",
    "\n",
    "    points_model = np.column_stack([\n",
    "        model_lats.values.ravel(),\n",
    "        model_lons.values.ravel(),\n",
    "    ])\n",
    "\n",
    "    # Build KDTree\n",
    "    print(\"Building KD-tree and finding neighbors...\")\n",
    "    k = 4 if neighbors is None else neighbors\n",
    "    kdtree = cKDTree(points_model)\n",
    "    distances, flat_indices = kdtree.query(points_obs, k=k)\n",
    "\n",
    "    # Convert flat indices back to 2D indices\n",
    "    _, ny = ds_model_1.x.size, ds_model_1.y.size\n",
    "    x_indices = flat_indices // ny\n",
    "    y_indices = flat_indices % ny\n",
    "\n",
    "    # Precompute weights with proper dimensions\n",
    "    weights = xr.DataArray(\n",
    "        1.0 / (distances + 1e-10), dims=[\"station\", \"neighbor\"]\n",
    "    )\n",
    "    weights = weights / weights.sum(\"neighbor\")\n",
    "\n",
    "    def interpolate_variable(var):\n",
    "        print(f\"Processing variable: {var}\")\n",
    "\n",
    "        # Select nearest neighbors for both datasets\n",
    "        data_1 = ds_model_1[var].isel(\n",
    "            x=xr.DataArray(x_indices, dims=[\"station\", \"neighbor\"]),\n",
    "            y=xr.DataArray(y_indices, dims=[\"station\", \"neighbor\"]),\n",
    "        )\n",
    "        data_2 = ds_model_2[var].isel(\n",
    "            x=xr.DataArray(x_indices, dims=[\"station\", \"neighbor\"]),\n",
    "            y=xr.DataArray(y_indices, dims=[\"station\", \"neighbor\"]),\n",
    "        )\n",
    "\n",
    "        # Create interpolated datasets using weighted sum along neighbor dimension\n",
    "        result_1 = (data_1 * weights).sum(dim=\"neighbor\")\n",
    "        result_2 = (data_2 * weights).sum(dim=\"neighbor\")\n",
    "\n",
    "        return var, result_1, result_2\n",
    "\n",
    "    # Process all variables\n",
    "    results = {}\n",
    "    for var in vars_plot:\n",
    "        var_name, data_1, data_2 = interpolate_variable(var)\n",
    "        results[var_name] = (data_1, data_2)\n",
    "\n",
    "    # Create output datasets\n",
    "    ds_interp_1 = xr.Dataset(\n",
    "        {var: results[var][0] for var in vars_plot},\n",
    "        coords={\n",
    "            \"start_time\": ds_model_1.start_time,\n",
    "            \"elapsed_forecast_duration\": ds_model_1.elapsed_forecast_duration,\n",
    "            \"station\": ds_obs.station,\n",
    "            \"forecast_time\": ds_model_1.forecast_time,\n",
    "            \"latitude\": ds_obs.latitude,\n",
    "            \"longitude\": ds_obs.longitude,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ds_interp_2 = xr.Dataset(\n",
    "        {var: results[var][1] for var in vars_plot},\n",
    "        coords={\n",
    "            \"start_time\": ds_model_2.start_time,\n",
    "            \"elapsed_forecast_duration\": ds_model_2.elapsed_forecast_duration,\n",
    "            \"station\": ds_obs.station,\n",
    "            \"forecast_time\": ds_model_2.forecast_time,\n",
    "            \"latitude\": ds_obs.latitude,\n",
    "            \"longitude\": ds_obs.longitude,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return ds_interp_1, ds_interp_2\n",
    "\n",
    "\n",
    "ds_nwp_interp, ds_ml_interp = interpolate_to_obs(\n",
    "    ds_nwp,\n",
    "    ds_ml,\n",
    "    ds_obs,\n",
    "    VARIABLES_ML.values(),\n",
    ")\n",
    "\n",
    "with ProgressBar():\n",
    "    print(\"Computing interpolated datasets...\")\n",
    "    ds_nwp_interp = ds_nwp_interp.compute()\n",
    "    print(\"NWP interpolation done.\")\n",
    "    ds_ml_interp = ds_ml_interp.compute()\n",
    "    print(\"ML interpolation done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559ad7a",
   "metadata": {},
   "source": [
    "After intepolation all three datasets contain the same number of station-level data points.\n",
    "These datapoints are visualised on a map to show the spatial distribution of the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db450ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the interpolated model data\n",
    "def plot_comparison_interpolated(\n",
    "    ds_obs, ds_interp_1, ds_interp_2, var_plot, plot_time=None\n",
    "):\n",
    "    \"\"\"Plot comparison between observations and interpolated model datasets across forecast steps.\"\"\"\n",
    "    if isinstance(plot_time, str):\n",
    "        plot_time = pd.to_datetime(plot_time)\n",
    "\n",
    "    n_steps = len(ds_interp_1.elapsed_forecast_duration)\n",
    "\n",
    "    # Increased top margin for suptitle\n",
    "    fig = plt.figure(figsize=(15, 3 * n_steps + 0.5), dpi=100)\n",
    "    axes = np.array([\n",
    "        [\n",
    "            plt.subplot(n_steps, 3, i * 3 + j + 1, projection=PROJECTION)\n",
    "            for j in range(3)\n",
    "        ]\n",
    "        for i in range(n_steps)\n",
    "    ])\n",
    "\n",
    "    # Set extent for all subplots\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "    arrays_for_minmax = []\n",
    "    for step_idx, step in enumerate(ds_interp_1.elapsed_forecast_duration):\n",
    "        forecast_time = ds_interp_1.forecast_time.sel(\n",
    "            start_time=plot_time, elapsed_forecast_duration=step\n",
    "        )\n",
    "        arrays_for_minmax.extend([\n",
    "            ds_obs[var_plot].sel(time=forecast_time).values,\n",
    "            ds_interp_1[var_plot]\n",
    "            .sel(start_time=plot_time, elapsed_forecast_duration=step)\n",
    "            .values,\n",
    "            ds_interp_2[var_plot]\n",
    "            .sel(start_time=plot_time, elapsed_forecast_duration=step)\n",
    "            .values,\n",
    "        ])\n",
    "\n",
    "    vmin = min(np.nanmin(arr) for arr in arrays_for_minmax)\n",
    "    vmax = max(np.nanmax(arr) for arr in arrays_for_minmax)\n",
    "\n",
    "    for step_idx, step in enumerate(ds_interp_1.elapsed_forecast_duration):\n",
    "        forecast_time = ds_interp_1.forecast_time.sel(\n",
    "            start_time=plot_time, elapsed_forecast_duration=step\n",
    "        )\n",
    "        forecast_hours = int(step.values / 1e9 / 3600)\n",
    "\n",
    "        for ax_idx, (ax, data, title) in enumerate(\n",
    "            zip(\n",
    "                axes[step_idx],\n",
    "                [ds_obs, ds_interp_1, ds_interp_2],\n",
    "                [\"Observations\", \"NWP\", \"ML\"],\n",
    "            )\n",
    "        ):\n",
    "            if ax_idx == 0:\n",
    "                scatter = ax.scatter(\n",
    "                    ds_obs.longitude,\n",
    "                    ds_obs.latitude,\n",
    "                    c=data[var_plot].sel(time=forecast_time),\n",
    "                    cmap=COLORMAP,\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax,\n",
    "                    rasterized=True,\n",
    "                )\n",
    "            else:\n",
    "                scatter = ax.scatter(\n",
    "                    ds_obs.longitude,\n",
    "                    ds_obs.latitude,\n",
    "                    c=data[var_plot].sel(\n",
    "                        start_time=plot_time, elapsed_forecast_duration=step\n",
    "                    ),\n",
    "                    cmap=COLORMAP,\n",
    "                    transform=ccrs.PlateCarree(),\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax,\n",
    "                    rasterized=True,\n",
    "                )\n",
    "\n",
    "            ax.add_feature(cfeature.COASTLINE, edgecolor=\"grey\")\n",
    "            ax.add_feature(cfeature.BORDERS, linestyle=\":\", edgecolor=\"grey\")\n",
    "\n",
    "            # Configure gridlines - only on edges\n",
    "            gl = ax.gridlines(\n",
    "                draw_labels=True, alpha=0.2, x_inline=False, y_inline=False\n",
    "            )\n",
    "            gl.top_labels = False\n",
    "            gl.right_labels = False\n",
    "            gl.left_labels = ax_idx == 0  # Only leftmost column\n",
    "            gl.bottom_labels = step_idx == n_steps - 1  # Only bottom row\n",
    "\n",
    "            if step_idx == 0:\n",
    "                ax.set_title(f\"{title}\\n+{forecast_hours}h\")\n",
    "            else:\n",
    "                ax.set_title(f\"+{forecast_hours}h\")\n",
    "\n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(\n",
    "        top=0.9,\n",
    "        bottom=0.05,\n",
    "        hspace=0.15,\n",
    "        wspace=0.03,\n",
    "    )\n",
    "\n",
    "    # Add colorbar with adjusted position\n",
    "    cbar_ax = fig.add_axes([0.15, 0.0, 0.7, 0.02])\n",
    "    plt.colorbar(\n",
    "        scatter,\n",
    "        cax=cbar_ax,\n",
    "        orientation=\"horizontal\",\n",
    "        label=VARIABLE_UNITS[var_plot],\n",
    "    )\n",
    "\n",
    "    # Adjusted suptitle position\n",
    "    plt.suptitle(\n",
    "        f\"{var_plot} Comparison at {str(plot_time.dt.date.values)} - {str(plot_time.dt.hour.values)} UTC\",\n",
    "        y=0.95,  # Higher position\n",
    "        fontsize=14,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Call the function for each variable\n",
    "for var in VARIABLES_ML.values():\n",
    "    fig = plot_comparison_interpolated(\n",
    "        ds_obs,\n",
    "        ds_nwp_interp,\n",
    "        ds_ml_interp,\n",
    "        var,\n",
    "        plot_time=plot_time,\n",
    "    )\n",
    "    plt.show()\n",
    "    save_plot(fig, f\"interpolated_comparison_panel_{var}\", time=plot_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7f540",
   "metadata": {},
   "source": [
    "### 2. Histograms\n",
    "By examining these distributions, we can assess whether the ML model and NWP model accurately capture the variability and frequency of different atmospheric states.\n",
    "\n",
    "**Distribution Shape:** The histograms show whether the models replicate the skewness, kurtosis, and overall shape of the ground truth data distributions.\n",
    "\n",
    "**Extreme Values:** Identifying how the models handle extreme conditions, such as unusually high or low temperatures, is crucial for weather prediction and risk assessment.\n",
    "\n",
    "**Normalization Needs:** Differences in scale between variables suggest that normalization may be necessary for accurate comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722053ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interpolated_histograms(\n",
    "    ds_obs, ds_nwp_interp, ds_ml_interp, subsample=10\n",
    "):\n",
    "    \"\"\"Plot histograms for interpolated station data comparing observations, NWP and ML predictions.\n",
    "\n",
    "    Args:\n",
    "        ds_obs: xarray Dataset containing observations\n",
    "        ds_nwp_interp: xarray Dataset containing interpolated NWP predictions\n",
    "        ds_ml_interp: xarray Dataset containing interpolated ML predictions\n",
    "        subsample: int, subsampling factor for faster plotting\n",
    "    \"\"\"\n",
    "    # Sample data for faster plotting\n",
    "    ds_obs_sampled = ds_obs.isel(\n",
    "        time=slice(None, None, subsample), station=slice(None, None, subsample)\n",
    "    )\n",
    "    ds_nwp_sampled = ds_nwp_interp.isel(\n",
    "        start_time=slice(None, None, subsample),\n",
    "        station=slice(None, None, subsample),\n",
    "    )\n",
    "    ds_ml_sampled = ds_ml_interp.isel(\n",
    "        start_time=slice(None, None, subsample),\n",
    "        station=slice(None, None, subsample),\n",
    "    )\n",
    "\n",
    "    for variable_name in VARIABLES_ML.values():\n",
    "        if variable_name not in ds_obs:\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(11, 7), dpi=DPI)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        data_obs = ds_obs_sampled[variable_name].values.flatten()\n",
    "        data_ml = ds_ml_sampled[variable_name].values.flatten()\n",
    "        data_nwp = ds_nwp_sampled[variable_name].values.flatten()\n",
    "\n",
    "        # Create histograms for observations\n",
    "        ax.hist(\n",
    "            data_obs,\n",
    "            bins=300,\n",
    "            density=True,\n",
    "            color=COLORS[\"gt\"],\n",
    "            label=\"Observations\",\n",
    "            histtype=\"stepfilled\",\n",
    "            linewidth=0,\n",
    "        )\n",
    "\n",
    "        # Plot NWP interpolated data\n",
    "        ax.hist(\n",
    "            data_nwp,\n",
    "            bins=300,\n",
    "            alpha=0.8,\n",
    "            density=True,\n",
    "            color=COLORS[\"nwp\"],\n",
    "            label=\"NWP Interpolated\",\n",
    "            histtype=\"stepfilled\",\n",
    "            linewidth=0,\n",
    "        )\n",
    "\n",
    "        # Create histogram for ML interpolated data\n",
    "        ax.hist(\n",
    "            data_ml,\n",
    "            bins=300,\n",
    "            alpha=0.8,\n",
    "            density=True,\n",
    "            color=COLORS[\"ml\"],\n",
    "            label=\"ML Interpolated\",\n",
    "            histtype=\"stepfilled\",\n",
    "            linewidth=0,\n",
    "        )\n",
    "\n",
    "        # Add labels and title\n",
    "        units = VARIABLE_UNITS[variable_name]\n",
    "        ax.set_title(\n",
    "            f\"Distribution of {variable_name} at Station Locations\", pad=20\n",
    "        )\n",
    "        ax.set_xlabel(f\"{units}\")\n",
    "\n",
    "        # Place legend in top left\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(0.02, 0.98))\n",
    "\n",
    "        # Adjust axis limits\n",
    "        current_ylim = ax.get_ylim()\n",
    "        ax.set_ylim(0, current_ylim[1] * 1.3)\n",
    "\n",
    "        # Calculate statistics\n",
    "        stats_data = {\n",
    "            \"Obs\": [\n",
    "                f\"{skew(data_obs, nan_policy='omit'):.2f}\",\n",
    "                f\"{kurtosis(data_obs, nan_policy='omit'):.2f}\",\n",
    "            ],\n",
    "            \"NWP\": [\n",
    "                f\"{skew(data_nwp, nan_policy='omit'):.2f}\",\n",
    "                f\"{kurtosis(data_nwp, nan_policy='omit'):.2f}\",\n",
    "            ],\n",
    "            \"ML\": [\n",
    "                f\"{skew(data_ml, nan_policy='omit'):.2f}\",\n",
    "                f\"{kurtosis(data_ml, nan_policy='omit'):.2f}\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # Create and position table\n",
    "        col_labels = [\"Skewness\", \"Kurtosis\"]\n",
    "        row_labels = list(stats_data.keys())\n",
    "        cell_text = [\n",
    "            [stats_data[row][i] for i in range(2)] for row in row_labels\n",
    "        ]\n",
    "\n",
    "        table = ax.table(\n",
    "            cellText=cell_text,\n",
    "            rowLabels=row_labels,\n",
    "            colLabels=col_labels,\n",
    "            cellLoc=\"center\",\n",
    "            loc=\"upper right\",\n",
    "            bbox=[0.72, 0.78, 0.25, 0.18],\n",
    "        )\n",
    "\n",
    "        # Style table\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(13)\n",
    "\n",
    "        for (row, col), cell in table._cells.items():\n",
    "            cell.set_text_props(wrap=True)\n",
    "            cell.set_facecolor(\"white\")\n",
    "            cell.set_alpha(0.9)\n",
    "            cell.set_edgecolor(\"#D3D3D3\")\n",
    "\n",
    "            if col == -1:\n",
    "                cell.set_width(0.15)\n",
    "                cell.set_text_props(horizontalalignment=\"right\")\n",
    "            else:\n",
    "                cell.set_width(0.12)\n",
    "                cell.set_text_props(horizontalalignment=\"center\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "        save_plot(fig, f\"histogram_interpolated_{variable_name}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Call the function after interpolation\n",
    "plot_interpolated_histograms(\n",
    "    ds_obs, ds_nwp_interp, ds_ml_interp, subsample=SUBSAMPLE_HISTOGRAM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76360a",
   "metadata": {},
   "source": [
    "### 5. Various Verification Metrics\n",
    "The final chapter consolidates various statistical metrics to provide a broad\n",
    "evaluation of the ML model's performance. By considering multiple metrics, we\n",
    "gain a nuanced understanding of both the strengths and weaknesses of the model.\n",
    "\n",
    "**Metric Diversity:** Including ME, STDEV-ERR, MAE, RMSE, MSE, Pearson correlation, covers different aspects of model performance, from average errors to spatial pattern accuracy.\n",
    "\n",
    "**ME** (Mean Error): Indicates the average discrepancy between the model and ground truth values. A positive value indicates that the model tends to overestimate, while a negative value suggests underestimation. Also called Bias.\n",
    "\n",
    "**STDEV-ERR** (Standard Deviation of Errors): Shows the variability of errors, highlighting whether the model is consistent in its predictions.\n",
    "\n",
    "**MAE, MSE and RMSE:** Offer insights into the average magnitude of errors, with\n",
    "RMSE emphasizing larger discrepancies. The colors indicating high errors are\n",
    "only implemented for these three metrics with standardization.\n",
    "\n",
    "**Pearson Correlation:** Assesses the linear relationship, indicating whether\n",
    "the model captures variability even if biases exist.\n",
    "\n",
    "**Wasserstein Distance:** Provides a holistic view of distributional similarity\n",
    "across variables. Same as chapter 3.\n",
    "\n",
    "**Holistic Assessment:** The combination of metrics provides a comprehensive\n",
    "performance profile, essential for model validation and comparison. More complex metrics are explained in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_by_efd(\n",
    "    ds_obs,\n",
    "    ds_nwp=None,\n",
    "    ds_ml=None,\n",
    "    metrics_to_compute=None,\n",
    "    subsample_points=1e7,\n",
    "    prefix=\"metrics\",\n",
    "):\n",
    "    \"\"\"Calculate metrics for each Elapsed Forecast Duration for station data using xarray.\"\"\"\n",
    "    if metrics_to_compute is None:\n",
    "        metrics_to_compute = METRICS\n",
    "\n",
    "    variables = list(ds_obs.data_vars)\n",
    "    elapsed_forecast_durations = ds_ml.elapsed_forecast_duration\n",
    "    elapsed_forecast_durations_hours = elapsed_forecast_durations.values.astype(\n",
    "        \"timedelta64[s]\"\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "\n",
    "    metrics_by_efd = {}\n",
    "    combined_metrics = {}\n",
    "\n",
    "    for efd, lt_hours in zip(\n",
    "        elapsed_forecast_durations, elapsed_forecast_durations_hours\n",
    "    ):\n",
    "        print(\n",
    "            f\"\\nCalculating metrics for elapsed forecast duration: {lt_hours.item():.1f}h\"\n",
    "        )\n",
    "\n",
    "        ds_ml_lead = ds_ml.sel(elapsed_forecast_duration=efd)\n",
    "        if ds_nwp is not None:\n",
    "            ds_nwp_lead = ds_nwp.sel(elapsed_forecast_duration=efd)\n",
    "            ds_nwp_lead[\"start_time\"] = ds_nwp_lead.forecast_time\n",
    "            ds_nwp_lead = ds_nwp_lead.rename_dims({\n",
    "                \"start_time\": \"time\"\n",
    "            }).rename_vars({\"start_time\": \"time\"})\n",
    "\n",
    "        ds_ml_lead[\"start_time\"] = ds_ml_lead.forecast_time\n",
    "        ds_ml_lead = ds_ml_lead.rename_dims({\"start_time\": \"time\"}).rename_vars({\n",
    "            \"start_time\": \"time\"\n",
    "        })\n",
    "\n",
    "        forecast_times = ds_ml_lead.forecast_time.values.flatten()\n",
    "        ds_obs_lead = ds_obs.sel(time=forecast_times)\n",
    "        metrics_dict = {}\n",
    "\n",
    "        for var in variables:\n",
    "            print(f\"Processing {var}\")\n",
    "\n",
    "            # Get data as xarray DataArrays\n",
    "            y_true = ds_obs_lead[var]\n",
    "            y_pred_ml = ds_ml_lead[var]\n",
    "\n",
    "            # Create masks for each dataset\n",
    "            mask_true = xr.where(~np.isnan(y_true), True, False)\n",
    "            mask_ml = xr.where(~np.isnan(y_pred_ml), True, False)\n",
    "\n",
    "            if ds_nwp is not None and var in ds_nwp:\n",
    "                y_pred_nwp = ds_nwp_lead[var]\n",
    "                mask_nwp = xr.where(~np.isnan(y_pred_nwp), True, False)\n",
    "                # Combine masks\n",
    "                valid_mask = mask_true & mask_ml & mask_nwp\n",
    "            else:\n",
    "                valid_mask = mask_true & mask_ml\n",
    "\n",
    "            y_true = y_true.where(valid_mask)\n",
    "            y_pred_ml = y_pred_ml.where(valid_mask)\n",
    "            if ds_nwp is not None and var in ds_nwp:\n",
    "                y_pred_nwp = y_pred_nwp.where(valid_mask)\n",
    "\n",
    "            metrics_dict[var] = {}\n",
    "\n",
    "            # Calculate ML metrics using xarray\n",
    "            if \"MAE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"MAE ML\"] = mae(y_pred_ml, y_true).values\n",
    "            if \"RMSE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"RMSE ML\"] = rmse(y_pred_ml, y_true).values\n",
    "            if \"MSE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"MSE ML\"] = mse(y_pred_ml, y_true).values\n",
    "            if \"ME\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"ME ML\"] = mean_error(\n",
    "                    y_pred_ml, y_true\n",
    "                ).values\n",
    "            if \"STDEV_ERR\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"STDEV_ERR ML\"] = (\n",
    "                    (y_pred_ml - y_true).std().values\n",
    "                )\n",
    "            if \"RelativeMAE\" in metrics_to_compute:\n",
    "                rel_mae = (\n",
    "                    abs(y_pred_ml - y_true) / (abs(y_true) + 1e-6)\n",
    "                ).mean()\n",
    "                metrics_dict[var][\"RelativeMAE ML\"] = rel_mae.values\n",
    "            if \"RelativeRMSE\" in metrics_to_compute:\n",
    "                rel_rmse = np.sqrt(\n",
    "                    ((y_pred_ml - y_true) ** 2 / (y_true**2 + 1e-6)).mean()\n",
    "                )\n",
    "                metrics_dict[var][\"RelativeRMSE ML\"] = rel_rmse.values\n",
    "            if \"PearsonR\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"PearsonR ML\"] = pearsonr(\n",
    "                    y_pred_ml, y_true\n",
    "                ).values\n",
    "            if \"Wasserstein\" in metrics_to_compute:\n",
    "                # Use the valid_mask directly instead of checking for NaN values again\n",
    "                pred_vals = y_pred_ml.values[valid_mask.values]\n",
    "                true_vals = y_true.values[valid_mask.values]\n",
    "                metrics_dict[var][\"Wasserstein ML\"] = wasserstein_distance(\n",
    "                    pred_vals, true_vals\n",
    "                )\n",
    "\n",
    "            # Calculate NWP metrics if available\n",
    "            if ds_nwp is not None and var in ds_nwp:\n",
    "                if \"MAE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"MAE NWP\"] = mae(\n",
    "                        y_pred_nwp, y_true\n",
    "                    ).values\n",
    "                if \"RMSE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"RMSE NWP\"] = rmse(\n",
    "                        y_pred_nwp, y_true\n",
    "                    ).values\n",
    "                if \"MSE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"MSE NWP\"] = mse(\n",
    "                        y_pred_nwp, y_true\n",
    "                    ).values\n",
    "                if \"ME\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"ME NWP\"] = mean_error(\n",
    "                        y_pred_nwp, y_true\n",
    "                    ).values\n",
    "                if \"STDEV_ERR\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"STDEV_ERR NWP\"] = (\n",
    "                        (y_pred_nwp - y_true).std().values\n",
    "                    )\n",
    "                if \"RelativeMAE\" in metrics_to_compute:\n",
    "                    rel_mae = (\n",
    "                        abs(y_pred_nwp - y_true) / (abs(y_true) + 1e-6)\n",
    "                    ).mean()\n",
    "                    metrics_dict[var][\"RelativeMAE NWP\"] = rel_mae.values\n",
    "                if \"RelativeRMSE\" in metrics_to_compute:\n",
    "                    rel_rmse = np.sqrt(\n",
    "                        ((y_pred_nwp - y_true) ** 2 / (y_true**2 + 1e-6)).mean()\n",
    "                    )\n",
    "                    metrics_dict[var][\"RelativeRMSE NWP\"] = rel_rmse.values\n",
    "                if \"PearsonR\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"PearsonR NWP\"] = pearsonr(\n",
    "                        y_pred_nwp, y_true\n",
    "                    ).values\n",
    "                if \"Wasserstein\" in metrics_to_compute:\n",
    "                    # For Wasserstein, we need to convert to numpy arrays\n",
    "                    pred_vals = y_pred_nwp.values[valid_mask.values]\n",
    "                    true_vals = y_true.values[valid_mask.values]\n",
    "                    metrics_dict[var][\"Wasserstein NWP\"] = wasserstein_distance(\n",
    "                        pred_vals, true_vals\n",
    "                    )\n",
    "\n",
    "            # Store combined metrics\n",
    "            for metric_name, value in metrics_dict[var].items():\n",
    "                key = f\"{var}_{metric_name}\"\n",
    "                if key not in combined_metrics:\n",
    "                    combined_metrics[key] = []\n",
    "                combined_metrics[key].append(value)\n",
    "\n",
    "        metrics_by_efd[lt_hours.item()] = pd.DataFrame.from_dict(\n",
    "            metrics_dict, orient=\"index\"\n",
    "        )\n",
    "\n",
    "    # Create combined metrics DataFrame\n",
    "    elapsed_forecast_durations_hours_float = [\n",
    "        x.item() for x in elapsed_forecast_durations_hours\n",
    "    ]\n",
    "    combined_df = pd.DataFrame(\n",
    "        combined_metrics, index=elapsed_forecast_durations_hours_float\n",
    "    )\n",
    "    combined_df.index.name = \"Forecast Hours\"\n",
    "\n",
    "    return metrics_by_efd, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939843b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "metrics_by_efd, combined_metrics = calculate_metrics_by_efd(\n",
    "    ds_obs=ds_obs,\n",
    "    ds_nwp=ds_nwp_interp,\n",
    "    ds_ml=ds_ml_interp,\n",
    "    metrics_to_compute=METRICS,\n",
    ")\n",
    "export_table(combined_metrics, \"combined_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb851c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution of a specific metric over elapsed forecast duration\n",
    "elapsed_forecast_durations = list(metrics_by_efd.keys())\n",
    "metrics_to_compute = METRICS\n",
    "for variable in VARIABLES_ML.values():\n",
    "    for metric in metrics_to_compute:\n",
    "        try:\n",
    "            # Skip if any scores are missing\n",
    "            ml_scores = [\n",
    "                df.loc[variable, f\"{metric} ML\"]\n",
    "                for df in metrics_by_efd.values()\n",
    "            ]\n",
    "            nwp_scores = [\n",
    "                df.loc[variable, f\"{metric} NWP\"]\n",
    "                for df in metrics_by_efd.values()\n",
    "            ]\n",
    "\n",
    "            # Convert elapsed forecast durations from hours to timedelta\n",
    "            hours = [\n",
    "                x / np.timedelta64(1, \"h\")\n",
    "                for x in ds_ml.elapsed_forecast_duration.values\n",
    "            ]\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6), dpi=DPI)\n",
    "\n",
    "            # Plot ML scores\n",
    "            ax.plot(\n",
    "                hours,\n",
    "                ml_scores,\n",
    "                label=\"ML\",\n",
    "                color=COLORS[\"ml\"],\n",
    "                linestyle=LINE_STYLES[\"ml\"][0],\n",
    "                marker=LINE_STYLES[\"ml\"][1],\n",
    "            )\n",
    "\n",
    "            # Plot NWP scores if they exist and are not all NaN\n",
    "            if not all(pd.isna(nwp_scores)):\n",
    "                ax.plot(\n",
    "                    hours,\n",
    "                    nwp_scores,\n",
    "                    label=\"NWP\",\n",
    "                    color=COLORS[\"nwp\"],\n",
    "                    linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "                    marker=LINE_STYLES[\"nwp\"][1],\n",
    "                )\n",
    "\n",
    "            ax.set_xlabel(\"Elapsed Forecast Duration [h]\")\n",
    "            ax.set_ylabel(f\"{metric} [{VARIABLE_UNITS[variable]}]\")\n",
    "            ax.set_title(f\"{metric} Evolution for {variable}\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            save_plot(fig, f\"{metric}_{variable}_evolution\")\n",
    "            plt.close()\n",
    "\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"Skipping {metric} for {variable}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd780d",
   "metadata": {},
   "source": [
    "#### Equitable Threat Score (Traditional Version)\n",
    "Range: [-1/3, 1], where:\n",
    "- 1 = perfect score\n",
    "- 0 = no skill compared to random chance\n",
    "- -1/3 = worst possible performance\n",
    "\n",
    "**Key Properties:**\n",
    "- Measures how well predicted events correspond to observed events, accounting for hits due to random chance\n",
    "- Particularly useful for rare events (like precipitation above a high threshold)\n",
    "- More equitable than simple Threat Score by accounting for hits due to random chance\n",
    "\n",
    "**Advantages:**\n",
    "- Well-established metric in meteorological verification\n",
    "- Reference point at 0 makes interpretation clear\n",
    "- Penalizes both misses and false alarms\n",
    "- Accounts for random chance, making it more robust than basic threat scores\n",
    "\n",
    "#### Frequency Bias Index\n",
    "Range: 0 to infinity, where:\n",
    "- 1 = no bias\n",
    "- < 1 = underforecasting\n",
    "- > 1 = overforecasting\n",
    "\n",
    "**Key Properties:**\n",
    "- FBI measures the ratio of observed to forecasted events, indicating whether the model tends to over- or underforecast\n",
    "- It's particularly useful for understanding systematic biases in event frequency\n",
    "\n",
    "**Advantages:**\n",
    "- Provides a clear indication of over- or underforecasting\n",
    "- Easy to interpret: 1 indicates no bias, while values above or below 1 show the direction and magnitude of the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_meteoswiss_metrics(ds_gt, ds_ml, ds_nwp=None, subsample=None):\n",
    "    \"\"\"Calculate MeteoSwiss verification metrics (FBI and ETS) for station data.\"\"\"\n",
    "    metrics_by_var = {}\n",
    "\n",
    "    # Get available variables in each dataset\n",
    "    gt_ml_vars = set(ds_gt.variables) & set(ds_ml.variables)\n",
    "    nwp_vars = set(ds_nwp.variables) if ds_nwp is not None else set()\n",
    "\n",
    "    all_variables = {\n",
    "        \"precipitation\": {\n",
    "            \"thresholds\": THRESHOLDS_PRECIPITATION,\n",
    "            \"unit\": \"mm/h\",\n",
    "        },\n",
    "        \"wind_u_10m\": {\"thresholds\": THRESHOLDS_WIND, \"unit\": \"m/s\"},\n",
    "        \"wind_v_10m\": {\"thresholds\": THRESHOLDS_WIND, \"unit\": \"m/s\"},\n",
    "    }\n",
    "\n",
    "    # Filter variables that exist in gt and ml\n",
    "    all_variables = {k: v for k, v in all_variables.items() if k in gt_ml_vars}\n",
    "\n",
    "    # Initialize metrics structure\n",
    "    for var_name in all_variables:\n",
    "        metrics_by_var[var_name] = {}\n",
    "        for thr in all_variables[var_name][\"thresholds\"]:\n",
    "            metric_key = f\"{thr}{all_variables[var_name]['unit']}\"\n",
    "            metrics_by_var[var_name][metric_key] = {\n",
    "                \"FBI_ML\": [],\n",
    "                \"ETS_ML\": [],\n",
    "                \"FBI_NWP\": [] if ds_nwp is not None else None,\n",
    "                \"ETS_NWP\": [] if ds_nwp is not None else None,\n",
    "            }\n",
    "\n",
    "    # Apply subsampling if requested\n",
    "    if subsample:\n",
    "        ds_gt = ds_gt.isel(time=slice(None, None, subsample))\n",
    "        ds_ml = ds_ml.isel(start_time=slice(None, None, subsample))\n",
    "        if ds_nwp is not None:\n",
    "            ds_nwp = ds_nwp.isel(start_time=slice(None, None, subsample))\n",
    "\n",
    "    for efd in ds_ml.elapsed_forecast_duration.values:\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\nCalculating metrics for elapsed forecast duration: {efd / np.timedelta64(1, 'h'):.1f}h\"\n",
    "            )\n",
    "\n",
    "            ds_ml_lead = ds_ml.sel(elapsed_forecast_duration=efd)\n",
    "            ds_nwp_lead = (\n",
    "                ds_nwp.sel(elapsed_forecast_duration=efd)\n",
    "                if ds_nwp is not None\n",
    "                else None\n",
    "            )\n",
    "            forecast_times = ds_ml_lead.forecast_time.values\n",
    "            ds_gt_lead = ds_gt.sel(time=forecast_times)\n",
    "\n",
    "            for var_name, var_config in all_variables.items():\n",
    "                print(f\"Processing {var_name}\")\n",
    "                try:\n",
    "                    # Get data and create masks\n",
    "                    y_true = ds_gt_lead[var_name]\n",
    "                    y_ml = ds_ml_lead[var_name]\n",
    "                    y_nwp = (\n",
    "                        ds_nwp_lead[var_name]\n",
    "                        if ds_nwp_lead is not None and var_name in nwp_vars\n",
    "                        else None\n",
    "                    )\n",
    "\n",
    "                    # Create masks\n",
    "                    mask_true = xr.where(~np.isnan(y_true), True, False)\n",
    "                    mask_ml = xr.where(~np.isnan(y_ml), True, False)\n",
    "                    if y_nwp is not None:\n",
    "                        mask_nwp = xr.where(~np.isnan(y_nwp), True, False)\n",
    "                        valid_mask = mask_true & mask_ml & mask_nwp\n",
    "                    else:\n",
    "                        valid_mask = mask_true & mask_ml\n",
    "\n",
    "                    # Apply masks\n",
    "                    y_true = y_true.where(valid_mask)\n",
    "                    y_ml = y_ml.where(valid_mask)\n",
    "                    if y_nwp is not None:\n",
    "                        y_nwp = y_nwp.where(valid_mask)\n",
    "\n",
    "                    for thr in var_config[\"thresholds\"]:\n",
    "                        metric_key = f\"{thr}{var_config['unit']}\"\n",
    "\n",
    "                        # Calculate ML metrics using TEO\n",
    "                        event_operator = TEO(default_event_threshold=thr)\n",
    "                        ml_contingency = (\n",
    "                            event_operator.make_contingency_manager(\n",
    "                                y_ml, y_true\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        fbi_ml = ml_contingency.frequency_bias().values\n",
    "                        ets_ml = ml_contingency.equitable_threat_score().values\n",
    "\n",
    "                        metrics_by_var[var_name][metric_key][\"FBI_ML\"].append(\n",
    "                            fbi_ml\n",
    "                        )\n",
    "                        metrics_by_var[var_name][metric_key][\"ETS_ML\"].append(\n",
    "                            ets_ml\n",
    "                        )\n",
    "\n",
    "                        # Calculate NWP metrics if available\n",
    "                        if y_nwp is not None:\n",
    "                            nwp_contingency = (\n",
    "                                event_operator.make_contingency_manager(\n",
    "                                    y_nwp, y_true\n",
    "                                )\n",
    "                            )\n",
    "                            fbi_nwp = nwp_contingency.frequency_bias().values\n",
    "                            ets_nwp = (\n",
    "                                nwp_contingency.equitable_threat_score().values\n",
    "                            )\n",
    "\n",
    "                            metrics_by_var[var_name][metric_key][\n",
    "                                \"FBI_NWP\"\n",
    "                            ].append(fbi_nwp)\n",
    "                            metrics_by_var[var_name][metric_key][\n",
    "                                \"ETS_NWP\"\n",
    "                            ].append(ets_nwp)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {var_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing elapsed forecast duration {efd}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return metrics_by_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47101d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_meteoswiss_metrics_evolution(metrics_by_var, var_name, metric_prefix):\n",
    "    \"\"\"Plot evolution of MeteoSwiss metrics over forecast time with three fixed viridis colors.\"\"\"\n",
    "    forecast_hours = sorted(list(metrics_by_var.keys()))\n",
    "\n",
    "    # Get three fixed colors from viridis (start, middle, end)\n",
    "    colors = [plt.cm.viridis(x) for x in [0, 0.5, 0.99]]\n",
    "\n",
    "    # Find all metrics matching the prefix\n",
    "    all_metrics = []\n",
    "    for hour in forecast_hours:\n",
    "        if var_name in metrics_by_var[hour]:\n",
    "            for metric in metrics_by_var[hour][var_name].index:\n",
    "                if metric_prefix in metric:\n",
    "                    all_metrics.append(metric)\n",
    "\n",
    "    # Sort metrics by threshold value numerically\n",
    "    def get_threshold(metric):\n",
    "        return float(metric.split(\"_\")[1].replace(\"mm\", \"\").replace(\"ms\", \"\"))\n",
    "\n",
    "    all_metrics = sorted(list(set(all_metrics)), key=get_threshold)\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(\n",
    "            f\"No metrics found with prefix {metric_prefix} for variable {var_name}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Group by model (ML/NWP)\n",
    "    ml_metrics = [m for m in all_metrics if m.endswith(\"ML\")]\n",
    "    nwp_metrics = [m for m in all_metrics if m.endswith(\"NWP\")]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n",
    "\n",
    "    # Plot ML metrics\n",
    "    for i, metric in enumerate(ml_metrics):\n",
    "        values = []\n",
    "        for hour in forecast_hours:\n",
    "            if (\n",
    "                var_name in metrics_by_var[hour]\n",
    "                and metric in metrics_by_var[hour][var_name].index\n",
    "            ):\n",
    "                values.append(metrics_by_var[hour][var_name].loc[metric, 0])\n",
    "            else:\n",
    "                values.append(np.nan)\n",
    "\n",
    "        threshold = metric.split(\"_\")[1].replace(\"mm\", \"\").replace(\"ms\", \"\")\n",
    "        ax.plot(\n",
    "            forecast_hours,\n",
    "            values,\n",
    "            linestyle=LINE_STYLES[\"ml\"][0],\n",
    "            marker=LINE_STYLES[\"ml\"][1],\n",
    "            color=colors[i],\n",
    "            label=f\"ML {threshold}{'mm' if var_name == 'precipitation' else 'm/s'}\",\n",
    "        )\n",
    "\n",
    "    # Plot NWP metrics\n",
    "    for i, metric in enumerate(nwp_metrics):\n",
    "        values = []\n",
    "        for hour in forecast_hours:\n",
    "            if (\n",
    "                var_name in metrics_by_var[hour]\n",
    "                and metric in metrics_by_var[hour][var_name].index\n",
    "            ):\n",
    "                values.append(metrics_by_var[hour][var_name].loc[metric, 0])\n",
    "            else:\n",
    "                values.append(np.nan)\n",
    "\n",
    "        threshold = metric.split(\"_\")[1].replace(\"mm\", \"\").replace(\"ms\", \"\")\n",
    "        ax.plot(\n",
    "            forecast_hours,\n",
    "            values,\n",
    "            linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "            marker=LINE_STYLES[\"nwp\"][1],\n",
    "            color=colors[i],\n",
    "            label=f\"NWP {threshold}{'mm' if var_name == 'precipitation' else 'm/s'}\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Forecast Lead Time (hours)\")\n",
    "    ax.set_ylabel(f\"{metric_prefix}\")\n",
    "    ax.set_title(f\"{metric_prefix} for {var_name}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, f\"{metric_prefix.lower()}_{var_name}_evolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd645a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MeteoSwiss metrics\n",
    "def reshape_meteoswiss_metrics(metrics_dict):\n",
    "    \"\"\"Reshape MeteoSwiss metrics into a single DataFrame with proper column structure\"\"\"\n",
    "    # Create a list to store all data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each forecast hour\n",
    "    for hour in metrics_dict:\n",
    "        row_data = {\"forecast_hour\": hour}\n",
    "        # Iterate through each variable\n",
    "        for var in metrics_dict[hour]:\n",
    "            # Get metrics for this variable\n",
    "            var_metrics = metrics_dict[hour][var]\n",
    "            # Add each metric to the row data\n",
    "            for metric_name in var_metrics.index:\n",
    "                row_data[f\"{var}_{metric_name}\"] = var_metrics.loc[\n",
    "                    metric_name, 0\n",
    "                ]\n",
    "        data.append(row_data)\n",
    "\n",
    "    # Create DataFrame from collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index(\"forecast_hour\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Use it after calculate_meteoswiss_metrics\n",
    "print(\"Calculating MeteoSwiss metrics...\")\n",
    "meteoswiss_metrics = calculate_meteoswiss_metrics(\n",
    "    ds_obs, ds_ml_interp, ds_nwp_interp, subsample=SUBSAMPLE_HISTOGRAM\n",
    ")\n",
    "metrics_df = reshape_meteoswiss_metrics(meteoswiss_metrics)\n",
    "\n",
    "# Now export the reshaped DataFrame\n",
    "export_table(\n",
    "    metrics_df, \"meteoswiss_metrics\", caption=\"MeteoSwiss verification metrics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"precipitation\" in ds_obs:\n",
    "    plot_meteoswiss_metrics_evolution(\n",
    "        meteoswiss_metrics, \"precipitation\", \"ETS\"\n",
    "    )\n",
    "    plot_meteoswiss_metrics_evolution(\n",
    "        meteoswiss_metrics, \"precipitation\", \"FBI\"\n",
    "    )\n",
    "\n",
    "if \"wind_u_10m\" in ds_obs:\n",
    "    plot_meteoswiss_metrics_evolution(meteoswiss_metrics, \"wind_u_10m\", \"ETS\")\n",
    "    plot_meteoswiss_metrics_evolution(meteoswiss_metrics, \"wind_u_10m\", \"FBI\")\n",
    "\n",
    "if \"wind_v_10m\" in ds_obs:\n",
    "    plot_meteoswiss_metrics_evolution(meteoswiss_metrics, \"wind_v_10m\", \"ETS\")\n",
    "    plot_meteoswiss_metrics_evolution(meteoswiss_metrics, \"wind_v_10m\", \"FBI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ce3e0",
   "metadata": {},
   "source": [
    "The wind vector RMSE takes into account the magnitude and direction of the wind, providing a more comprehensive measure of error than scalar metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dbeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_vector_rmse(u_true, v_true, u_pred, v_pred):\n",
    "    \"\"\"Calculate RMSE based on wind vector differences.\"\"\"\n",
    "    u_diff = u_true - u_pred\n",
    "    v_diff = v_true - v_pred\n",
    "    vector_diff = np.sqrt(u_diff**2 + v_diff**2)\n",
    "    return float(np.sqrt(np.mean(vector_diff**2)))\n",
    "\n",
    "\n",
    "def calculate_wind_vector_metrics(\n",
    "    ds_obs, ds_ml_interp, ds_nwp_interp=None, subsample=None\n",
    "):\n",
    "    \"\"\"Calculate wind vector metrics for station data.\n",
    "\n",
    "    Args:\n",
    "        ds_obs: xarray Dataset containing observations\n",
    "        ds_ml_interp: xarray Dataset containing interpolated ML predictions\n",
    "        ds_nwp_interp: xarray Dataset containing interpolated NWP predictions\n",
    "        subsample: int, subsampling factor for faster processing\n",
    "    \"\"\"\n",
    "    # Sample data if requested\n",
    "    if subsample:\n",
    "        ds_obs_sample = ds_obs.isel(\n",
    "            time=slice(None, None, subsample),\n",
    "            station=slice(None, None, subsample),\n",
    "        )\n",
    "        ds_ml_sample = ds_ml_interp.isel(\n",
    "            start_time=slice(None, None, subsample),\n",
    "            station=slice(None, None, subsample),\n",
    "        )\n",
    "        if ds_nwp_interp is not None:\n",
    "            ds_nwp_sample = ds_nwp_interp.isel(\n",
    "                start_time=slice(None, None, subsample),\n",
    "                station=slice(None, None, subsample),\n",
    "            )\n",
    "    else:\n",
    "        ds_obs_sample = ds_obs\n",
    "        ds_ml_sample = ds_ml_interp\n",
    "        ds_nwp_sample = ds_nwp_interp if ds_nwp_interp is not None else None\n",
    "\n",
    "    # Check if wind components exist\n",
    "    if \"wind_u_10m\" not in ds_obs_sample or \"wind_v_10m\" not in ds_obs_sample:\n",
    "        print(\"Wind components not found in observation dataset\")\n",
    "        return None\n",
    "\n",
    "    if \"wind_u_10m\" not in ds_ml_sample or \"wind_v_10m\" not in ds_ml_sample:\n",
    "        print(\"Wind components not found in ML dataset\")\n",
    "        return None\n",
    "\n",
    "    # Get elapsed forecast durations\n",
    "    elapsed_forecast_durations = ds_ml_sample.elapsed_forecast_duration\n",
    "    forecast_hours = [\n",
    "        float(efd / np.timedelta64(1, \"h\"))\n",
    "        for efd in elapsed_forecast_durations\n",
    "    ]\n",
    "\n",
    "    # Initialize lists to store RMSE values over time\n",
    "    ml_rmse_over_time = []\n",
    "    nwp_rmse_over_time = []\n",
    "\n",
    "    for efd in elapsed_forecast_durations:\n",
    "        # Get ML data for this forecast lead time\n",
    "        ds_ml_lead = ds_ml_sample.sel(elapsed_forecast_duration=efd)\n",
    "\n",
    "        # Get observation times corresponding to these forecast times\n",
    "        forecast_times = ds_ml_lead.forecast_time.values\n",
    "        ds_obs_lead = ds_obs_sample.sel(time=forecast_times)\n",
    "\n",
    "        # Get wind components for observations and ML\n",
    "        u_true = ds_obs_lead[\"wind_u_10m\"].values\n",
    "        v_true = ds_obs_lead[\"wind_v_10m\"].values\n",
    "        u_ml = ds_ml_lead[\"wind_u_10m\"].values\n",
    "        v_ml = ds_ml_lead[\"wind_v_10m\"].values\n",
    "\n",
    "        # Create valid mask\n",
    "        valid_mask = (\n",
    "            ~np.isnan(u_true)\n",
    "            & ~np.isnan(v_true)\n",
    "            & ~np.isnan(u_ml)\n",
    "            & ~np.isnan(v_ml)\n",
    "        )\n",
    "\n",
    "        if np.any(valid_mask):\n",
    "            # Calculate ML wind vector RMSE\n",
    "            wind_rmse_ml = wind_vector_rmse(\n",
    "                u_true[valid_mask],\n",
    "                v_true[valid_mask],\n",
    "                u_ml[valid_mask],\n",
    "                v_ml[valid_mask],\n",
    "            )\n",
    "            ml_rmse_over_time.append(wind_rmse_ml)\n",
    "        else:\n",
    "            ml_rmse_over_time.append(np.nan)\n",
    "\n",
    "        # Calculate NWP RMSE if available\n",
    "        if (\n",
    "            ds_nwp_sample is not None\n",
    "            and \"wind_u_10m\" in ds_nwp_sample\n",
    "            and \"wind_v_10m\" in ds_nwp_sample\n",
    "        ):\n",
    "            ds_nwp_lead = ds_nwp_sample.sel(elapsed_forecast_duration=efd)\n",
    "            u_nwp = ds_nwp_lead[\"wind_u_10m\"].values\n",
    "            v_nwp = ds_nwp_lead[\"wind_v_10m\"].values\n",
    "\n",
    "            valid_mask_nwp = valid_mask & ~np.isnan(u_nwp) & ~np.isnan(v_nwp)\n",
    "\n",
    "            if np.any(valid_mask_nwp):\n",
    "                wind_rmse_nwp = wind_vector_rmse(\n",
    "                    u_true[valid_mask_nwp],\n",
    "                    v_true[valid_mask_nwp],\n",
    "                    u_nwp[valid_mask_nwp],\n",
    "                    v_nwp[valid_mask_nwp],\n",
    "                )\n",
    "                nwp_rmse_over_time.append(wind_rmse_nwp)\n",
    "            else:\n",
    "                nwp_rmse_over_time.append(np.nan)\n",
    "        else:\n",
    "            nwp_rmse_over_time.append(np.nan)\n",
    "\n",
    "    # Create result DataFrame\n",
    "    time_series_df = pd.DataFrame({\n",
    "        \"Elapsed Forecast Duration\": forecast_hours,\n",
    "        \"ML Vector RMSE\": ml_rmse_over_time,\n",
    "        \"NWP Vector RMSE\": nwp_rmse_over_time,\n",
    "    })\n",
    "\n",
    "    return time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wind_vector_rmse(time_series_df):\n",
    "    \"\"\"Plot wind vector RMSE evolution over forecast time.\n",
    "\n",
    "    Args:\n",
    "        time_series_df: Time series DataFrame from calculate_wind_vector_metrics\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=DPI)\n",
    "\n",
    "    # Plot ML RMSE\n",
    "    ax.plot(\n",
    "        time_series_df[\"Elapsed Forecast Duration\"],\n",
    "        time_series_df[\"ML Vector RMSE\"],\n",
    "        color=COLORS[\"ml\"],\n",
    "        linestyle=LINE_STYLES[\"ml\"][0],\n",
    "        marker=LINE_STYLES[\"ml\"][1],\n",
    "        label=\"ML\",\n",
    "    )\n",
    "\n",
    "    # Plot NWP RMSE if available\n",
    "    if not all(np.isnan(time_series_df[\"NWP Vector RMSE\"])):\n",
    "        ax.plot(\n",
    "            time_series_df[\"Elapsed Forecast Duration\"],\n",
    "            time_series_df[\"NWP Vector RMSE\"],\n",
    "            color=COLORS[\"nwp\"],\n",
    "            linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "            marker=LINE_STYLES[\"nwp\"][1],\n",
    "            label=\"NWP\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Forecast Lead Time (hours)\")\n",
    "    ax.set_ylabel(\"Wind Vector RMSE (m/s)\")\n",
    "    ax.set_title(\"Wind Vector RMSE Evolution\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, \"wind_vector_rmse_evolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2b093",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Calculate wind vector metrics\n",
    "if \"wind_u_10m\" in ds_obs and \"wind_v_10m\" in ds_obs:\n",
    "    print(\"Calculating wind vector metrics...\")\n",
    "    wind_timeseries = calculate_wind_vector_metrics(\n",
    "        ds_obs, ds_ml_interp, ds_nwp_interp, subsample=SUBSAMPLE_HISTOGRAM\n",
    "    )\n",
    "    plot_wind_vector_rmse(wind_timeseries)\n",
    "    export_table(\n",
    "        wind_timeseries,\n",
    "        \"wind_vector_metrics_timeseries\",\n",
    "        caption=\"Wind vector RMSE over forecast lead time\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Wind components not found in the data\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
