{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5d8017",
   "metadata": {},
   "source": [
    "##  Gridded Model Verification\n",
    "\n",
    "This script verifies output from a ML-based foundation model versus a\n",
    "traditional NWP system for the atmospheric system. The defaults set at the top of\n",
    "this script are tailored to the Alps-Clariden HPC system at CSCS.\n",
    "- The NWP-model is called COSMO-E and is initialised with the ensemble mean of the analysis. Only surface level data is available in the archive at MeteoSwiss.\n",
    "- The ML-model is called Neural-LAM and is initialised with the deterministic analysis.\n",
    "- The Ground Truth is the same deterministic analysis as was used to train the ML-model.\n",
    "- The boundary data for both models is IFS HRES from ECMWF, where the NWP-model got 6 hourly boundary updates and the ML model 12 hourly.\n",
    "\n",
    "For more info about the COSMO model see:\n",
    "- https://www.cosmo-model.org/content/model/cosmo/coreDocumentation/cosmo_io_guide_6.00.pdf\n",
    "- https://www.research-collection.ethz.ch/handle/20.500.11850/720460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0311b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from pysteps.verification.salscores import sal  # requires scikit-image\n",
    "from scipy.stats import wasserstein_distance, kurtosis, skew\n",
    "from scores.continuous import (\n",
    "    mae,\n",
    "    mse,\n",
    "    rmse,\n",
    ")\n",
    "from scores.continuous.correlation import pearsonr\n",
    "from scores.spatial import fss_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a806b47",
   "metadata": {},
   "source": [
    "**--------> Enter all your user settings in the cell below. <--------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf621a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFAULTS ###\n",
    "# This config will be applied to the data before any plotting. The data will be\n",
    "# sliced and indexed according to the values in this config.The whole analysis and\n",
    "# plotting will be done on the reduced data.\n",
    "\n",
    "# IF YOUR DATA HAS DIFFERENT DIMENSIONS OR NAMES, PLEASE ADJUST THE CELLS BELOW\n",
    "# MAKE SURE THE XARRAY DATASETS LOOK OKAY BEFORE RUNNING CHAPTER 1-4\n",
    "\n",
    "# This path should point to the data that was used to train the model (default is mdp-datastore)\n",
    "PATH_GROUND_TRUTH = \"/iopsstor/scratch/cscs/sadamov/pyprojects_data/neural-lam/cosmo.datastore.zarr\"\n",
    "# This path should point to the NWP forecast data in zarr format\n",
    "PATH_NWP = \"/capstor/store/cscs/swissai/a01/sadamov/cosmo_e_forecast.zarr\"\n",
    "# This path should point to the ML forecast data in zarr format (e.g. produced by neural-lam in `eval` mode)\n",
    "PATH_ML = \"/iopsstor/scratch/cscs/sadamov/pyprojects_data/neural-lam/eval_results/preds_7_19_margin_interior_lr_0001_ar_12.zarr\"\n",
    "# This path should point to the boundary data in zarr format (default is MDP-datastore)\n",
    "PATH_BOUNDARY = \"/iopsstor/scratch/cscs/sadamov/pyprojects_data/neural-lam/ifs_7_19_margin_interior.datastore.zarr\"\n",
    "\n",
    "# elapsed forecast duration in steps for the forecast - [0] refers to the first forecast step at t+1\n",
    "# this should be a list of integers\n",
    "ELAPSED_FORECAST_DURATION = list(range(0, 120, 24))\n",
    "# Select specific start_times for the forecast. This is the start and end of\n",
    "# a slice in xarray. The start_time is included, the end_time is excluded.\n",
    "# This should be a list of two strings in the format \"YYYY-MM-DDTHH:MM:SS\"\n",
    "# Should be handy to evaluate certain dates, e.g. for a case study of a storm\n",
    "START_TIMES = [\"2020-02-13T00:00:00\", \"2020-02-15T00:00:00\"]\n",
    "# Select specific plot times for the forecast (will be used to create maps for all variables)\n",
    "# This only affect chapter one with the plotting of the maps\n",
    "# Map creation takes a lot of time so this is limited to a single time step\n",
    "# Simply rerun these cells and chapter one for more time steps\n",
    "PLOT_TIME = \"2020-02-13T00:00:00\"\n",
    "\n",
    "# Selection spatial grid in projection\n",
    "# This is used to slice the data to a specific region\n",
    "# This is in projection of the ground truth data\n",
    "# The default is the whole domain [None, None]\n",
    "X = [None, None]\n",
    "Y = [None, None]\n",
    "\n",
    "# Map projection settings for plotting\n",
    "# This is the projection of the ground truth data\n",
    "PROJECTION = ccrs.RotatedPole(\n",
    "    pole_longitude=190,\n",
    "    pole_latitude=43,\n",
    "    central_rotated_longitude=10,\n",
    ")\n",
    "\n",
    "# Define how variables map between different data sources\n",
    "\n",
    "# Define here which of the variables are available in the ground truth data\n",
    "# The keys are the names of the variables in the ground truth data\n",
    "# The values are the conventional names, used in this notebook\n",
    "VARIABLES_GROUND_TRUTH = {\n",
    "    # Surface and near-surface variables\n",
    "    \"T_2M\": \"temperature_2m\",\n",
    "    \"U_10M\": \"wind_u_10m\",\n",
    "    \"V_10M\": \"wind_v_10m\",\n",
    "    \"PMSL\": \"pressure_sea_level\",\n",
    "    \"PS\": \"surface_pressure\",\n",
    "    \"TOT_PREC\": \"precipitation\",\n",
    "    \"ASHFL_S\": \"surface_sensible_heat_flux\",\n",
    "    \"ASOB_S\": \"surface_net_shortwave_radiation\",\n",
    "    \"ATHB_S\": \"surface_net_longwave_radiation\",\n",
    "    # Upper air variables - U component\n",
    "    \"U_lev_6\": \"wind_u_level_6\",\n",
    "    \"U_lev_12\": \"wind_u_level_12\",\n",
    "    \"U_lev_20\": \"wind_u_level_20\",\n",
    "    \"U_lev_27\": \"wind_u_level_27\",\n",
    "    \"U_lev_31\": \"wind_u_level_31\",\n",
    "    \"U_lev_39\": \"wind_u_level_39\",\n",
    "    \"U_lev_45\": \"wind_u_level_45\",\n",
    "    \"U_lev_60\": \"wind_u_level_60\",\n",
    "    # Upper air variables - V component\n",
    "    # \"V_lev_6\": \"wind_v_level_6\",\n",
    "    # \"V_lev_12\": \"wind_v_level_12\",\n",
    "    # \"V_lev_20\": \"wind_v_level_20\",\n",
    "    # \"V_lev_27\": \"wind_v_level_27\",\n",
    "    # \"V_lev_31\": \"wind_v_level_31\",\n",
    "    # \"V_lev_39\": \"wind_v_level_39\",\n",
    "    # \"V_lev_45\": \"wind_v_level_45\",\n",
    "    # \"V_lev_60\": \"wind_v_level_60\",\n",
    "    # # Upper air variables - Pressure\n",
    "    # \"PP_lev_6\": \"pressure_level_6\",\n",
    "    # \"PP_lev_12\": \"pressure_level_12\",\n",
    "    # \"PP_lev_20\": \"pressure_level_20\",\n",
    "    # \"PP_lev_27\": \"pressure_level_27\",\n",
    "    # \"PP_lev_31\": \"pressure_level_31\",\n",
    "    # \"PP_lev_39\": \"pressure_level_39\",\n",
    "    # \"PP_lev_45\": \"pressure_level_45\",\n",
    "    # \"PP_lev_60\": \"pressure_level_60\",\n",
    "    # # Upper air variables - Temperature\n",
    "    # \"T_lev_6\": \"temperature_level_6\",\n",
    "    # \"T_lev_12\": \"temperature_level_12\",\n",
    "    # \"T_lev_20\": \"temperature_level_20\",\n",
    "    # \"T_lev_27\": \"temperature_level_27\",\n",
    "    # \"T_lev_31\": \"temperature_level_31\",\n",
    "    # \"T_lev_39\": \"temperature_level_39\",\n",
    "    # \"T_lev_45\": \"temperature_level_45\",\n",
    "    # \"T_lev_60\": \"temperature_level_60\",\n",
    "    # # Upper air variables - Relative Humidity\n",
    "    # \"RELHUM_lev_6\": \"relative_humidity_level_6\",\n",
    "    # \"RELHUM_lev_12\": \"relative_humidity_level_12\",\n",
    "    # \"RELHUM_lev_20\": \"relative_humidity_level_20\",\n",
    "    # \"RELHUM_lev_27\": \"relative_humidity_level_27\",\n",
    "    # \"RELHUM_lev_31\": \"relative_humidity_level_31\",\n",
    "    # \"RELHUM_lev_39\": \"relative_humidity_level_39\",\n",
    "    # \"RELHUM_lev_45\": \"relative_humidity_level_45\",\n",
    "    # \"RELHUM_lev_60\": \"relative_humidity_level_60\",\n",
    "    # # Upper air variables - Vertical velocity\n",
    "    # \"W_lev_6\": \"vertical_velocity_level_6\",\n",
    "    # \"W_lev_12\": \"vertical_velocity_level_12\",\n",
    "    # \"W_lev_20\": \"vertical_velocity_level_20\",\n",
    "    # \"W_lev_27\": \"vertical_velocity_level_27\",\n",
    "    # \"W_lev_31\": \"vertical_velocity_level_31\",\n",
    "    # \"W_lev_39\": \"vertical_velocity_level_39\",\n",
    "    # \"W_lev_45\": \"vertical_velocity_level_45\",\n",
    "    # \"W_lev_60\": \"vertical_velocity_level_60\",\n",
    "}\n",
    "\n",
    "# Since the default ground_truth is the datastore that was used for model training\n",
    "# the variables are identical to the VARIABLES_GROUND_TRUTH\n",
    "VARIABLES_ML = VARIABLES_GROUND_TRUTH\n",
    "\n",
    "# For the NWP-Forecast only a limited set of variables is available\n",
    "# These variables are mapped to the same conventional names\n",
    "# The script is flexible and will only calculate the NWP-metrics for the variables that are available\n",
    "# The script will not break if some of the variables are not available\n",
    "VARIABLES_NWP = {\n",
    "    \"wind_u_10m\": \"wind_u_10m\",\n",
    "    \"wind_v_10m\": \"wind_v_10m\",\n",
    "    \"precipitation_1hr\": \"precipitation\",\n",
    "    \"pressure_sea_level\": \"pressure_sea_level\",\n",
    "    \"surface_pressure\": \"surface_pressure\",\n",
    "    \"temperature_2m\": \"temperature_2m\",\n",
    "}\n",
    "\n",
    "# These variables are only used for chapter 1, the mapplots.\n",
    "# They will be plotted for the ground truth, NWP and ML\n",
    "VARIABLES_BOUNDARY = {\n",
    "    # Surface and near-surface variables\n",
    "    \"mean_sea_level_pressure\": \"pressure_sea_level\",\n",
    "    \"2m_temperature\": \"temperature_2m\",\n",
    "    \"10m_u_component_of_wind\": \"wind_u_10m\",\n",
    "    \"10m_v_component_of_wind\": \"wind_v_10m\",\n",
    "    \"surface_pressure\": \"surface_pressure\",\n",
    "    # Upper air variables - U component\n",
    "    \"u_component_of_wind100hPa\": \"wind_u_level_6\",\n",
    "    \"u_component_of_wind200hPa\": \"wind_u_level_12\",\n",
    "    \"u_component_of_wind400hPa\": \"wind_u_level_20\",\n",
    "    \"u_component_of_wind600hPa\": \"wind_u_level_27\",\n",
    "    \"u_component_of_wind700hPa\": \"wind_u_level_31\",\n",
    "    \"u_component_of_wind850hPa\": \"wind_u_level_39\",\n",
    "    \"u_component_of_wind925hPa\": \"wind_u_level_45\",\n",
    "    \"u_component_of_wind1000hPa\": \"wind_u_level_60\",\n",
    "    # Upper air variables - V component\n",
    "    \"v_component_of_wind100hPa\": \"wind_v_level_6\",\n",
    "    \"v_component_of_wind200hPa\": \"wind_v_level_12\",\n",
    "    \"v_component_of_wind400hPa\": \"wind_v_level_20\",\n",
    "    \"v_component_of_wind600hPa\": \"wind_v_level_27\",\n",
    "    \"v_component_of_wind700hPa\": \"wind_v_level_31\",\n",
    "    \"v_component_of_wind850hPa\": \"wind_v_level_39\",\n",
    "    \"v_component_of_wind925hPa\": \"wind_v_level_45\",\n",
    "    \"v_component_of_wind1000hPa\": \"wind_v_level_60\",\n",
    "    # Upper air variables - Temperature\n",
    "    \"temperature100hPa\": \"temperature_level_6\",\n",
    "    \"temperature200hPa\": \"temperature_level_12\",\n",
    "    \"temperature400hPa\": \"temperature_level_20\",\n",
    "    \"temperature600hPa\": \"temperature_level_27\",\n",
    "    \"temperature700hPa\": \"temperature_level_31\",\n",
    "    \"temperature850hPa\": \"temperature_level_39\",\n",
    "    \"temperature925hPa\": \"temperature_level_45\",\n",
    "    \"temperature1000hPa\": \"temperature_level_60\",\n",
    "    # Upper air variables - Vertical velocity\n",
    "    \"vertical_velocity100hPa\": \"vertical_velocity_level_6\",\n",
    "    \"vertical_velocity200hPa\": \"vertical_velocity_level_12\",\n",
    "    \"vertical_velocity400hPa\": \"vertical_velocity_level_20\",\n",
    "    \"vertical_velocity600hPa\": \"vertical_velocity_level_27\",\n",
    "    \"vertical_velocity700hPa\": \"vertical_velocity_level_31\",\n",
    "    \"vertical_velocity850hPa\": \"vertical_velocity_level_39\",\n",
    "    \"vertical_velocity925hPa\": \"vertical_velocity_level_45\",\n",
    "    \"vertical_velocity1000hPa\": \"vertical_velocity_level_60\",\n",
    "}\n",
    "\n",
    "# These variables will be used as `basename` for the vertical profiles.\n",
    "# Since the input of the zarr archives is expected to have data vars that are 2D in space\n",
    "# we need some base_name prefix to create the 3D variables\n",
    "VARIABLES_3D = [\n",
    "    \"wind_u_level\",\n",
    "    \"wind_v_level\",\n",
    "    \"pressure_level\",\n",
    "    \"temperature_level\",\n",
    "    \"relative_humidity_level\",\n",
    "    \"vertical_velocity_level\",\n",
    "]\n",
    "\n",
    "# Add units dictionary after the imports\n",
    "# units from zarr archives are not reliable and should rather be defined here\n",
    "VARIABLE_UNITS = {\n",
    "    # Surface and near-surface variables\n",
    "    \"temperature_2m\": \"K\",\n",
    "    \"wind_u_10m\": \"m/s\",\n",
    "    \"wind_v_10m\": \"m/s\",\n",
    "    \"pressure_sea_level\": \"Pa\",\n",
    "    \"surface_pressure\": \"Pa\",\n",
    "    \"precipitation\": \"mm/h\",\n",
    "    \"surface_sensible_heat_flux\": \"W/m²\",\n",
    "    \"surface_net_shortwave_radiation\": \"W/m²\",\n",
    "    \"surface_net_longwave_radiation\": \"W/m²\",\n",
    "    # Upper air variables\n",
    "    \"wind_u_level\": \"m/s\",\n",
    "    \"wind_v_level\": \"m/s\",\n",
    "    \"pressure_level\": \"hPa\",\n",
    "    \"temperature_level\": \"K\",\n",
    "    \"relative_humidity_level\": \"%\",\n",
    "    \"vertical_velocity_level\": \"Pa/s\",\n",
    "}\n",
    "\n",
    "# Define Thresholds for the ETS metric (Equitable Threat Score)\n",
    "# These are calculated for wind and precipitation if available\n",
    "# The score creates contingency tables for different thresholds\n",
    "# The ETS is calculated for each threshold and the results are plotted\n",
    "# The default thresholds are [0.1, 1, 5] for precipitation and [2.5, 5, 10] for wind\n",
    "THRESHOLDS_PRECIPITATION = [0.1, 1, 5]  # mm/h\n",
    "THRESHOLDS_WIND = [2.5, 5, 10]  # m/s\n",
    "\n",
    "# This setting is relevant for the mapplots in chapter 1\n",
    "# Higher levels of ZOOM will zoom in on the map, cropping the boundary\n",
    "ZOOM = 2  # Halves the extent of the mapplot\n",
    "\n",
    "# For some chapters a random seed is required to reproduce the results\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# The DPI used in all plots in the notebook, export to pdf will always be 300 DPI\n",
    "DPI = 100\n",
    "\n",
    "# Subsample the data for faster plotting, 10 refers to every 10th element\n",
    "# This is used to create the histograms in chapter 2 (along space and time)\n",
    "# and in chapter 3 for the energy spectra (along time)\n",
    "# There is a trade-off between speed and accuracy, that each user has to find\n",
    "SUBSAMPLE_HISTOGRAM = 10\n",
    "\n",
    "# Subsample the data for FSS threshold calculation, 1e7 refers to the number of elements\n",
    "# This is not critical, as it is only used to calculate the 90% threshold\n",
    "# for the FSS based on the ground truth data\n",
    "SUBSAMPLE_FSS_THRESHOLD = 1e7\n",
    "\n",
    "# Takes a long time, but if you see NaN in your output, you can set this to True\n",
    "# This will check if there are any missing values in the data further below\n",
    "CHECK_MISSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for plots and tables\n",
    "Path(\"plots\").mkdir(exist_ok=True)\n",
    "Path(\"tables\").mkdir(exist_ok=True)\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "COLORS = {\n",
    "    \"gt\": \"#000000\",  # Black\n",
    "    \"ml\": \"#E69F00\",  # Orange\n",
    "    \"nwp\": \"#56B4E9\",  # Light blue\n",
    "    \"error\": \"#CC79A7\",  # Pink\n",
    "}\n",
    "\n",
    "# Line styles and markers for accessibility\n",
    "LINE_STYLES = {\n",
    "    \"gt\": (\"solid\", \"o\"),\n",
    "    \"ml\": (\"dashed\", \"s\"),\n",
    "    \"nwp\": (\"dotted\", \"^\"),\n",
    "}\n",
    "\n",
    "# Colorblind-friendly colormap for 2D plots\n",
    "COLORMAP = \"viridis\"\n",
    "\n",
    "# Add level-specific units by reusing base units\n",
    "required_levels = {\n",
    "    int(key.split(\"_\")[-1]) for key in VARIABLES_GROUND_TRUTH if \"lev_\" in key\n",
    "}\n",
    "for level in required_levels:\n",
    "    for base_var, unit in VARIABLE_UNITS.items():\n",
    "        if \"_level\" in base_var:\n",
    "            VARIABLE_UNITS[f\"{base_var}_{level}\"] = unit\n",
    "\n",
    "\n",
    "def save_plot(fig, name, time=None, remove_title=True, dpi=300):\n",
    "    \"\"\"Helper function to save plots consistently\n",
    "\n",
    "    Args:\n",
    "        fig: matplotlib figure object\n",
    "        name (str): base name for the plot file\n",
    "        time (datetime, optional): timestamp to append to filename\n",
    "        remove_title (bool): remove suptitle/title hierarchically if True\n",
    "        dpi (int): resolution for the saved figure, defaults to 300\n",
    "    \"\"\"\n",
    "    if time is not None:\n",
    "        name = f\"{name}_{time.dt.strftime('%Y%m%d_%H').values}\"\n",
    "\n",
    "    # Sanitize filename by replacing problematic characters\n",
    "    safe_name = name.replace(\"/\", \"_per_\")\n",
    "\n",
    "    # Normalize the path and ensure plots directory exists\n",
    "    plot_dir = Path(\"plots\")\n",
    "    plot_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Remove titles if requested\n",
    "    if remove_title:\n",
    "        if hasattr(fig, \"texts\") and fig.texts:  # Check for suptitle\n",
    "            fig.suptitle(\"\")\n",
    "        ax = fig.gca()\n",
    "        if ax.get_title():\n",
    "            ax.set_title(\"\")\n",
    "\n",
    "    pdf_path = plot_dir / f\"{safe_name}.pdf\"\n",
    "    fig.savefig(pdf_path, bbox_inches=\"tight\", dpi=dpi)\n",
    "\n",
    "\n",
    "def export_table(df, name, caption=\"\"):\n",
    "    \"\"\"Helper function to export tables consistently\"\"\"\n",
    "    # Export to LaTeX with caption\n",
    "    latex_str = df.to_latex(\n",
    "        float_format=\"%.4f\", caption=caption, label=f\"tab:{name}\"\n",
    "    )\n",
    "    with open(f\"tables/{name}.tex\", \"w\") as f:\n",
    "        f.write(latex_str)\n",
    "\n",
    "    # Export to CSV\n",
    "    df.to_csv(f\"tables/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ml = xr.open_zarr(PATH_ML)\n",
    "ds_ml = ds_ml.sel(state_feature=list(VARIABLES_ML.keys()))\n",
    "ds_ml = ds_ml.sel(y=slice(*Y), x=slice(*X))\n",
    "ds_ml = ds_ml.sel(start_time=slice(*START_TIMES))\n",
    "for feature in ds_ml.state_feature.values:\n",
    "    ds_ml[VARIABLES_ML[feature]] = ds_ml[\"state\"].sel(state_feature=feature)\n",
    "forecast_times = (\n",
    "    ds_ml.start_time.values[:, None] + ds_ml.elapsed_forecast_duration.values\n",
    ")\n",
    "ds_ml = ds_ml.assign_coords(\n",
    "    forecast_time=(\n",
    "        (\"start_time\", \"elapsed_forecast_duration\"),\n",
    "        forecast_times,\n",
    "    )\n",
    ")\n",
    "ds_ml = ds_ml.drop_vars([\"state\", \"state_feature\", \"time\"])\n",
    "ds_ml = ds_ml.transpose(\"start_time\", \"elapsed_forecast_duration\", \"x\", \"y\")\n",
    "ds_ml = ds_ml[\n",
    "    [\n",
    "        \"start_time\",\n",
    "        \"elapsed_forecast_duration\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_ML.values(),\n",
    "    ]\n",
    "]\n",
    "ds_ml = ds_ml.isel(elapsed_forecast_duration=ELAPSED_FORECAST_DURATION)\n",
    "\n",
    "ds_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf42e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gt = xr.open_zarr(PATH_GROUND_TRUTH)\n",
    "ds_gt = ds_gt.set_index(grid_index=[\"y\", \"x\"]).unstack(\"grid_index\")\n",
    "ds_gt = ds_gt.sel(y=slice(*Y), x=slice(*X))\n",
    "ds_gt = ds_gt.sel(state_feature=list(VARIABLES_ML.keys()))\n",
    "ds_gt = ds_gt.sel(split_name=\"test\").drop_dims([\n",
    "    \"forcing_feature\",\n",
    "    \"static_feature\",\n",
    "    \"split_part\",\n",
    "])\n",
    "for feature in ds_gt.state_feature.values:\n",
    "    ds_gt[VARIABLES_ML[feature]] = ds_gt[\"state\"].sel(state_feature=feature)\n",
    "ds_gt = ds_gt.drop_vars([\n",
    "    \"state\",\n",
    "    \"state_feature\",\n",
    "    \"state_feature_units\",\n",
    "    \"state_feature_long_name\",\n",
    "    \"state_feature_source_dataset\",\n",
    "    \"state__train__diff_mean\",\n",
    "    \"state__train__diff_std\",\n",
    "    \"state__train__mean\",\n",
    "    \"state__train__std\",\n",
    "])\n",
    "ds_gt = ds_gt.transpose(\"time\", \"x\", \"y\")\n",
    "ds_gt = ds_gt[\n",
    "    [\n",
    "        \"time\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_GROUND_TRUTH.values(),\n",
    "    ]\n",
    "]\n",
    "ds_gt = ds_gt.sel(time=np.unique(ds_ml.forecast_time.values.flatten()))\n",
    "ds_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nwp = xr.open_zarr(PATH_NWP)\n",
    "ds_nwp = ds_nwp.sel(y=slice(*Y), x=slice(*X), time=slice(*START_TIMES))\n",
    "# The NWP data starts at elapsed forecast duration 0 = start_time\n",
    "ds_nwp = ds_nwp.drop_isel(lead_time=0).isel(lead_time=ELAPSED_FORECAST_DURATION)\n",
    "ds_nwp = ds_nwp[VARIABLES_NWP.keys()].rename(VARIABLES_NWP)\n",
    "ds_nwp = ds_nwp.rename_dims({\n",
    "    \"lead_time\": \"elapsed_forecast_duration\",\n",
    "    \"time\": \"start_time\",\n",
    "})\n",
    "ds_nwp = ds_nwp.rename_vars({\n",
    "    \"lead_time\": \"elapsed_forecast_duration\",\n",
    "    \"time\": \"start_time\",\n",
    "})\n",
    "forecast_times = (\n",
    "    ds_nwp.start_time.values[:, None] + ds_nwp.elapsed_forecast_duration.values\n",
    ")\n",
    "ds_nwp = ds_nwp.assign_coords(\n",
    "    forecast_time=(\n",
    "        (\"start_time\", \"elapsed_forecast_duration\"),\n",
    "        forecast_times,\n",
    "    )\n",
    ")\n",
    "ds_nwp = ds_nwp.transpose(\"start_time\", \"elapsed_forecast_duration\", \"x\", \"y\")\n",
    "ds_nwp = ds_nwp[\n",
    "    [\n",
    "        \"start_time\",\n",
    "        \"elapsed_forecast_duration\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_NWP.values(),\n",
    "    ]\n",
    "]\n",
    "\n",
    "ds_nwp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46114d2",
   "metadata": {},
   "source": [
    "Check for missing data in any of the variables. If you have missing data, you need to handle it before running the verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b77c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if CHECK_MISSING:\n",
    "    with LocalCluster(\n",
    "        n_workers=16,\n",
    "        threads_per_worker=1,\n",
    "        memory_limit=\"16GB\",\n",
    "    ) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            missing_counts = dask.compute(\n",
    "                {\n",
    "                    var: ds_gt[var].isnull().sum().values\n",
    "                    for var in ds_gt.data_vars\n",
    "                },\n",
    "                {\n",
    "                    var: ds_nwp[var].isnull().sum().values\n",
    "                    for var in ds_nwp.data_vars\n",
    "                },\n",
    "                {\n",
    "                    var: ds_ml[var].isnull().sum().values\n",
    "                    for var in ds_ml.data_vars\n",
    "                },\n",
    "            )\n",
    "    # Unpack results\n",
    "    gt_missing, nwp_missing, ml_missing = missing_counts\n",
    "\n",
    "    # Print results\n",
    "    print(\"Ground Truth\")\n",
    "    for var, count in gt_missing.items():\n",
    "        print(f\"{var}: {count} missing values\")\n",
    "\n",
    "    print(\"\\nNWP Model\")\n",
    "    for var, count in nwp_missing.items():\n",
    "        print(f\"{var}: {count} missing values\")\n",
    "\n",
    "    print(\"\\nML Model\")\n",
    "    for var, count in ml_missing.items():\n",
    "        print(f\"{var}: {count} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ds_gt.sizes[\"x\"] == ds_ml.sizes[\"x\"]\n",
    "assert ds_gt.sizes[\"x\"] == ds_nwp.sizes[\"x\"]\n",
    "assert ds_gt.sizes[\"y\"] == ds_ml.sizes[\"y\"]\n",
    "assert ds_gt.sizes[\"y\"] == ds_nwp.sizes[\"y\"]\n",
    "assert ds_gt.sizes[\"time\"] == len(\n",
    "    np.unique(ds_ml.forecast_time.values.flatten())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eadd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates\n",
    "if hasattr(ds_gt, \"longitude\") and hasattr(ds_gt, \"latitude\"):\n",
    "    lons = ds_gt.longitude.values\n",
    "    lats = ds_gt.latitude.values\n",
    "elif hasattr(ds_gt, \"lon\") and hasattr(ds_gt, \"lat\"):\n",
    "    lons = ds_gt.lon.values\n",
    "    lats = ds_gt.lat.values\n",
    "\n",
    "lon_min = lons.min()\n",
    "lon_max = lons.max()\n",
    "lat_min = lats.min()\n",
    "lat_max = lats.max()\n",
    "\n",
    "# Transform domain bounds to rotated coordinates\n",
    "transformer = PROJECTION.transform_points(\n",
    "    ccrs.PlateCarree(),\n",
    "    np.array([lon_min, lon_max]),\n",
    "    np.array([lat_min, lat_max]),\n",
    ")\n",
    "\n",
    "# Get rotated coordinate bounds\n",
    "rot_lon_min, rot_lon_max = transformer[:, 0].min(), transformer[:, 0].max()\n",
    "rot_lat_min, rot_lat_max = transformer[:, 1].min(), transformer[:, 1].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da85c5",
   "metadata": {},
   "source": [
    "### 1. Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c76fb",
   "metadata": {},
   "source": [
    "**Random Time Selection:** A random time step is selected to avoid bias in the comparison, ensuring that the assessment is representative of typical model performance.\n",
    "\n",
    "**Consistent Color Scales:** By setting the same minimum and maximum values across all datasets for each variable, we ensure that color differences in the plots reflect true discrepancies, not artifacts of scaling.\n",
    "\n",
    "**Spatial Patterns:** The plots reveal how the ML model and NWP model represent geographical features like weather fronts, high and low-pressure systems, and temperature gradients. Visual comparisons can immediately highlight areas where the models perform well or poorly, guiding further investigation.\n",
    "\n",
    "**Edge Effects:** Near the boundaries, artifacts may occur as the model does not calculate a loss in the boundary region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76799646",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ds_boundary = xr.open_zarr(PATH_BOUNDARY)\n",
    "\n",
    "temporal_dim = \"time\" if \"time\" in ds_boundary.dims else \"analysis_time\"\n",
    "forecast_duration_dim = (\n",
    "    \"elapsed_forecast_duration\"\n",
    "    if \"elapsed_forecast_duration\" in ds_boundary.dims\n",
    "    else None\n",
    ")\n",
    "dims_to_transpose = [\n",
    "    dim\n",
    "    for dim in [temporal_dim, forecast_duration_dim, \"latitude\", \"longitude\"]\n",
    "    if dim is not None\n",
    "]\n",
    "\n",
    "ds_boundary = ds_boundary.sel(forcing_feature=list(VARIABLES_BOUNDARY.keys()))\n",
    "ds_boundary = ds_boundary.sel(split_name=\"test\").drop_dims([\n",
    "    \"split_part\",\n",
    "    \"static_feature\",\n",
    "])\n",
    "for feature in ds_boundary.forcing_feature.values:\n",
    "    ds_boundary[VARIABLES_BOUNDARY[feature]] = ds_boundary[\"forcing\"].sel(\n",
    "        forcing_feature=feature\n",
    "    )\n",
    "ds_boundary = ds_boundary.drop_vars([\n",
    "    \"forcing\",\n",
    "    \"forcing_feature\",\n",
    "    \"forcing_feature_units\",\n",
    "    \"forcing_feature_long_name\",\n",
    "    \"forcing_feature_source_dataset\",\n",
    "    \"forcing__train__diff_mean\",\n",
    "    \"forcing__train__diff_std\",\n",
    "    \"forcing__train__mean\",\n",
    "    \"forcing__train__std\",\n",
    "])\n",
    "ds_boundary = ds_boundary.set_index(grid_index=[\"latitude\", \"longitude\"])\n",
    "ds_boundary = ds_boundary.unstack(\"grid_index\")\n",
    "ds_boundary = ds_boundary.transpose(*dims_to_transpose)\n",
    "longitude_new = np.where(\n",
    "    ds_boundary[\"longitude\"] > 180,\n",
    "    ds_boundary[\"longitude\"] - 360,\n",
    "    ds_boundary[\"longitude\"],\n",
    ")\n",
    "ds_boundary = ds_boundary.assign_coords(longitude=longitude_new).sortby([\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "])\n",
    "\n",
    "\n",
    "lon_mesh, lat_mesh = np.meshgrid(ds_boundary.longitude, ds_boundary.latitude)\n",
    "ds_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_maps(\n",
    "    ds_gt,\n",
    "    ds_ml,\n",
    "    ds_nwp,\n",
    "    ds_boundary=None,\n",
    "    var=None,\n",
    "    plot_time=None,\n",
    "    step_size_gt=pd.Timedelta(\"1h\"),\n",
    "    random_seed=42,\n",
    "    zoom_factor=None,\n",
    "):\n",
    "    # Handle variable selection\n",
    "    variables = [var] if var else VARIABLES_GROUND_TRUTH.values()\n",
    "\n",
    "    if plot_time is None:\n",
    "        random.seed(random_seed)\n",
    "        time_index = random.randint(0, len(ds_gt.time) - 1)\n",
    "        time_selected = ds_ml.time[time_index].values\n",
    "    else:\n",
    "        time_selected = plot_time\n",
    "\n",
    "    # Get number of forecast dimensions\n",
    "    n_elapsed_forecast_durations = len(ds_ml.elapsed_forecast_duration)\n",
    "\n",
    "    for var in variables:\n",
    "        # Determine number of columns based on NWP data availability\n",
    "        n_cols = 3 if (ds_nwp is not None and var in ds_nwp) else 2\n",
    "\n",
    "        # Create figure with n_elapsed_forecast_durations rows and n_cols columns\n",
    "        fig = plt.figure(\n",
    "            figsize=(7 * n_cols, 4.5 * n_elapsed_forecast_durations), dpi=DPI\n",
    "        )\n",
    "        axes = np.array([\n",
    "            [\n",
    "                plt.subplot(\n",
    "                    n_elapsed_forecast_durations,\n",
    "                    n_cols,\n",
    "                    i * n_cols + j + 1,\n",
    "                    projection=PROJECTION,\n",
    "                )\n",
    "                for j in range(n_cols)\n",
    "            ]\n",
    "            for i in range(n_elapsed_forecast_durations)\n",
    "        ])\n",
    "\n",
    "        # Initialize arrays for global min/max\n",
    "        arrays_for_minmax = []\n",
    "\n",
    "        # Process each elapsed forecast dimension\n",
    "        for dim_idx, elapsed_forecast_dimension in enumerate(\n",
    "            ds_ml.elapsed_forecast_duration\n",
    "        ):\n",
    "            ds_ml_time = ds_ml.sel(\n",
    "                start_time=time_selected,\n",
    "                elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "            )\n",
    "            ds_nwp_time = (\n",
    "                ds_nwp.sel(\n",
    "                    start_time=time_selected,\n",
    "                    elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "                )\n",
    "                if ds_nwp is not None\n",
    "                else None\n",
    "            )\n",
    "            ds_gt_time = ds_gt.sel(time=ds_ml_time.forecast_time)\n",
    "\n",
    "            # Get boundary data with original time selection logic\n",
    "            if ds_boundary is not None and var in ds_boundary:\n",
    "                if \"elapsed_forecast_duration\" in ds_boundary:\n",
    "                    if (\n",
    "                        ds_boundary.sel(\n",
    "                            analysis_time=time_selected - step_size_gt,\n",
    "                            method=\"pad\",\n",
    "                        ).analysis_time.values\n",
    "                        == time_selected - step_size_gt\n",
    "                    ):\n",
    "                        steps = 2\n",
    "                    else:\n",
    "                        steps = 1\n",
    "\n",
    "                    ds_boundary_var = ds_boundary[var].sel(\n",
    "                        analysis_time=time_selected - steps * step_size_gt,\n",
    "                        method=\"pad\",\n",
    "                    )\n",
    "                    forecast_times = (\n",
    "                        ds_boundary_var.analysis_time.values\n",
    "                        + ds_boundary_var.elapsed_forecast_duration.values\n",
    "                    )\n",
    "                    ds_boundary_var = ds_boundary_var.assign_coords(\n",
    "                        forecast_time=(\n",
    "                            \"elapsed_forecast_duration\",\n",
    "                            forecast_times,\n",
    "                        )\n",
    "                    ).set_xindex(\"forecast_time\")\n",
    "                    ds_boundary_var = ds_boundary_var.sel(\n",
    "                        forecast_time=time_selected, method=\"pad\"\n",
    "                    )\n",
    "                else:\n",
    "                    ds_boundary_var = ds_boundary[var].sel(\n",
    "                        time=time_selected, method=\"pad\"\n",
    "                    )\n",
    "                arrays_for_minmax.append(ds_boundary_var.values)\n",
    "\n",
    "            # Collect data for min/max calculation\n",
    "            arrays_for_minmax.extend([\n",
    "                ds_gt_time[var].values,\n",
    "                ds_ml_time[var].values,\n",
    "            ])\n",
    "            if ds_nwp_time is not None and var in ds_nwp_time:\n",
    "                arrays_for_minmax.append(ds_nwp_time[var].values)\n",
    "\n",
    "        # Calculate global min/max\n",
    "        combined_array = np.concatenate([\n",
    "            arr.flatten() for arr in arrays_for_minmax\n",
    "        ])\n",
    "        vmin, vmax = np.nanmin(combined_array), np.nanmax(combined_array)\n",
    "\n",
    "        # Plot for each elapsed forecast dimension\n",
    "        for dim_idx, elapsed_forecast_dimension in enumerate(\n",
    "            ds_ml.elapsed_forecast_duration\n",
    "        ):\n",
    "            ds_ml_time = ds_ml.sel(\n",
    "                start_time=time_selected,\n",
    "                elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "            )\n",
    "            ds_nwp_time = (\n",
    "                ds_nwp.sel(\n",
    "                    start_time=time_selected,\n",
    "                    elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "                )\n",
    "                if ds_nwp is not None\n",
    "                else None\n",
    "            )\n",
    "            ds_gt_time = ds_gt.sel(time=ds_ml_time.forecast_time)\n",
    "\n",
    "            # Calculate forecast hours\n",
    "            forecast_hours = int(elapsed_forecast_dimension.values / 1e9 / 3600)\n",
    "\n",
    "            # Plot boundary on all subplots if available\n",
    "            if ds_boundary is not None and var in ds_boundary:\n",
    "                for ax in axes[dim_idx]:\n",
    "                    ax.contourf(\n",
    "                        lon_mesh,\n",
    "                        lat_mesh,\n",
    "                        ds_boundary_var.values,\n",
    "                        transform=ccrs.PlateCarree(),\n",
    "                        cmap=\"viridis\",\n",
    "                        vmin=vmin,\n",
    "                        vmax=vmax,\n",
    "                        alpha=0.5,\n",
    "                        levels=20,\n",
    "                    )\n",
    "                    if zoom_factor is not None:\n",
    "                        set_map_extent(ax, zoom_factor, ds_boundary_var)\n",
    "\n",
    "            # Plot ground truth\n",
    "            im0 = plot_field(axes[dim_idx, 0], ds_gt_time[var], vmin, vmax)\n",
    "\n",
    "            # Add titles - model names only in first row, forecast hours in all rows\n",
    "            if dim_idx == 0:\n",
    "                axes[dim_idx, 0].set_title(f\"Ground Truth\\n+{forecast_hours}h\")\n",
    "                if ds_nwp_time is not None and var in ds_nwp_time:\n",
    "                    axes[dim_idx, 1].set_title(f\"NWP\\n+{forecast_hours}h\")\n",
    "                    axes[dim_idx, 2].set_title(f\"ML\\n+{forecast_hours}h\")\n",
    "                else:\n",
    "                    axes[dim_idx, 1].set_title(f\"ML\\n+{forecast_hours}h\")\n",
    "            else:\n",
    "                axes[dim_idx, 0].set_title(f\"+{forecast_hours}h\")\n",
    "                if ds_nwp_time is not None and var in ds_nwp_time:\n",
    "                    axes[dim_idx, 1].set_title(f\"+{forecast_hours}h\")\n",
    "                    axes[dim_idx, 2].set_title(f\"+{forecast_hours}h\")\n",
    "                else:\n",
    "                    axes[dim_idx, 1].set_title(f\"+{forecast_hours}h\")\n",
    "\n",
    "            col = 1\n",
    "            # Plot NWP if available\n",
    "            if ds_nwp_time is not None and var in ds_nwp_time:\n",
    "                plot_field(axes[dim_idx, col], ds_nwp_time[var], vmin, vmax)\n",
    "                col += 1\n",
    "\n",
    "            # Plot ML prediction\n",
    "            plot_field(axes[dim_idx, col], ds_ml_time[var], vmin, vmax)\n",
    "\n",
    "        # Add common features and colorbar\n",
    "        add_map_features(axes)\n",
    "        add_colorbar(fig, im0, var)\n",
    "\n",
    "        plt.subplots_adjust(\n",
    "            top=0.95,\n",
    "            bottom=0.05,\n",
    "            hspace=0.2,\n",
    "            wspace=0.05,\n",
    "        )\n",
    "        title = f\"{var} starting at {str(time_selected.dt.date.values)} - {time_selected.dt.hour.values:02d} UTC\"\n",
    "        plt.suptitle(title, y=0.98)\n",
    "        plt.show()\n",
    "        save_plot(fig, f\"map_{var}_multi_efd\", time_selected)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def set_map_extent(ax, zoom_factor=None, boundary_data=None):\n",
    "    \"\"\"Set the map extent based on zoom factor and boundary data.\"\"\"\n",
    "    if zoom_factor is not None and boundary_data is not None:\n",
    "        # Get the boundary extent\n",
    "        lon = (\n",
    "            boundary_data.longitude\n",
    "            if hasattr(boundary_data, \"longitude\")\n",
    "            else boundary_data.lon\n",
    "        )\n",
    "        lat = (\n",
    "            boundary_data.latitude\n",
    "            if hasattr(boundary_data, \"latitude\")\n",
    "            else boundary_data.lat\n",
    "        )\n",
    "\n",
    "        # Calculate center\n",
    "        lon_center = (lon.max() + lon.min()) / 2\n",
    "        lat_center = (lat.max() + lat.min()) / 2\n",
    "\n",
    "        # Calculate ranges\n",
    "        lon_range = (lon.max() - lon.min()) / zoom_factor\n",
    "        lat_range = (lat.max() - lat.min()) / zoom_factor\n",
    "\n",
    "        # Set new extent\n",
    "        ax.set_extent(\n",
    "            [\n",
    "                lon_center - lon_range / 2,\n",
    "                lon_center + lon_range / 2,\n",
    "                lat_center - lat_range / 2,\n",
    "                lat_center + lat_range / 2,\n",
    "            ],\n",
    "            crs=ccrs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_field(ax, data, vmin, vmax):\n",
    "    return ax.pcolormesh(\n",
    "        data.longitude if hasattr(data, \"longitude\") else data.lon,\n",
    "        data.latitude if hasattr(data, \"latitude\") else data.lat,\n",
    "        data.values,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cmap=\"viridis\",\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_map_features(axes):\n",
    "    n_rows, _ = axes.shape\n",
    "    for i, ax_row in enumerate(axes):\n",
    "        for j, ax in enumerate(ax_row):\n",
    "            ax.coastlines(resolution=\"50m\")\n",
    "            ax.add_feature(cfeature.BORDERS, linestyle=\"-\", alpha=0.7)\n",
    "            gl = ax.gridlines(\n",
    "                draw_labels=True, dms=True, x_inline=False, y_inline=False\n",
    "            )\n",
    "\n",
    "            # Turn off all labels by default\n",
    "            gl.top_labels = False\n",
    "            gl.bottom_labels = False\n",
    "            gl.left_labels = False\n",
    "            gl.right_labels = False\n",
    "\n",
    "            # Enable left labels only for leftmost column\n",
    "            if j == 0:\n",
    "                gl.left_labels = True\n",
    "\n",
    "            # Enable bottom labels only for last row\n",
    "            if i == n_rows - 1:\n",
    "                gl.bottom_labels = True\n",
    "\n",
    "\n",
    "def add_colorbar(fig, im, var):\n",
    "    cbar_ax = fig.add_axes([0.2, 0.0, 0.6, 0.02])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation=\"horizontal\")\n",
    "    cbar.set_label(VARIABLE_UNITS[var])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0382c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_gt = pd.Timedelta(ds_gt.time.diff(\"time\").min().values, \"h\")\n",
    "ds_ml = ds_ml.assign_coords({\n",
    "    \"lon\": ((\"x\", \"y\"), ds_nwp.lon.values),\n",
    "    \"lat\": ((\"x\", \"y\"), ds_nwp.lat.values),\n",
    "})\n",
    "if PLOT_TIME is None:\n",
    "    time_selected = None\n",
    "else:\n",
    "    time_selected = ds_ml.sel(start_time=PLOT_TIME).start_time\n",
    "create_comparison_maps(\n",
    "    ds_gt=ds_gt,\n",
    "    ds_ml=ds_ml,\n",
    "    ds_nwp=ds_nwp,\n",
    "    ds_boundary=ds_boundary,\n",
    "    plot_time=time_selected,\n",
    "    step_size_gt=step_size_gt,\n",
    "    zoom_factor=ZOOM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73b3c1",
   "metadata": {},
   "source": [
    "#### Mean Error Plot For The Same Time Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92322ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_error_maps(\n",
    "    ds_gt,\n",
    "    ds_ml,\n",
    "    ds_nwp=None,\n",
    "    var=None,\n",
    "    plot_time=None,\n",
    "    random_seed=42,\n",
    "):\n",
    "    \"\"\"Create error maps for model outputs organized in subfigures.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds_gt : xarray.Dataset\n",
    "        Ground truth dataset\n",
    "    ds_ml : xarray.Dataset\n",
    "        ML model predictions\n",
    "    ds_nwp : xarray.Dataset, optional\n",
    "        NWP model predictions\n",
    "    var : str, optional\n",
    "        Variable to plot (if None, plots all variables)\n",
    "    plot_time : str, optional\n",
    "        Specific time to plot (format: \"YYYY-MM-DD HH:MM:SS\")\n",
    "    random_seed : int, default=42\n",
    "        Random seed for time selection\n",
    "    \"\"\"\n",
    "    # Handle variable selection\n",
    "    variables = [var] if var else VARIABLES_GROUND_TRUTH.values()\n",
    "\n",
    "    if plot_time is None:\n",
    "        random.seed(random_seed)\n",
    "        time_index = random.randint(0, len(ds_gt.time) - 1)\n",
    "        time_selected = ds_ml.time[time_index].values\n",
    "    else:\n",
    "        time_selected = plot_time\n",
    "\n",
    "    # Get number of forecast dimensions\n",
    "    n_elapsed_forecast_durations = len(ds_ml.elapsed_forecast_duration)\n",
    "\n",
    "    for var in variables:\n",
    "        # Determine number of columns based on NWP data availability\n",
    "        n_cols = 2 if (ds_nwp is not None and var in ds_nwp) else 1\n",
    "\n",
    "        # Create figure with n_elapsed_forecast_durations rows and n_cols columns\n",
    "        fig = plt.figure(\n",
    "            figsize=(7 * n_cols, 4.5 * n_elapsed_forecast_durations), dpi=DPI\n",
    "        )\n",
    "        axes = np.array([\n",
    "            [\n",
    "                plt.subplot(\n",
    "                    n_elapsed_forecast_durations,\n",
    "                    n_cols,\n",
    "                    i * n_cols + j + 1,\n",
    "                    projection=PROJECTION,\n",
    "                )\n",
    "                for j in range(n_cols)\n",
    "            ]\n",
    "            for i in range(n_elapsed_forecast_durations)\n",
    "        ])\n",
    "\n",
    "        # Initialize arrays for global min/max\n",
    "        arrays_for_minmax = []\n",
    "\n",
    "        # Process each elapsed forecast dimension\n",
    "        for dim_idx, elapsed_forecast_dimension in enumerate(\n",
    "            ds_ml.elapsed_forecast_duration\n",
    "        ):\n",
    "            ds_ml_time = ds_ml.sel(\n",
    "                start_time=time_selected,\n",
    "                elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "            )\n",
    "            ds_nwp_time = (\n",
    "                ds_nwp.sel(\n",
    "                    start_time=time_selected,\n",
    "                    elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "                )\n",
    "                if ds_nwp is not None\n",
    "                else None\n",
    "            )\n",
    "            ds_gt_time = ds_gt.sel(time=ds_ml_time.forecast_time)\n",
    "\n",
    "            # Calculate errors\n",
    "            error_ml = ds_ml_time[var] - ds_gt_time[var]\n",
    "            arrays_for_minmax.append(error_ml.values)\n",
    "\n",
    "            if ds_nwp_time is not None and var in ds_nwp_time:\n",
    "                error_nwp = ds_nwp_time[var] - ds_gt_time[var]\n",
    "                arrays_for_minmax.append(error_nwp.values)\n",
    "\n",
    "        # Calculate global min/max for symmetric colorbar\n",
    "        max_abs_error = np.max(np.abs(arrays_for_minmax))\n",
    "        vmin, vmax = -max_abs_error, max_abs_error\n",
    "\n",
    "        # Plot for each elapsed forecast dimension\n",
    "        for dim_idx, elapsed_forecast_dimension in enumerate(\n",
    "            ds_ml.elapsed_forecast_duration\n",
    "        ):\n",
    "            ds_ml_time = ds_ml.sel(\n",
    "                start_time=time_selected,\n",
    "                elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "            )\n",
    "            ds_nwp_time = (\n",
    "                ds_nwp.sel(\n",
    "                    start_time=time_selected,\n",
    "                    elapsed_forecast_duration=elapsed_forecast_dimension,\n",
    "                )\n",
    "                if ds_nwp is not None\n",
    "                else None\n",
    "            )\n",
    "            ds_gt_time = ds_gt.sel(time=ds_ml_time.forecast_time)\n",
    "\n",
    "            # Calculate forecast hours\n",
    "            forecast_hours = int(elapsed_forecast_dimension.values / 1e9 / 3600)\n",
    "\n",
    "            # Get coordinates\n",
    "            lons = (\n",
    "                ds_gt_time.longitude\n",
    "                if hasattr(ds_gt_time, \"longitude\")\n",
    "                else ds_gt_time.lon\n",
    "            )\n",
    "            lats = (\n",
    "                ds_gt_time.latitude\n",
    "                if hasattr(ds_gt_time, \"latitude\")\n",
    "                else ds_gt_time.lat\n",
    "            )\n",
    "\n",
    "            # Plot NWP error if available\n",
    "            if ds_nwp_time is not None and var in ds_nwp_time:\n",
    "                error_nwp = ds_nwp_time[var] - ds_gt_time[var]\n",
    "                plot_error_field(\n",
    "                    axes[dim_idx, 0], lons, lats, error_nwp.values, vmin, vmax\n",
    "                )\n",
    "                if dim_idx == 0:\n",
    "                    axes[dim_idx, 0].set_title(f\"NWP Error\\n+{forecast_hours}h\")\n",
    "                else:\n",
    "                    axes[dim_idx, 0].set_title(f\"+{forecast_hours}h\")\n",
    "\n",
    "            # Plot ML error\n",
    "            error_ml = ds_ml_time[var] - ds_gt_time[var]\n",
    "            col_idx = 1 if ds_nwp_time is not None and var in ds_nwp_time else 0\n",
    "            im = plot_error_field(\n",
    "                axes[dim_idx, col_idx], lons, lats, error_ml.values, vmin, vmax\n",
    "            )\n",
    "            if dim_idx == 0:\n",
    "                axes[dim_idx, col_idx].set_title(\n",
    "                    f\"ML Error\\n+{forecast_hours}h\"\n",
    "                )\n",
    "            else:\n",
    "                axes[dim_idx, col_idx].set_title(f\"+{forecast_hours}h\")\n",
    "\n",
    "        # Add common features and colorbar\n",
    "        add_map_features(axes)\n",
    "        add_error_colorbar(fig, im, var)\n",
    "\n",
    "        plt.subplots_adjust(\n",
    "            top=0.95,\n",
    "            bottom=0.05,\n",
    "            hspace=0.2,\n",
    "            wspace=0.05,\n",
    "        )\n",
    "        title = f\"Error in {var} starting at {str(time_selected.dt.date.values)} - {time_selected.dt.hour.values:02d} UTC\"\n",
    "        plt.suptitle(title, y=0.98)\n",
    "        plt.show()\n",
    "        save_plot(fig, f\"errormap_{var}_multi_efd\", time_selected)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_error_field(ax, lons, lats, data, vmin, vmax):\n",
    "    return ax.pcolormesh(\n",
    "        lons,\n",
    "        lats,\n",
    "        data,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=\"RdBu\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_error_colorbar(fig, im, var):\n",
    "    cbar_ax = fig.add_axes([0.2, 0.0, 0.6, 0.02])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation=\"horizontal\")\n",
    "    cbar.set_label(f\"Error in {VARIABLE_UNITS[var]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"lon\" not in ds_ml.coords:\n",
    "    ds_ml = ds_ml.assign_coords({\n",
    "        \"lon\": ((\"x\", \"y\"), ds_nwp.lon.values),\n",
    "        \"lat\": ((\"x\", \"y\"), ds_nwp.lat.values),\n",
    "    })\n",
    "if PLOT_TIME is None:\n",
    "    time_selected = None\n",
    "else:\n",
    "    time_selected = ds_ml.sel(start_time=PLOT_TIME).start_time\n",
    "create_error_maps(\n",
    "    ds_gt=ds_gt,\n",
    "    ds_ml=ds_ml,\n",
    "    ds_nwp=ds_nwp,\n",
    "    plot_time=time_selected,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df26997",
   "metadata": {},
   "source": [
    "### 2. Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74267537",
   "metadata": {},
   "source": [
    "By examining these distributions, we can assess whether the ML model and NWP model accurately capture the variability and frequency of different atmospheric states.\n",
    "\n",
    "**Distribution Shape:** The histograms show whether the models replicate the skewness, kurtosis, and overall shape of the ground truth data distributions.\n",
    "\n",
    "**Extreme Values:** Identifying how the models handle extreme conditions, such as unusually high or low temperatures, is crucial for weather prediction and risk assessment.\n",
    "\n",
    "**Normalization Needs:** Differences in scale between variables suggest that normalization may be necessary for accurate comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of each dimension\n",
    "ds_gt_sampled = ds_gt.isel(\n",
    "    time=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    "    x=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    "    y=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    ")\n",
    "ds_ml_sampled = ds_ml.isel(\n",
    "    start_time=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    "    x=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    "    y=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    ")\n",
    "ds_nwp_sampled = ds_nwp.isel(\n",
    "    start_time=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    "    x=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    "    y=slice(None, None, SUBSAMPLE_HISTOGRAM),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c28a1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for variable_name in VARIABLES_GROUND_TRUTH.values():\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=DPI)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    data_gt = ds_gt_sampled[variable_name].values.flatten()\n",
    "    data_ml = ds_ml_sampled[variable_name].values.flatten()\n",
    "\n",
    "    ax.hist(\n",
    "        data_gt,\n",
    "        bins=500,\n",
    "        density=True,\n",
    "        color=COLORS[\"gt\"],\n",
    "        label=\"Ground Truth\",\n",
    "    )\n",
    "    # Plot NWP if available\n",
    "    if variable_name in ds_nwp:\n",
    "        data_nwp = ds_nwp_sampled[variable_name].values.flatten()\n",
    "        ax.hist(\n",
    "            data_nwp,\n",
    "            bins=500,\n",
    "            alpha=0.8,\n",
    "            density=True,\n",
    "            color=COLORS[\"nwp\"],\n",
    "            label=\"NWP Model Prediction\",\n",
    "        )\n",
    "\n",
    "    # Create histograms for ML and ground truth\n",
    "    ax.hist(\n",
    "        data_ml,\n",
    "        bins=500,\n",
    "        alpha=0.8,\n",
    "        density=True,\n",
    "        color=COLORS[\"ml\"],\n",
    "        label=\"ML Model Prediction\",\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    units = VARIABLE_UNITS[variable_name]\n",
    "    ax.set_title(f\"Distribution of {variable_name} ({units})\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Calculate skewness and kurtosis\n",
    "    stats_gt = f\"Ground Truth:\\nSkewness: {skew(data_gt):.2f}\\nKurtosis: {kurtosis(data_gt):.2f}\"\n",
    "    stats_ml = f\"ML Model:\\nSkewness: {skew(data_ml):.2f}\\nKurtosis: {kurtosis(data_ml):.2f}\"\n",
    "\n",
    "    # Combine stats\n",
    "    stats_text = stats_gt + \"\\n\\n\" + stats_ml\n",
    "\n",
    "    if variable_name in ds_nwp:\n",
    "        stats_nwp = f\"NWP Model:\\nSkewness: {skew(data_nwp):.2f}\\nKurtosis: {kurtosis(data_nwp):.2f}\"\n",
    "        stats_text = stats_text + \"\\n\\n\" + stats_nwp\n",
    "\n",
    "    # Add text box\n",
    "    ax.text(\n",
    "        0.95,\n",
    "        0.55,\n",
    "        stats_text,\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(alpha=0.8, facecolor=\"white\", edgecolor=\"black\"),\n",
    "        color=\"black\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        horizontalalignment=\"right\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, f\"histogram_{variable_name}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1dc37e",
   "metadata": {},
   "source": [
    "### 3. Energy Spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edca0eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This chapter examines how energy is distributed across different spatial scales\n",
    "in the atmosphere by computing and comparing the energy spectra of both models.\n",
    "This analysis is critical in understanding the models' capabilities to simulate\n",
    "atmospheric processes ranging from large-scale weather systems to small-scale\n",
    "turbulence.\n",
    "\n",
    "**FFT Computation:** The Fast Fourier Transform (FFT) is used to transform spatial data into the frequency domain, revealing how different scales contribute to the overall energy. The energy spectra are averaged over latitudes.\n",
    "\n",
    "**Scale Representation:** The energy spectra show whether the ML model captures the correct amount of energy at various spatial scales.\n",
    "\n",
    "**Effective Resolution:** Identifying the effective resolution helps understand the smallest scales that the model can reliably simulate.\n",
    "\n",
    "**Numerical Artifacts:** Limitations in numerical precision can introduce artifacts in the spectra, especially at the smallest scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0be521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_energy_spectra(data):\n",
    "    \"\"\"Calculate the energy spectra of the given data using 2D FFT.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xarray.DataArray\n",
    "        The data for which the energy spectra should be calculated.\n",
    "        Expected dimensions must include 'x' and 'y', other dimensions will be handled automatically.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wavenumber : np.ndarray\n",
    "        The isotropic wavenumbers.\n",
    "    power : np.ndarray\n",
    "        The power spectrum averaged over all non-spatial dimensions.\n",
    "    effective_resolution : float\n",
    "        The effective resolution of the model.\n",
    "    \"\"\"\n",
    "    # Get grid spacing in meters\n",
    "    dx = abs(float(data.x[1] - data.x[0]))\n",
    "    dy = abs(float(data.y[1] - data.y[0]))\n",
    "\n",
    "    # Identify spatial dimensions\n",
    "    spatial_dims = [\"y\", \"x\"]\n",
    "\n",
    "    # Move spatial dimensions to the end\n",
    "    other_dims = [dim for dim in data.dims if dim not in spatial_dims]\n",
    "    var_data = data.transpose(*other_dims, *spatial_dims).values\n",
    "\n",
    "    # Reshape the array to combine all non-spatial dimensions\n",
    "    shape = var_data.shape\n",
    "    ny, nx = shape[-2:]\n",
    "\n",
    "    if len(shape) > 2:\n",
    "        var_data = var_data.reshape(-1, ny, nx)\n",
    "    else:\n",
    "        var_data = var_data[np.newaxis, :, :]\n",
    "\n",
    "    # Compute 2D FFT for each sample\n",
    "    fft_data = np.fft.fft2(var_data, axes=(-2, -1))\n",
    "    power_spectrum = (np.abs(fft_data) ** 2).mean(\n",
    "        axis=0\n",
    "    )  # Average over all samples\n",
    "\n",
    "    # Rest of the function remains the same\n",
    "    # Get wavenumbers\n",
    "    kx = np.fft.fftfreq(nx, d=dx)\n",
    "    ky = np.fft.fftfreq(ny, d=dy)\n",
    "\n",
    "    # Create 2D wavenumber grid\n",
    "    kxx, kyy = np.meshgrid(kx, ky)\n",
    "    k_mag = np.sqrt(kxx**2 + kyy**2)\n",
    "\n",
    "    # Create wavenumber bins for azimuthal averaging\n",
    "    k_bins = np.logspace(\n",
    "        np.log10(k_mag[k_mag > 0].min()), np.log10(k_mag.max()), num=50\n",
    "    )\n",
    "\n",
    "    # Perform azimuthal averaging\n",
    "    k_averaged = []\n",
    "    power_averaged = []\n",
    "\n",
    "    for i in range(len(k_bins) - 1):\n",
    "        k_mask = (k_mag >= k_bins[i]) & (k_mag < k_bins[i + 1])\n",
    "        if k_mask.any():\n",
    "            k_averaged.append(np.mean(k_mag[k_mask]))\n",
    "            power_averaged.append(np.mean(power_spectrum[k_mask]))\n",
    "\n",
    "    # Convert to arrays\n",
    "    k_averaged = np.array(k_averaged)\n",
    "    power_averaged = np.array(power_averaged)\n",
    "\n",
    "    # Calculate effective resolution\n",
    "    effective_resolution = 1 / (4 * dx)\n",
    "\n",
    "    # Remove first and last two wavenumbers\n",
    "    return k_averaged[2:-2], power_averaged[2:-2], effective_resolution\n",
    "\n",
    "\n",
    "def plot_energy_spectra(ds_gt, ds_nwp, ds_ml, var, level=None):\n",
    "    \"\"\"Plot energy spectra comparison with LSD metric.\"\"\"\n",
    "    if level is not None:\n",
    "        var_data = ds_gt[var].sel(z=level)\n",
    "        if var in ds_nwp:\n",
    "            var_data_nwp = ds_nwp[var].sel(z=level)\n",
    "        var_data_ml = ds_ml[var].sel(z=level)\n",
    "    else:\n",
    "        var_data = ds_gt[var]\n",
    "        if var in ds_nwp:\n",
    "            var_data_nwp = ds_nwp[var]\n",
    "        var_data_ml = ds_ml[var]\n",
    "\n",
    "    # Calculate energy spectra\n",
    "    wavenumber_gt, spectrum_gt, effective_resolution = calculate_energy_spectra(\n",
    "        var_data\n",
    "    )\n",
    "    if var in ds_nwp:\n",
    "        wavenumber_nwp, spectrum_nwp, _ = calculate_energy_spectra(var_data_nwp)\n",
    "    else:\n",
    "        spectrum_nwp = None\n",
    "    wavenumber_ml, spectrum_ml, _ = calculate_energy_spectra(var_data_ml)\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=DPI)\n",
    "\n",
    "    # Plot spectra\n",
    "    ax.loglog(\n",
    "        wavenumber_gt,\n",
    "        spectrum_gt,\n",
    "        color=COLORS[\"gt\"],\n",
    "        label=\"Ground Truth\",\n",
    "        linestyle=LINE_STYLES[\"gt\"][0],\n",
    "        marker=LINE_STYLES[\"gt\"][1],\n",
    "        markevery=5,\n",
    "    )  # Add markers every 5 points for clarity\n",
    "\n",
    "    if var in ds_nwp:\n",
    "        ax.loglog(\n",
    "            wavenumber_nwp,\n",
    "            spectrum_nwp,\n",
    "            color=COLORS[\"nwp\"],\n",
    "            label=\"NWP Model Prediction\",\n",
    "            linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "            marker=LINE_STYLES[\"nwp\"][1],\n",
    "            markevery=3,\n",
    "        )\n",
    "\n",
    "    ax.loglog(\n",
    "        wavenumber_ml,\n",
    "        spectrum_ml,\n",
    "        color=COLORS[\"ml\"],\n",
    "        label=\"ML Model Prediction\",\n",
    "        linestyle=LINE_STYLES[\"ml\"][0],\n",
    "        marker=LINE_STYLES[\"ml\"][1],\n",
    "        markevery=4,\n",
    "    )\n",
    "\n",
    "    # Plot effective resolution\n",
    "    ax.axvline(\n",
    "        effective_resolution,\n",
    "        color=\"salmon\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Effective Model Resolution\",\n",
    "    )\n",
    "\n",
    "    # Add LSD metric\n",
    "    add_lsd_to_plot(ax, spectrum_gt, spectrum_nwp, spectrum_ml)\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xlabel(\"Wavenumber [1/m]\")\n",
    "    unit = VARIABLE_UNITS.get(var, \"\")\n",
    "    ax.set_ylabel(f\"Power Spectral Density [{unit}²/m]\")\n",
    "    title = f\"Energy Spectra Comparison for {var}\"\n",
    "    if level is not None:\n",
    "        title += f\" at Level {level} hPa\"\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "\n",
    "    # Save plot\n",
    "    plot_name = f\"energy_spectra_{var}\"\n",
    "    if level is not None:\n",
    "        plot_name += f\"_level_{level}\"\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(fig, plot_name)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def calculate_log_spectral_distance(true_spectrum, nwp_spectrum, ml_spectrum):\n",
    "    \"\"\"\n",
    "    Calculate the Log Spectral Distance between three power spectra\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    log_spec1 = np.log10(true_spectrum + eps)\n",
    "    log_spec3 = np.log10(ml_spectrum + eps)\n",
    "    lsd_ml = np.sqrt(np.mean((log_spec1 - log_spec3) ** 2))\n",
    "    if nwp_spectrum is None:\n",
    "        return None, lsd_ml\n",
    "    log_spec2 = np.log10(nwp_spectrum + eps)\n",
    "    lsd_nwp = np.sqrt(np.mean((log_spec1 - log_spec2) ** 2))\n",
    "    return lsd_nwp, lsd_ml\n",
    "\n",
    "\n",
    "def add_lsd_to_plot(ax, true_spectrum, nwp_spectrum, ml_spectrum):\n",
    "    \"\"\"\n",
    "    Add LSD metric as text box to spectrum plot\n",
    "    \"\"\"\n",
    "    if nwp_spectrum is None:\n",
    "        lsd_nwp = None\n",
    "        _, lsd_ml = calculate_log_spectral_distance(\n",
    "            true_spectrum, None, ml_spectrum\n",
    "        )\n",
    "        textstr = f\"LSD ML = {lsd_ml:.4f}\"\n",
    "    else:\n",
    "        lsd_nwp, lsd_ml = calculate_log_spectral_distance(\n",
    "            true_spectrum, nwp_spectrum, ml_spectrum\n",
    "        )\n",
    "        textstr = f\"LSD NWP = {lsd_nwp:.4f}, LSD ML = {lsd_ml:.4f}\"\n",
    "    props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5)\n",
    "    ax.text(\n",
    "        0.4,\n",
    "        0.05,\n",
    "        textstr,\n",
    "        transform=ax.transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=props,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e005e9",
   "metadata": {},
   "source": [
    "To interpret the Log-Spectral Distance (LSD) metric:\n",
    "The LSD quantifies the difference between two spectra, with lower values indicating better similarity (area between the two spectra). \n",
    "\n",
    "Lower values indicate better similarity between spectra\n",
    "- LSD = 0 means identical spectra\n",
    "\n",
    "Typical values depend on the specific application, but generally:\n",
    "- LSD < 1: Good similarity\n",
    "- 1 < LSD < 2: Moderate differences\n",
    "- LSD > 2: Significant differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with LocalCluster(\n",
    "    n_workers=16,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=\"16GB\",\n",
    ") as cluster:\n",
    "    with Client(cluster) as client:\n",
    "        for var in VARIABLES_GROUND_TRUTH.values():\n",
    "            plot_energy_spectra(\n",
    "                ds_gt.isel(time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "                ds_nwp.isel(start_time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "                ds_ml.isel(start_time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "                var,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_export_lsd_table(\n",
    "    ds_gt, ds_ml, ds_nwp, variables, name, caption=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Display and export a table with LSD metrics for variables as rows and ML/NWP as columns.\n",
    "    Handles missing variables in NWP dataset gracefully.\n",
    "\n",
    "    Args:\n",
    "        ds_gt, ds_ml, ds_nwp: Input datasets\n",
    "        variables: Variables to analyze\n",
    "        name: Name for the exported files\n",
    "        caption: Caption for the LaTeX table\n",
    "    \"\"\"\n",
    "    # Initialize data dictionary with ML and NWP columns\n",
    "    lsd_data = {var: {\"ML\": None, \"NWP\": None} for var in variables}\n",
    "\n",
    "    for var in variables:\n",
    "        # Skip if variable not in ground truth\n",
    "        if var not in ds_gt:\n",
    "            continue\n",
    "\n",
    "        var_data_gt = ds_gt[var]\n",
    "\n",
    "        # Calculate ML metrics if variable exists in ML data\n",
    "        if var in ds_ml:\n",
    "            var_data_ml = ds_ml[var]\n",
    "            _, spectrum_gt, _ = calculate_energy_spectra(var_data_gt)\n",
    "            _, spectrum_ml, _ = calculate_energy_spectra(var_data_ml)\n",
    "            _, lsd_ml = calculate_log_spectral_distance(\n",
    "                spectrum_gt, None, spectrum_ml\n",
    "            )\n",
    "            lsd_data[var][\"ML\"] = lsd_ml\n",
    "\n",
    "        # Calculate NWP metrics if variable exists in NWP data\n",
    "        if var in ds_nwp:\n",
    "            var_data_nwp = ds_nwp[var]\n",
    "            # Recalculate ground truth spectrum only if not done for ML\n",
    "            if var not in ds_ml:\n",
    "                _, spectrum_gt, _ = calculate_energy_spectra(var_data_gt)\n",
    "            _, spectrum_nwp, _ = calculate_energy_spectra(var_data_nwp)\n",
    "            _, lsd_nwp = calculate_log_spectral_distance(\n",
    "                spectrum_gt, None, spectrum_nwp\n",
    "            )\n",
    "            lsd_data[var][\"NWP\"] = lsd_nwp\n",
    "\n",
    "    df = pd.DataFrame(lsd_data).T\n",
    "\n",
    "    # Display styled table\n",
    "    styled_df = df.style.format(\n",
    "        lambda x: f\"{x:.3f}\" if pd.notnull(x) else \"-\"\n",
    "    ).map(\n",
    "        lambda x: f\"color: {'green' if x < 1 else 'orange' if x < 2 else 'red'}\"\n",
    "        if pd.notnull(x)\n",
    "        else \"\"\n",
    "    )\n",
    "    display(styled_df)\n",
    "\n",
    "    # Export raw dataframe\n",
    "    export_table(df, name, caption)\n",
    "\n",
    "\n",
    "with LocalCluster(\n",
    "    n_workers=16,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=\"16GB\",\n",
    ") as cluster:\n",
    "    with Client(cluster) as client:\n",
    "        display_and_export_lsd_table(\n",
    "            ds_gt.isel(time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "            ds_nwp.isel(start_time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "            ds_ml.isel(start_time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "            VARIABLES_GROUND_TRUTH.values(),\n",
    "            name=\"lsd_metrics\",\n",
    "            caption=\"Log Spectral Distance (LSD) metrics comparison between ML and NWP models\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46231998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavenumber_evolution(\n",
    "    ds_gt, ds_ml, ds_nwp, variables, wavenumbers=[1e-2, 5e-2, 1e-1], level=None\n",
    "):\n",
    "    for var in variables:\n",
    "        # Skip if variable not in ground truth\n",
    "        if var not in ds_gt:\n",
    "            continue\n",
    "\n",
    "        # Skip if variable not in ML dataset\n",
    "        if var not in ds_ml:\n",
    "            continue\n",
    "\n",
    "        _, axes = plt.subplots(1, len(wavenumbers), figsize=(18, 6), dpi=DPI)\n",
    "        forecast_times = ds_ml.elapsed_forecast_duration.values\n",
    "\n",
    "        for idx, target_k in enumerate(wavenumbers):\n",
    "            spectra = {\n",
    "                \"gt\": [],\n",
    "                \"ml\": [],\n",
    "                \"nwp\": [] if (ds_nwp is not None and var in ds_nwp) else None,\n",
    "            }\n",
    "\n",
    "            # Calculate GT spectrum once\n",
    "            if level is not None:\n",
    "                gt_data = ds_gt[var].sel(z=level)\n",
    "            else:\n",
    "                gt_data = ds_gt[var]\n",
    "            k, gt_spec, _ = calculate_energy_spectra(gt_data.isel(time=0))\n",
    "            idx_k = np.abs(k - target_k).argmin()\n",
    "            gt_power = gt_spec[idx_k]\n",
    "\n",
    "            for time in forecast_times:\n",
    "                # Handle ML and NWP data\n",
    "                data = {}\n",
    "                for key, ds in zip([\"ml\", \"nwp\"], [ds_ml, ds_nwp]):\n",
    "                    # Skip if dataset is None or variable not in NWP\n",
    "                    if ds is None or (key == \"nwp\" and var not in ds):\n",
    "                        continue\n",
    "\n",
    "                    if level is not None:\n",
    "                        data[key] = ds[var].sel(\n",
    "                            elapsed_forecast_duration=time, z=level\n",
    "                        )\n",
    "                    else:\n",
    "                        data[key] = ds[var].sel(elapsed_forecast_duration=time)\n",
    "\n",
    "                    k, spec, _ = calculate_energy_spectra(data[key])\n",
    "                    idx_k = np.abs(k - target_k).argmin()\n",
    "                    spectra[key].append(spec[idx_k])\n",
    "                spectra[\"gt\"].append(gt_power)\n",
    "\n",
    "            # Plot evolution for current wavenumber\n",
    "            hours = forecast_times / np.timedelta64(1, \"h\")\n",
    "            for key, values in spectra.items():\n",
    "                if not values:\n",
    "                    continue\n",
    "\n",
    "                axes[idx].plot(\n",
    "                    hours,\n",
    "                    values,\n",
    "                    color=COLORS[key],\n",
    "                    linestyle=LINE_STYLES[key][0],\n",
    "                    marker=LINE_STYLES[key][1],\n",
    "                    label=key.upper(),\n",
    "                    markevery=5,\n",
    "                )\n",
    "\n",
    "            # Add titles and labels\n",
    "            axes[idx].set_title(f\"k = {target_k:.1e}\")\n",
    "            axes[idx].set_xlabel(\"Elapsed Forecast Duration [h]\")\n",
    "            if idx == 0:\n",
    "                axes[idx].set_ylabel(f\"Power Spectrum [{var}]\")\n",
    "                axes[idx].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "with LocalCluster(\n",
    "    n_workers=16,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=\"16GB\",\n",
    ") as cluster:\n",
    "    with Client(cluster) as client:\n",
    "        plot_wavenumber_evolution(\n",
    "            ds_gt.isel(time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "            ds_nwp.isel(start_time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "            ds_ml.isel(start_time=slice(None, None, SUBSAMPLE_HISTOGRAM)),\n",
    "            variables=VARIABLES_GROUND_TRUTH.values(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b36745",
   "metadata": {},
   "source": [
    "### 4. Vertical Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897b6fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In this chapter, the focus is on assessing how the relative error between the ML\n",
    "model and ground truth data.\n",
    "Vertical profiles are essential for understanding the atmospheric structure and\n",
    "processes at different pressure levels. Obviously these plots only work for 3D\n",
    "variables.\n",
    "\n",
    "**Relative Error Calculation:** Using percentage differences provides a\n",
    "normalized measure of error that is comparable across variables and vertical\n",
    "levels.\n",
    "\n",
    "**Altitude-Specific Insights:** The plots reveal whether the ML model performs\n",
    "consistently across different altitudes or if certain layers pose challenges.\n",
    "\n",
    "**Atmospheric Dynamics:** Accurate representation of vertical profiles is\n",
    "crucial for modeling phenomena like convection or jet stream anomalies.\n",
    "\n",
    "**Pressure Level Interpretation:** Lower vertical levels correspond to higher\n",
    "altitudes. Inverted axes help represent this correctly but can be\n",
    "counterintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vertical_errors(ds_gt, ds_ml, variables):\n",
    "    \"\"\"\n",
    "    Plot vertical profiles of relative error for each variable separately.\n",
    "    Variables with same prefix are stacked along level dimension.\n",
    "    Shows separate lines for each forecast duration.\n",
    "    \"\"\"\n",
    "    LINESTYLES = [\n",
    "        \"-\",\n",
    "        \"--\",\n",
    "        \":\",\n",
    "        \"-.\",\n",
    "        (0, (3, 1, 1, 1)),\n",
    "        (0, (3, 1, 1, 1, 1, 1)),\n",
    "    ]\n",
    "\n",
    "    # Group variables by their prefix (without level)\n",
    "    var_groups = {}\n",
    "\n",
    "    for var in variables.values():\n",
    "        if any(var.startswith(v) for v in VARIABLES_3D):\n",
    "            base_name = \"_\".join(var.split(\"_\")[:-2])\n",
    "            level = int(var.split(\"_\")[-1])\n",
    "\n",
    "            if base_name not in var_groups:\n",
    "                var_groups[base_name] = {\"vars\": [], \"levels\": []}\n",
    "            var_groups[base_name][\"vars\"].append(var)\n",
    "            var_groups[base_name][\"levels\"].append(level)\n",
    "            var_groups[base_name][\"unit\"] = VARIABLE_UNITS[var]\n",
    "\n",
    "    if len(var_groups) == 0:\n",
    "        print(\"No 3D variables found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    # Plot each variable separately\n",
    "    for base_name, group in var_groups.items():\n",
    "        # Create new figure for each variable\n",
    "        fig, ax = plt.subplots(figsize=(8, 6), dpi=DPI)\n",
    "\n",
    "        # Sort by level to ensure correct ordering\n",
    "        sorted_idx = np.argsort(group[\"levels\"])\n",
    "        sorted_vars = [group[\"vars\"][i] for i in sorted_idx]\n",
    "        sorted_levels = [group[\"levels\"][i] for i in sorted_idx]\n",
    "\n",
    "        # Get unique forecast durations\n",
    "        unique_durations = np.unique([\n",
    "            match[\"ml_duration_idx\"] for match in matched_indices\n",
    "        ])\n",
    "\n",
    "        for dur_idx in unique_durations:\n",
    "            # Filter matches for this duration\n",
    "            dur_matches = [\n",
    "                m for m in matched_indices if m[\"ml_duration_idx\"] == dur_idx\n",
    "            ]\n",
    "            dur_times = ds_gt_times[[m[\"gt_idx\"] for m in dur_matches]]\n",
    "\n",
    "            # Stack variables for this duration\n",
    "            stacked_gt = xr.concat(\n",
    "                [ds_gt.sel(time=dur_times)[var] for var in sorted_vars],\n",
    "                dim=pd.Index(sorted_levels, name=\"level\"),\n",
    "            )\n",
    "\n",
    "            # Create ML data for this duration\n",
    "            ml_data = {}\n",
    "            for var in sorted_vars:\n",
    "                matched_data = np.zeros((\n",
    "                    len(dur_matches),\n",
    "                    ds_ml.x.size,\n",
    "                    ds_ml.y.size,\n",
    "                ))\n",
    "                for j, match in enumerate(dur_matches):\n",
    "                    matched_data[j] = (\n",
    "                        ds_ml[var]\n",
    "                        .isel(\n",
    "                            start_time=match[\"ml_start_time_idx\"],\n",
    "                            elapsed_forecast_duration=match[\"ml_duration_idx\"],\n",
    "                        )\n",
    "                        .values\n",
    "                    )\n",
    "                ml_data[var] = ((\"time\", \"x\", \"y\"), matched_data)\n",
    "\n",
    "            ds_ml_dur = xr.Dataset(\n",
    "                data_vars=ml_data,\n",
    "                coords={\"time\": dur_times, \"x\": ds_ml.x, \"y\": ds_ml.y},\n",
    "            )\n",
    "\n",
    "            stacked_ml = xr.concat(\n",
    "                [ds_ml_dur[var] for var in sorted_vars],\n",
    "                dim=pd.Index(sorted_levels, name=\"level\"),\n",
    "            )\n",
    "\n",
    "            # Calculate relative error for this duration\n",
    "            relative_error = (\n",
    "                abs(stacked_ml - stacked_gt) / abs(stacked_gt + epsilon)\n",
    "            ).mean(dim=[\"time\", \"y\", \"x\"]) * 100\n",
    "\n",
    "            # Plot with different color for each duration\n",
    "            dur_hours = (\n",
    "                ds_ml.elapsed_forecast_duration.values[dur_idx]\n",
    "                .astype(\"timedelta64[h]\")\n",
    "                .astype(int)\n",
    "            )\n",
    "            ax.plot(\n",
    "                relative_error,\n",
    "                relative_error.level,\n",
    "                linewidth=2,\n",
    "                linestyle=LINESTYLES[dur_idx % len(LINESTYLES)],\n",
    "                label=f\"Forecast +{dur_hours}h\",\n",
    "            )\n",
    "\n",
    "        unit = group[\"unit\"][0]\n",
    "        ax.set_title(f\"Relative Error for {base_name} [{unit}]\", size=12)\n",
    "        ax.set_xlabel(\"Relative Error (%)\", size=10)\n",
    "        ax.set_ylabel(\"Level\", size=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.invert_yaxis()\n",
    "        ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_plot(fig, f\"vertical_profile_{base_name}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Create a mapping of all forecast times between ds_ml and ds_gt\n",
    "ds_ml_times = ds_ml.forecast_time.values.ravel()  # Flatten the forecast times\n",
    "ds_gt_times = ds_gt.time.values\n",
    "\n",
    "# Find exact matches between ML forecast times and ground truth times\n",
    "matched_indices = []\n",
    "for gt_idx, gt_time in enumerate(ds_gt_times):\n",
    "    # Find exact matches\n",
    "    ml_matches = np.where(ds_ml_times == gt_time)[0]\n",
    "\n",
    "    for ml_idx in ml_matches:\n",
    "        # Convert flat index to 2D indices\n",
    "        start_idx = ml_idx // len(ds_ml.elapsed_forecast_duration)\n",
    "        duration_idx = ml_idx % len(ds_ml.elapsed_forecast_duration)\n",
    "\n",
    "        matched_indices.append({\n",
    "            \"gt_idx\": gt_idx,\n",
    "            \"ml_start_time_idx\": start_idx,\n",
    "            \"ml_duration_idx\": duration_idx,\n",
    "        })\n",
    "\n",
    "with LocalCluster(\n",
    "    n_workers=16,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=\"16GB\",\n",
    ") as cluster:\n",
    "    with Client(cluster) as client:\n",
    "        plot_vertical_errors(\n",
    "            ds_gt,\n",
    "            ds_ml,\n",
    "            VARIABLES_GROUND_TRUTH,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acd86a",
   "metadata": {},
   "source": [
    "### 5. Various Verification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b338bf",
   "metadata": {},
   "source": [
    "The final chapter consolidates various statistical metrics to provide a broad\n",
    "evaluation of the ML model's performance. By considering multiple metrics, we\n",
    "gain a nuanced understanding of both the strengths and weaknesses of the model.\n",
    "\n",
    "**Metric Diversity:** Including MAE, RMSE, MSE, Pearson correlation, and the\n",
    "Fractions Skill Score (FSS) covers different aspects of model performance, from\n",
    "average errors to spatial pattern accuracy.\n",
    "\n",
    "**MAE, MSE and RMSE:** Offer insights into the average magnitude of errors, with\n",
    "RMSE emphasizing larger discrepancies. The colors indicating high errors are\n",
    "only implemented for these three metrics with standardization.\n",
    "\n",
    "**Pearson Correlation:** Assesses the linear relationship, indicating whether\n",
    "the model captures variability even if biases exist.\n",
    "\n",
    "**FSS:** Evaluates spatial accuracy, which is particularly important for\n",
    "predicting localized weather events.\n",
    "\n",
    "**Wasserstein Distance:** Provides a holistic view of distributional similarity\n",
    "across variables. Same as chapter 3.\n",
    "\n",
    "**Holistic Assessment:** The combination of metrics provides a comprehensive\n",
    "performance profile, essential for model validation and comparison. More complex metrics are explained in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca95bd3",
   "metadata": {},
   "source": [
    "#### Fractional Skill Score\n",
    "Range: 0 to 1, where:\n",
    "- 1 = perfect score\n",
    "- 0 = no skill compared to random chance\n",
    "\n",
    "**Key Properties:**\n",
    "- FSS measures the spatial agreement between two fields, accounting for the spatial scale of the features\n",
    "- It's particularly useful for assessing the spatial distribution of precipitation, cloud cover, or other fields with spatial structure\n",
    "\n",
    "**Advantages:**\n",
    "- More meaningful than simple correlation for spatial fields\n",
    "- Accounts for the spatial scale of features\n",
    "- Provides a single value for the entire field comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These helper functions are only used to calculate the FSS threshold\n",
    "max_spatial_dim = np.maximum(ds_gt.x.size, ds_gt.y.size)\n",
    "window_size = (max_spatial_dim // 100,) * 2\n",
    "n_points = int(\n",
    "    np.minimum(\n",
    "        SUBSAMPLE_FSS_THRESHOLD,\n",
    "        ds_ml[list(VARIABLES_GROUND_TRUTH.values())[0]]\n",
    "        .isel(elapsed_forecast_duration=0)\n",
    "        .size,\n",
    "    )\n",
    ")\n",
    "print(f\"Using window size for FSS: {window_size}\")\n",
    "print(f\"Using n_points for FSS: {n_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993f9ff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Modified metrics calculation function with table saving\n",
    "def calculate_metrics_by_efd(\n",
    "    ds_gt,\n",
    "    ds_nwp=None,\n",
    "    ds_ml=None,\n",
    "    metrics_to_compute=None,\n",
    "    subsample_points=1e7,\n",
    "    window_size=3,\n",
    "    prefix=\"metrics\",\n",
    "):\n",
    "    \"\"\"Calculate metrics for each Elapsed Forecast Duration and save to disk.\"\"\"\n",
    "    if isinstance(window_size, (int, float)):\n",
    "        window_size = (int(window_size), int(window_size))\n",
    "\n",
    "    if metrics_to_compute is None:\n",
    "        metrics_to_compute = [\n",
    "            \"MAE\",\n",
    "            \"RMSE\",\n",
    "            \"MSE\",\n",
    "            \"RelativeMAE\",\n",
    "            \"RelativeRMSE\",\n",
    "            \"PearsonR\",\n",
    "            \"FSS\",\n",
    "            \"Wasserstein\",\n",
    "        ]\n",
    "\n",
    "    variables = list(ds_gt.data_vars)\n",
    "    # Convert elapsed forecast durations to hours for display\n",
    "    elapsed_forecast_durations = ds_ml.elapsed_forecast_duration\n",
    "    elapsed_forecast_durations_hours = elapsed_forecast_durations.values.astype(\n",
    "        \"timedelta64[s]\"\n",
    "    ) / np.timedelta64(1, \"h\")\n",
    "\n",
    "    metrics_by_efd = {}\n",
    "    combined_metrics = {}\n",
    "\n",
    "    for efd, lt_hours in zip(\n",
    "        elapsed_forecast_durations, elapsed_forecast_durations_hours\n",
    "    ):\n",
    "        print(\n",
    "            f\"\\nCalculating metrics for elapsed forecast duration: {lt_hours.item():.1f}h\"\n",
    "        )\n",
    "\n",
    "        ds_ml_lead = ds_ml.sel(elapsed_forecast_duration=efd)\n",
    "        if ds_nwp is not None:\n",
    "            ds_nwp_lead = ds_nwp.sel(elapsed_forecast_duration=efd)\n",
    "\n",
    "        forecast_times = ds_ml_lead.forecast_time.values.flatten()\n",
    "        ds_gt_lead = ds_gt.sel(time=forecast_times)\n",
    "        metrics_dict = {}\n",
    "\n",
    "        for var in variables:\n",
    "            print(f\"Processing {var}\")\n",
    "            y_true = ds_gt_lead[var].compute()\n",
    "            y_pred_ml = ds_ml_lead[var].compute()\n",
    "\n",
    "            sample = np.random.choice(\n",
    "                y_true.values.ravel(),\n",
    "                int(min(subsample_points, y_true.size)),\n",
    "                replace=False,\n",
    "            )\n",
    "            quantile_90 = np.quantile(sample, 0.90)\n",
    "\n",
    "            metrics_dict[var] = {}\n",
    "\n",
    "            # Calculate ML metrics\n",
    "            if \"MAE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"MAE ML\"] = mae(y_true, y_pred_ml).values\n",
    "            if \"RMSE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"RMSE ML\"] = rmse(y_true, y_pred_ml).values\n",
    "            if \"MSE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"MSE ML\"] = mse(y_true, y_pred_ml).values\n",
    "            if \"RelativeMAE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"Relative MAE ML\"] = np.mean(\n",
    "                    np.abs(y_true.values - y_pred_ml.values)\n",
    "                    / (np.abs(y_true.values) + 1e-6)\n",
    "                )\n",
    "            if \"RelativeRMSE\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"Relative RMSE ML\"] = np.sqrt(\n",
    "                    np.mean(\n",
    "                        (y_true.values - y_pred_ml.values) ** 2\n",
    "                        / (y_true.values**2 + 1e-6)\n",
    "                    )\n",
    "                )\n",
    "            if \"PearsonR\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"Pearson R ML\"] = pearsonr(\n",
    "                    y_true, y_pred_ml\n",
    "                ).values\n",
    "            if \"FSS\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"FSS ML\"] = fss_2d(\n",
    "                    y_pred_ml,\n",
    "                    y_true,\n",
    "                    event_threshold=quantile_90,\n",
    "                    window_size=window_size,\n",
    "                    spatial_dims=[\"y\", \"x\"],\n",
    "                ).values\n",
    "            if \"Wasserstein\" in metrics_to_compute:\n",
    "                metrics_dict[var][\"Wasserstein ML\"] = wasserstein_distance(\n",
    "                    y_true.values.flatten(), y_pred_ml.values.flatten()\n",
    "                )\n",
    "\n",
    "            # Calculate NWP metrics if available\n",
    "            if ds_nwp is not None and var in ds_nwp:\n",
    "                y_pred_nwp = ds_nwp_lead[var].compute()\n",
    "\n",
    "                if \"MAE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"MAE NWP\"] = mae(\n",
    "                        y_true, y_pred_nwp\n",
    "                    ).values\n",
    "                if \"RMSE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"RMSE NWP\"] = rmse(\n",
    "                        y_true, y_pred_nwp\n",
    "                    ).values\n",
    "                if \"MSE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"MSE NWP\"] = mse(\n",
    "                        y_true, y_pred_nwp\n",
    "                    ).values\n",
    "                if \"RelativeMAE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"Relative MAE NWP\"] = np.mean(\n",
    "                        np.abs(y_true.values - y_pred_nwp.values)\n",
    "                        / (np.abs(y_true.values) + 1e-6)\n",
    "                    )\n",
    "                if \"RelativeRMSE\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"Relative RMSE NWP\"] = np.sqrt(\n",
    "                        np.mean(\n",
    "                            (y_true.values - y_pred_nwp.values) ** 2\n",
    "                            / (y_true.values**2 + 1e-6)\n",
    "                        )\n",
    "                    )\n",
    "                if \"PearsonR\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"Pearson R NWP\"] = pearsonr(\n",
    "                        y_true, y_pred_nwp\n",
    "                    ).values\n",
    "                if \"FSS\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"FSS NWP\"] = fss_2d(\n",
    "                        y_pred_nwp,\n",
    "                        y_true,\n",
    "                        event_threshold=quantile_90,\n",
    "                        window_size=window_size,\n",
    "                        spatial_dims=[\"y\", \"x\"],\n",
    "                    ).values\n",
    "                if \"Wasserstein\" in metrics_to_compute:\n",
    "                    metrics_dict[var][\"Wasserstein NWP\"] = wasserstein_distance(\n",
    "                        y_true.values.flatten(), y_pred_nwp.values.flatten()\n",
    "                    )\n",
    "\n",
    "            # Store combined metrics\n",
    "            for metric_name, value in metrics_dict[var].items():\n",
    "                key = f\"{var}_{metric_name}\"\n",
    "                if key not in combined_metrics:\n",
    "                    combined_metrics[key] = []\n",
    "                combined_metrics[key].append(value)\n",
    "\n",
    "        metrics_by_efd[lt_hours.item()] = pd.DataFrame.from_dict(\n",
    "            metrics_dict, orient=\"index\"\n",
    "        )\n",
    "\n",
    "    # Save combined metrics with hours as index\n",
    "    elapsed_forecast_durations_hours_float = [\n",
    "        x.item() for x in elapsed_forecast_durations_hours\n",
    "    ]\n",
    "    df_combined = pd.DataFrame(\n",
    "        combined_metrics, index=elapsed_forecast_durations_hours_float\n",
    "    )\n",
    "    export_table(df_combined, f\"{prefix}_combined\", \"Combined metrics\")\n",
    "\n",
    "    return metrics_by_efd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with LocalCluster(\n",
    "    n_workers=16,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=\"16GB\",\n",
    ") as cluster:\n",
    "    with Client(cluster) as client:\n",
    "        metrics_by_efd = calculate_metrics_by_efd(\n",
    "            ds_gt=ds_gt,\n",
    "            ds_nwp=ds_nwp,\n",
    "            ds_ml=ds_ml,\n",
    "            prefix=\"forecast_metrics\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f902159",
   "metadata": {},
   "outputs": [],
   "source": [
    "efd = list(metrics_by_efd.keys())[-1]  # Select your EFD for display!\n",
    "display(metrics_by_efd[efd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16736c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution of a specific metric over elapsed forecast duration\n",
    "elapsed_forecast_durations = list(metrics_by_efd.keys())\n",
    "metrics_to_compute = [\n",
    "    \"MAE\",\n",
    "    # \"RMSE\",\n",
    "    # \"MSE\",\n",
    "    # \"Relative MAE\",\n",
    "    # \"Relative RMSE\",\n",
    "    # \"Pearson R\",\n",
    "    # \"FSS\",\n",
    "    # \"Wasserstein\",\n",
    "]\n",
    "for variable in VARIABLES_GROUND_TRUTH.values():\n",
    "    for metric in metrics_to_compute:\n",
    "        try:\n",
    "            # Skip if any scores are missing\n",
    "            ml_scores = [\n",
    "                df.loc[variable, f\"{metric} ML\"]\n",
    "                for df in metrics_by_efd.values()\n",
    "            ]\n",
    "            nwp_scores = [\n",
    "                df.loc[variable, f\"{metric} NWP\"]\n",
    "                for df in metrics_by_efd.values()\n",
    "            ]\n",
    "\n",
    "            # Convert elapsed forecast durations from hours to timedelta\n",
    "            hours = [\n",
    "                x / np.timedelta64(1, \"h\")\n",
    "                for x in ds_ml.elapsed_forecast_duration.values\n",
    "            ]\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6), dpi=DPI)\n",
    "\n",
    "            # Plot ML scores\n",
    "            ax.plot(\n",
    "                hours,\n",
    "                ml_scores,\n",
    "                label=\"ML\",\n",
    "                color=COLORS[\"ml\"],\n",
    "                linestyle=LINE_STYLES[\"ml\"][0],\n",
    "                marker=LINE_STYLES[\"ml\"][1],\n",
    "            )\n",
    "\n",
    "            # Plot NWP scores if they exist and are not all NaN\n",
    "            if not all(pd.isna(nwp_scores)):\n",
    "                ax.plot(\n",
    "                    hours,\n",
    "                    nwp_scores,\n",
    "                    label=\"NWP\",\n",
    "                    color=COLORS[\"nwp\"],\n",
    "                    linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "                    marker=LINE_STYLES[\"nwp\"][1],\n",
    "                )\n",
    "\n",
    "            ax.set_xlabel(\"Elapsed Forecast Duration (hours)\")\n",
    "            ax.set_ylabel(f\"{metric} [{VARIABLE_UNITS[variable]}]\")\n",
    "            ax.set_title(f\"{metric} Evolution for {variable}\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            save_plot(fig, f\"{metric}_{variable}_evolution\")\n",
    "            plt.close()\n",
    "\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"Skipping {metric} for {variable}: {str(e)}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e8286",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#### Equitable Threat Score (Traditional Version)\n",
    "Range: [-1/3, 1], where:\n",
    "- 1 = perfect score\n",
    "- 0 = no skill compared to random chance\n",
    "- -1/3 = worst possible performance\n",
    "\n",
    "**Key Properties:**\n",
    "- Measures how well predicted events correspond to observed events, accounting for hits due to random chance\n",
    "- Particularly useful for rare events (like precipitation above a high threshold)\n",
    "- More equitable than simple Threat Score by accounting for hits due to random chance\n",
    "\n",
    "**Advantages:**\n",
    "- Well-established metric in meteorological verification\n",
    "- Reference point at 0 makes interpretation clear\n",
    "- Penalizes both misses and false alarms\n",
    "- Accounts for random chance, making it more robust than basic threat scores\n",
    "\n",
    "#### Frequency Bias Index\n",
    "Range: 0 to infinity, where:\n",
    "- 1 = no bias\n",
    "- < 1 = underforecasting\n",
    "- > 1 = overforecasting\n",
    "\n",
    "**Key Properties:**\n",
    "- FBI measures the ratio of observed to forecasted events, indicating whether the model tends to over- or underforecast\n",
    "- It's particularly useful for understanding systematic biases in event frequency\n",
    "\n",
    "**Advantages:**\n",
    "- Provides a clear indication of over- or underforecasting\n",
    "- Easy to interpret: 1 indicates no bias, while values above or below 1 show the direction and magnitude of the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43af69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_thresholds = THRESHOLDS_PRECIPITATION\n",
    "wind_thresholds = THRESHOLDS_WIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea758476",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set display options for all float values\n",
    "pd.set_option(\"display.float_format\", lambda x: \"{:.4f}\".format(x))\n",
    "\n",
    "\n",
    "def frequency_bias(obs, pred, threshold):\n",
    "    \"\"\"Calculate Frequency Bias Index (FBI) for binary events.\"\"\"\n",
    "    count_obs = np.sum(obs >= threshold)\n",
    "    count_pred = np.sum(pred >= threshold)\n",
    "    return count_pred / count_obs if count_obs > 0 else np.nan\n",
    "\n",
    "\n",
    "def mean_error(obs, pred):\n",
    "    \"\"\"Calculate mean error (ME) between prediction and observation.\"\"\"\n",
    "    return float(np.mean(pred - obs))\n",
    "\n",
    "\n",
    "def mean_absolute_error(obs, pred):\n",
    "    \"\"\"Calculate mean absolute error (MAE) between prediction and observation.\"\"\"\n",
    "    return float(np.mean(np.abs(pred - obs)))\n",
    "\n",
    "\n",
    "def ets_for_threshold(obs, pred, threshold):\n",
    "    \"\"\"Calculate traditional Equitable Threat Score (ETS).\n",
    "    Range: [-1/3, 1] where:\n",
    "    -1/3: Worst possible performance\n",
    "    0: Random chance performance\n",
    "    1: Perfect performance\n",
    "    \"\"\"\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        obs = xr.DataArray(obs)\n",
    "    if isinstance(pred, np.ndarray):\n",
    "        pred = xr.DataArray(pred)\n",
    "\n",
    "    # Convert to binary using threshold\n",
    "    f_binary = (pred > threshold).astype(int)\n",
    "    o_binary = (obs > threshold).astype(int)\n",
    "\n",
    "    try:\n",
    "        # Compute contingency table\n",
    "        hits = float(((f_binary == 1) & (o_binary == 1)).sum())\n",
    "        false_alarms = float(((f_binary == 1) & (o_binary == 0)).sum())\n",
    "        misses = float(((f_binary == 0) & (o_binary == 1)).sum())\n",
    "        correct_zeros = float(((f_binary == 0) & (o_binary == 0)).sum())\n",
    "\n",
    "        total = hits + false_alarms + misses + correct_zeros\n",
    "\n",
    "        # Calculate hits due to chance\n",
    "        hits_random = ((hits + false_alarms) * (hits + misses)) / total\n",
    "\n",
    "        # Calculate ETS score\n",
    "        denominator = hits - hits_random + false_alarms + misses\n",
    "        if denominator == 0:\n",
    "            return 0.0 if total > 0 else 1.0\n",
    "\n",
    "        return (hits - hits_random) / denominator\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing ETS: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def stdev_error(obs, pred):\n",
    "    \"\"\"Calculate standard deviation of error.\"\"\"\n",
    "    return abs(np.std(pred - obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_meteoswiss_metrics_over_efds(\n",
    "    ds_gt, ds_ml, ds_nwp, elapsed_forecast_durations\n",
    "):\n",
    "    \"\"\"Calculate MeteoSwiss verification metrics for each elapsed forecast duration.\"\"\"\n",
    "    metrics_by_efd = {}\n",
    "\n",
    "    # Get available variables in each dataset\n",
    "    gt_ml_vars = set(ds_gt.variables) & set(ds_ml.variables)\n",
    "    nwp_vars = set(ds_nwp.variables) if ds_nwp is not None else set()\n",
    "\n",
    "    all_variables = {\n",
    "        \"precipitation\": {\"thresholds\": precip_thresholds, \"unit\": \"mm/h\"},\n",
    "        \"wind_u_10m\": {\"thresholds\": wind_thresholds, \"unit\": \"m/s\"},\n",
    "        \"wind_v_10m\": {\"thresholds\": wind_thresholds, \"unit\": \"m/s\"},\n",
    "        \"temperature_2m\": {\"thresholds\": None, \"unit\": \"K\"},\n",
    "        \"surface_net_shortwave_radiation\": {\"thresholds\": None, \"unit\": \"W/m²\"},\n",
    "        \"surface_net_longwave_radiation\": {\"thresholds\": None, \"unit\": \"W/m²\"},\n",
    "    }\n",
    "\n",
    "    # Filter variables that exist in gt and ml\n",
    "    all_variables = {k: v for k, v in all_variables.items() if k in gt_ml_vars}\n",
    "\n",
    "    for efd in elapsed_forecast_durations:\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\nCalculating metrics for elapsed forecast duration: {efd / np.timedelta64(1, 'h'):.1f}h\"\n",
    "            )\n",
    "\n",
    "            ds_ml_lead = ds_ml.sel(elapsed_forecast_duration=efd)\n",
    "            ds_nwp_lead = (\n",
    "                ds_nwp.sel(elapsed_forecast_duration=efd)\n",
    "                if ds_nwp is not None\n",
    "                else None\n",
    "            )\n",
    "            forecast_times = ds_ml_lead.forecast_time.values\n",
    "            ds_gt_lead = ds_gt.sel(time=forecast_times)\n",
    "\n",
    "            metrics_dict = {}\n",
    "\n",
    "            for var_name, var_config in all_variables.items():\n",
    "                print(f\"Processing {var_name}\")\n",
    "                try:\n",
    "                    y_true = ds_gt_lead[var_name].values.flatten()\n",
    "                    y_ml = ds_ml_lead[var_name].values.flatten()\n",
    "                    y_nwp = (\n",
    "                        ds_nwp_lead[var_name].values.flatten()\n",
    "                        if ds_nwp_lead is not None and var_name in nwp_vars\n",
    "                        else None\n",
    "                    )\n",
    "\n",
    "                    if var_config[\"thresholds\"] is not None:\n",
    "                        for thr in var_config[\"thresholds\"]:\n",
    "                            metric_key = f\"{var_name}_{thr}{var_config['unit']}\"\n",
    "                            metrics_dict[metric_key] = {\n",
    "                                \"FBI_ML\": float(\n",
    "                                    frequency_bias(y_true, y_ml, thr)\n",
    "                                ),\n",
    "                                \"ETS_ML\": float(\n",
    "                                    ets_for_threshold(y_true, y_ml, thr)\n",
    "                                ),\n",
    "                            }\n",
    "\n",
    "                            if y_nwp is not None:\n",
    "                                metrics_dict[metric_key].update({\n",
    "                                    \"FBI_NWP\": float(\n",
    "                                        frequency_bias(y_true, y_nwp, thr)\n",
    "                                    ),\n",
    "                                    \"ETS_NWP\": float(\n",
    "                                        ets_for_threshold(y_true, y_nwp, thr)\n",
    "                                    ),\n",
    "                                })\n",
    "\n",
    "                    # Basic metrics\n",
    "                    metrics_dict[var_name] = {\n",
    "                        \"MAE_ML\": float(mean_absolute_error(y_true, y_ml)),\n",
    "                        \"ME_ML\": float(mean_error(y_true, y_ml)),\n",
    "                        \"STDEV_ML\": float(stdev_error(y_true, y_ml)),\n",
    "                    }\n",
    "\n",
    "                    if y_nwp is not None:\n",
    "                        metrics_dict[var_name].update({\n",
    "                            \"MAE_NWP\": float(\n",
    "                                mean_absolute_error(y_true, y_nwp)\n",
    "                            ),\n",
    "                            \"ME_NWP\": float(mean_error(y_true, y_nwp)),\n",
    "                            \"STDEV_NWP\": float(stdev_error(y_true, y_nwp)),\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {var_name}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            metrics_by_efd[efd] = metrics_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing elapsed forecast duration {efd}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return metrics_by_efd\n",
    "\n",
    "\n",
    "def plot_metrics_evolution(\n",
    "    metrics_by_efd,\n",
    "    elapsed_forecast_durations,\n",
    "    var_name,\n",
    "    metric_name,\n",
    "    output_dir=\"./plots\",\n",
    "):\n",
    "    \"\"\"Plot evolution of a specific metric over elapsed forecast durations with robust error handling.\"\"\"\n",
    "    try:\n",
    "        if (\n",
    "            not metrics_by_efd or not elapsed_forecast_durations.size\n",
    "        ):  # Changed check for numpy array\n",
    "            print(\"No metrics data or elapsed forecast durations provided\")\n",
    "            return\n",
    "\n",
    "        # Filter valid times\n",
    "        valid_times = []\n",
    "        for efd in elapsed_forecast_durations:\n",
    "            if (\n",
    "                efd in metrics_by_efd\n",
    "                and var_name in metrics_by_efd[efd]\n",
    "                and f\"{metric_name}_ML\" in metrics_by_efd[efd][var_name]\n",
    "            ):\n",
    "                valid_times.append(efd)\n",
    "\n",
    "        if not valid_times:\n",
    "            print(f\"No valid data to plot for {var_name} - {metric_name}\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        elapsed_forecast_durations_hours = [\n",
    "            efd / np.timedelta64(1, \"h\") for efd in valid_times\n",
    "        ]\n",
    "\n",
    "        ml_values = []\n",
    "        nwp_values = []\n",
    "\n",
    "        for efd in valid_times:\n",
    "            try:\n",
    "                metrics = metrics_by_efd[efd][var_name]\n",
    "                ml_val = metrics[f\"{metric_name}_ML\"]\n",
    "                # Handle potential numpy arrays\n",
    "                if isinstance(ml_val, np.ndarray):\n",
    "                    ml_values.append(float(ml_val.mean()))\n",
    "                else:\n",
    "                    ml_values.append(float(ml_val))\n",
    "\n",
    "                # Only try to get NWP values if the metric exists\n",
    "                if f\"{metric_name}_NWP\" in metrics:\n",
    "                    nwp_val = metrics[f\"{metric_name}_NWP\"]\n",
    "                    if isinstance(nwp_val, np.ndarray):\n",
    "                        nwp_values.append(float(nwp_val.mean()))\n",
    "                    else:\n",
    "                        nwp_values.append(float(nwp_val))\n",
    "            except (KeyError, TypeError) as e:\n",
    "                print(f\"Error extracting metrics for {efd}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if ml_values:\n",
    "            plt.plot(\n",
    "                elapsed_forecast_durations_hours,\n",
    "                ml_values,\n",
    "                label=\"ML\",\n",
    "                linestyle=LINE_STYLES[\"ml\"][0],\n",
    "                marker=LINE_STYLES[\"ml\"][1],\n",
    "            )\n",
    "\n",
    "        # Only plot NWP if we have values and they match the number of elapsed forecast durations\n",
    "        if nwp_values and len(nwp_values) == len(\n",
    "            elapsed_forecast_durations_hours\n",
    "        ):\n",
    "            plt.plot(\n",
    "                elapsed_forecast_durations_hours,\n",
    "                nwp_values,\n",
    "                label=\"NWP\",\n",
    "                linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "                marker=LINE_STYLES[\"nwp\"][1],\n",
    "            )\n",
    "\n",
    "        plt.xlabel(\"Elapsed Forecast Duration (hours)\")\n",
    "        plt.ylabel(f\"{metric_name} [{VARIABLE_UNITS.get(var_name, '')}]\")\n",
    "        plt.title(f\"{metric_name} Evolution for {var_name}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        save_plot(plt, f\"{var_name}_{metric_name}\")\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting {var_name} - {metric_name}: {str(e)}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Calculate metrics with error handling\n",
    "try:\n",
    "    with LocalCluster(\n",
    "        n_workers=16,\n",
    "        threads_per_worker=1,\n",
    "        memory_limit=\"16GB\",\n",
    "    ) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            metrics_by_efd = calculate_meteoswiss_metrics_over_efds(\n",
    "                ds_gt, ds_ml, ds_nwp, ds_ml.elapsed_forecast_duration.values\n",
    "            )\n",
    "            df_combined = pd.DataFrame(metrics_by_efd)\n",
    "            export_table(\n",
    "                df_combined,\n",
    "                \"forecast_metrics\",\n",
    "                \"Forecast metrics ala MeteoSwiss\",\n",
    "            )\n",
    "\n",
    "            elapsed_forecast_durations = ds_ml.elapsed_forecast_duration.values\n",
    "\n",
    "            # Plot metrics for available variables\n",
    "            if metrics_by_efd and elapsed_forecast_durations.size > 0:\n",
    "                for var in list(\n",
    "                    metrics_by_efd[elapsed_forecast_durations[0]].keys()\n",
    "                ):\n",
    "                    try:\n",
    "                        metrics = metrics_by_efd[elapsed_forecast_durations[0]][\n",
    "                            var\n",
    "                        ].keys()\n",
    "                        for metric in [\n",
    "                            m.split(\"_\")[0]\n",
    "                            for m in metrics\n",
    "                            if m.endswith(\"_ML\")\n",
    "                        ]:\n",
    "                            plot_metrics_evolution(\n",
    "                                metrics_by_efd,\n",
    "                                elapsed_forecast_durations,\n",
    "                                var,\n",
    "                                metric,\n",
    "                            )\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"Error processing plots for variable {var}: {str(e)}\"\n",
    "                        )\n",
    "                        continue\n",
    "            else:\n",
    "                print(\"No metrics data available for plotting\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in main execution: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_vector_rmse(u_true, v_true, u_pred, v_pred):\n",
    "    \"\"\"Calculate RMSE based on wind vector differences.\"\"\"\n",
    "    rmse_u = rmse(u_true, u_pred)\n",
    "    rmse_v = rmse(v_true, v_pred)\n",
    "    rmse_wind = np.sqrt(rmse_u**2 + rmse_v**2)\n",
    "    return float(rmse_wind)\n",
    "\n",
    "\n",
    "# Initialize results dictionary\n",
    "results_wind = {\"vector_metrics\": {}}\n",
    "elapsed_forecast_durations = ds_ml.elapsed_forecast_duration.values\n",
    "\n",
    "# Lists to store RMSE values over time\n",
    "ml_rmse_over_time = []\n",
    "nwp_rmse_over_time = []\n",
    "forecast_hours = [\n",
    "    efd / np.timedelta64(1, \"h\") for efd in elapsed_forecast_durations\n",
    "]\n",
    "\n",
    "if \"wind_u_10m\" in ds_gt and \"wind_v_10m\" in ds_gt:\n",
    "    for efd in elapsed_forecast_durations:\n",
    "        # Get corresponding forecast times\n",
    "        forecast_times = ds_ml.sel(\n",
    "            elapsed_forecast_duration=efd\n",
    "        ).forecast_time.values\n",
    "\n",
    "        # Get wind components for specific forecast time\n",
    "        u_true = ds_gt[\"wind_u_10m\"].sel(time=forecast_times)\n",
    "        v_true = ds_gt[\"wind_v_10m\"].sel(time=forecast_times)\n",
    "        u_ml = ds_ml[\"wind_u_10m\"].sel(elapsed_forecast_duration=efd)\n",
    "        v_ml = ds_ml[\"wind_v_10m\"].sel(elapsed_forecast_duration=efd)\n",
    "\n",
    "        # Calculate ML RMSE\n",
    "        wind_rmse_ml = wind_vector_rmse(u_true, v_true, u_ml, v_ml)\n",
    "        ml_rmse_over_time.append(wind_rmse_ml)\n",
    "\n",
    "        # Calculate NWP RMSE if available\n",
    "        if \"wind_u_10m\" in ds_nwp and \"wind_v_10m\" in ds_nwp:\n",
    "            u_nwp = ds_nwp[\"wind_u_10m\"].sel(elapsed_forecast_duration=efd)\n",
    "            v_nwp = ds_nwp[\"wind_v_10m\"].sel(elapsed_forecast_duration=efd)\n",
    "            wind_rmse_nwp = wind_vector_rmse(u_true, v_true, u_nwp, v_nwp)\n",
    "            nwp_rmse_over_time.append(wind_rmse_nwp)\n",
    "        else:\n",
    "            nwp_rmse_over_time.append(np.nan)\n",
    "\n",
    "    # Store average metrics\n",
    "    results_wind[\"vector_metrics\"] = {\n",
    "        \"RMSE ML\": np.mean(ml_rmse_over_time),\n",
    "        \"RMSE NWP\": np.mean(nwp_rmse_over_time),\n",
    "    }\n",
    "\n",
    "    # Create metrics DataFrame\n",
    "    vector_metrics_df = pd.DataFrame(\n",
    "        results_wind[\"vector_metrics\"], index=[\"Value\"]\n",
    "    ).T.round(4)\n",
    "\n",
    "    # Create time series DataFrame\n",
    "    time_series_df = pd.DataFrame({\n",
    "        \"Elapsed Forecast Duration\": forecast_hours,\n",
    "        \"ML RMSE\": ml_rmse_over_time,\n",
    "        \"NWP RMSE\": nwp_rmse_over_time,\n",
    "    })\n",
    "\n",
    "    # Plot RMSE over time\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot ML model\n",
    "    ax.plot(\n",
    "        forecast_hours,\n",
    "        ml_rmse_over_time,\n",
    "        linestyle=LINE_STYLES[\"ml\"][0],\n",
    "        marker=LINE_STYLES[\"ml\"][1],\n",
    "        label=\"ML\",\n",
    "        color=COLORS[\"ml\"],\n",
    "    )\n",
    "\n",
    "    # Plot NWP model if available\n",
    "    if not all(np.isnan(nwp_rmse_over_time)):\n",
    "        ax.plot(\n",
    "            forecast_hours,\n",
    "            nwp_rmse_over_time,\n",
    "            linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "            marker=LINE_STYLES[\"nwp\"][1],\n",
    "            label=\"NWP\",\n",
    "            color=COLORS[\"nwp\"],\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Elapsed Forecast Duration (hours)\")\n",
    "    ax.set_ylabel(\"Wind Vector RMSE (m/s)\")\n",
    "    ax.set_title(\"Wind Vector RMSE Evolution\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    save_plot(plt, \"wind_vector_rmse_evolution\")\n",
    "    plt.close()\n",
    "\n",
    "    # Export tables\n",
    "    export_table(\n",
    "        vector_metrics_df,\n",
    "        \"wind_vector_metrics\",\n",
    "        caption=\"Average Wind vector RMSE metrics comparing ML and NWP predictions\",\n",
    "    )\n",
    "\n",
    "    export_table(\n",
    "        time_series_df,\n",
    "        \"wind_vector_metrics_timeseries\",\n",
    "        caption=\"Wind vector RMSE over forecast duration\",\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nWind Vector RMSE Metrics:\")\n",
    "    display(vector_metrics_df)\n",
    "    print(\"\\nRMSE over forecast duration:\")\n",
    "    display(time_series_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b679ce5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Combined SAL = |S| + |A| + |L|\n",
    "- Range: [0 to 6]\n",
    "- 0: Perfect forecast\n",
    "- Higher values indicate worse forecasts\n",
    "\n",
    "1. Structure (S): [-2 to +2]\n",
    "- Measures how well the spatial patterns match\n",
    "- S = 0: Perfect structural agreement\n",
    "- S > 0: Predicted patterns too large/flat\n",
    "- S < 0: Predicted patterns too peaked/small\n",
    "\n",
    "2. Amplitude (A): [-2 to +2]\n",
    "- Measures the accuracy of domain-averaged values\n",
    "- A = 0: Perfect amplitude match\n",
    "- A > 0: Overestimation\n",
    "- A < 0: Underestimation\n",
    "\n",
    "3. Location (L): [0 to +2]\n",
    "- Measures the accuracy of spatial placement\n",
    "- L = 0: Perfect location match\n",
    "- L increases with distance between predicted and observed centers of mass\n",
    "\n",
    "SAL works best for:\n",
    "- Fields with distinct objects/features\n",
    "- Variables that can form coherent structures\n",
    "- Fields with clear boundaries/gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%echo skipping\n",
    "\n",
    "\n",
    "def calculate_sal_over_efds(\n",
    "    ds_gt, ds_nwp, ds_ml, thr_factor=0.067, thr_quantile=0.9\n",
    "):\n",
    "    \"\"\"Calculate SAL metrics for each elapsed forecast duration.\"\"\"\n",
    "    var = \"precipitation\"\n",
    "    if var not in ds_gt or var not in ds_nwp or var not in ds_ml:\n",
    "        raise ValueError(f\"Variable {var} not found in datasets.\")\n",
    "\n",
    "    elapsed_forecast_durations = ds_ml.elapsed_forecast_duration.values\n",
    "    forecast_hours = [\n",
    "        efd / np.timedelta64(1, \"h\") for efd in elapsed_forecast_durations\n",
    "    ]\n",
    "\n",
    "    # Initialize dictionaries for storing scores\n",
    "    scores = {\n",
    "        \"Structure\": {\"ML\": [], \"NWP\": []},\n",
    "        \"Amplitude\": {\"ML\": [], \"NWP\": []},\n",
    "        \"Location\": {\"ML\": [], \"NWP\": []},\n",
    "        \"Combined\": {\"ML\": [], \"NWP\": []},\n",
    "    }\n",
    "\n",
    "    # Calculate metrics for each elapsed forecast duration\n",
    "    for efd in elapsed_forecast_durations:\n",
    "        try:\n",
    "            # Get ML and NWP data for this elapsed forecast duration\n",
    "            ml_slice = ds_ml.sel(elapsed_forecast_duration=efd)[var].values\n",
    "            nwp_slice = ds_nwp.sel(elapsed_forecast_duration=efd)[var].values\n",
    "\n",
    "            # Calculate valid times for ground truth\n",
    "            valid_times = (\n",
    "                ds_ml.sel(elapsed_forecast_duration=efd).start_time + efd\n",
    "            )\n",
    "            gt_slice = ds_gt.sel(time=valid_times)[var].values\n",
    "\n",
    "            # Initialize temporary scores for this elapsed forecast duration\n",
    "            temp_scores = {\n",
    "                \"Structure\": {\"ML\": [], \"NWP\": []},\n",
    "                \"Amplitude\": {\"ML\": [], \"NWP\": []},\n",
    "                \"Location\": {\"ML\": [], \"NWP\": []},\n",
    "                \"Combined\": {\"ML\": [], \"NWP\": []},\n",
    "            }\n",
    "\n",
    "            # Calculate SAL for each timestep\n",
    "            for i in range(len(valid_times)):\n",
    "                # Calculate SAL scores\n",
    "                sal_score_nwp = sal(\n",
    "                    nwp_slice[i],\n",
    "                    gt_slice[i],\n",
    "                    thr_factor=thr_factor,\n",
    "                    thr_quantile=thr_quantile,\n",
    "                )\n",
    "                sal_score_ml = sal(\n",
    "                    ml_slice[i],\n",
    "                    gt_slice[i],\n",
    "                    thr_factor=thr_factor,\n",
    "                    thr_quantile=thr_quantile,\n",
    "                )\n",
    "\n",
    "                # Store individual scores\n",
    "                temp_scores[\"Structure\"][\"NWP\"].append(sal_score_nwp[0])\n",
    "                temp_scores[\"Amplitude\"][\"NWP\"].append(sal_score_nwp[1])\n",
    "                temp_scores[\"Location\"][\"NWP\"].append(sal_score_nwp[2])\n",
    "                temp_scores[\"Combined\"][\"NWP\"].append(\n",
    "                    sum(abs(x) for x in sal_score_nwp)\n",
    "                )\n",
    "\n",
    "                temp_scores[\"Structure\"][\"ML\"].append(sal_score_ml[0])\n",
    "                temp_scores[\"Amplitude\"][\"ML\"].append(sal_score_ml[1])\n",
    "                temp_scores[\"Location\"][\"ML\"].append(sal_score_ml[2])\n",
    "                temp_scores[\"Combined\"][\"ML\"].append(\n",
    "                    sum(abs(x) for x in sal_score_ml)\n",
    "                )\n",
    "\n",
    "            # Calculate mean scores for this elapsed forecast duration\n",
    "            for metric in scores:\n",
    "                scores[metric][\"NWP\"].append(\n",
    "                    np.nanmean(temp_scores[metric][\"NWP\"])\n",
    "                )\n",
    "                scores[metric][\"ML\"].append(\n",
    "                    np.nanmean(temp_scores[metric][\"ML\"])\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error calculating SAL for elapsed forecast duration {efd / np.timedelta64(1, 'h')}h: {str(e)}\"\n",
    "            )\n",
    "            for metric in scores:\n",
    "                scores[metric][\"ML\"].append(np.nan)\n",
    "                scores[metric][\"NWP\"].append(np.nan)\n",
    "            continue\n",
    "\n",
    "    # Create time series DataFrame\n",
    "    df_timeseries = pd.DataFrame({\n",
    "        \"Elapsed Forecast Duration\": forecast_hours,\n",
    "        **{\n",
    "            f\"{metric}_{model}\": scores[metric][model]\n",
    "            for metric in scores\n",
    "            for model in [\"ML\", \"NWP\"]\n",
    "        },\n",
    "    })\n",
    "\n",
    "    # Create summary DataFrame with means\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            (var, metric, model): np.nanmean(scores[metric][model])\n",
    "            for metric in scores\n",
    "            for model in [\"ML\", \"NWP\"]\n",
    "        },\n",
    "        index=[\"Value\"],\n",
    "    ).T\n",
    "\n",
    "    # Export tables\n",
    "    export_table(\n",
    "        df_summary,\n",
    "        \"sal_metrics_summary\",\n",
    "        caption=\"Average SAL metrics for precipitation prediction\",\n",
    "    )\n",
    "\n",
    "    export_table(\n",
    "        df_timeseries,\n",
    "        \"sal_metrics_timeseries\",\n",
    "        caption=\"SAL metrics evolution over forecast duration\",\n",
    "    )\n",
    "\n",
    "    # Create plots for each metric\n",
    "    for metric in scores:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Plot ML scores\n",
    "        ax.plot(\n",
    "            forecast_hours,\n",
    "            scores[metric][\"ML\"],\n",
    "            linestyle=LINE_STYLES[\"ml\"][0],\n",
    "            marker=LINE_STYLES[\"ml\"][1],\n",
    "            label=\"ML\",\n",
    "            color=COLORS[\"ml\"],\n",
    "        )\n",
    "\n",
    "        # Plot NWP scores\n",
    "        ax.plot(\n",
    "            forecast_hours,\n",
    "            scores[metric][\"NWP\"],\n",
    "            linestyle=LINE_STYLES[\"nwp\"][0],\n",
    "            marker=LINE_STYLES[\"nwp\"][1],\n",
    "            label=\"NWP\",\n",
    "            color=COLORS[\"nwp\"],\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(\"Elapsed Forecast Duration (hours)\")\n",
    "        ax.set_ylabel(f\"SAL {metric} Score\")\n",
    "        ax.set_title(f\"SAL {metric} Score Evolution\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        save_plot(plt, f\"sal_{metric.lower()}_evolution\")\n",
    "        plt.close()\n",
    "\n",
    "    return df_summary.style.format(\"{:.4f}\")\n",
    "\n",
    "\n",
    "if \"precipitation\" in ds_gt:\n",
    "    with LocalCluster(\n",
    "        n_workers=16,\n",
    "        threads_per_worker=1,\n",
    "        memory_limit=\"16GB\",\n",
    "    ) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            sal_metrics = calculate_sal_over_efds(ds_gt, ds_nwp, ds_ml)\n",
    "            display(sal_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "neural-lam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
